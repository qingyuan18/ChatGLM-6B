{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "252de0de",
   "metadata": {},
   "source": [
    "### SageMaker  Endpoint 部署ChatGLM\n",
    "  \n",
    "[ChatGLM](https://github.com/THUDM/ChatGLM-6B): ChatGLM-6B 是一个开源的、支持中英双语的对话语言模型，基于 General Language Model (GLM) 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。 ChatGLM-6B 使用了和 ChatGPT 相似的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。\n",
    "\n",
    "#### 准备\n",
    "1. 升级boto3, sagemaker python sdk  \n",
    "2. 准备inference.py, requirements.txt\n",
    "3. 准备s5cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f2c403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade boto3\n",
    "!pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4a30f3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::687912291502:role/service-role/AmazonSageMaker-ExecutionRole-20211013T113123\n",
      "sagemaker-us-west-2-687912291502\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region_name = boto3.session.Session().region_name\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(role)\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a2b0b22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy\n",
      "upload: ./model.tar.gz to s3://sagemaker-us-west-2-687912291502/chatglm/assets/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!touch dummy\n",
    "!tar czvf model.tar.gz dummy\n",
    "assets_dir = 's3://{0}/{1}/assets/'.format(bucket, 'chatglm')\n",
    "model_data = 's3://{0}/{1}/assets/model.tar.gz'.format(bucket, 'chatglm')\n",
    "!aws s3 cp model.tar.gz $assets_dir\n",
    "!rm -f dummy model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacc988d-061c-4e6b-b979-5d7d36806250",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p code\n",
    "!cp -r ./inference/  ./code/\n",
    "!curl -L https://github.com/peak/s5cmd/releases/download/v2.0.0/s5cmd_2.0.0_Linux-64bit.tar.gz | tar -xz && mv s5cmd ./code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eb1c3db-6f79-4838-8527-68ca36516af2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!aws s3 cp s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-simple-2023-05-06-1-2023-05-06-14-24-12-728/output/model.tar.gz ./\n",
    "#!tar -xvf model.tar.gz\n",
    "#!aws s3 ls s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/\n",
    "#!chmod +x ./code/s5cmd && ./code/s5cmd sync s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed/adgen-chatglm-6b-ft/* ./tmp2/\n",
    "!rm -rf ./tmp2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf211e5-da35-4605-92f9-c336f8c165e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### local test only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20323c48-5ccb-42b5-9375-a55927f8dd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import io\n",
    "import sys\n",
    "\n",
    "import traceback\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import requests\n",
    "import boto3\n",
    "import sagemaker\n",
    "import torch\n",
    "\n",
    "\n",
    "from torch import autocast\n",
    "#from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    HfArgumentParser,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.replace(\"\\n\", \"\\\\n\").replace(\"\\t\", \"\\\\t\")\n",
    "    return text\n",
    "\n",
    "def postprocess(text):\n",
    "    return text.replace(\"\\\\n\", \"\\n\").replace(\"\\\\t\", \"\\t\")\n",
    "\n",
    "def answer(text, sample=True, top_p=0.45, temperature=0.7,model=None):\n",
    "    text = preprocess(text)\n",
    "    response, history = model.chat(tokenizer, text, history=[])\n",
    "    \n",
    "    return postprocess(response)\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    print(\"=================model_fn_Start=================\")\n",
    "    model_s3_path = os.environ['MODEL_S3_PATH']\n",
    "    print(\"=================model s3 path==================\"+model_s3_path)\n",
    "    os.system(\"./s5cmd sync {0} {1}\".format(model_s3_path+\"*\",\"./tmp/model/\"))\n",
    "    if os.environ[\"MODEL_TYPE\"] == \"ptuning\":\n",
    "        config = AutoConfig.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True, pre_seq_len=128)\n",
    "        model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", config=config, trust_remote_code=True)\n",
    "        prefix_state_dict = torch.load(os.path.join(\"./tmp/model/\", \"pytorch_model.bin\"))\n",
    "        new_prefix_state_dict = {}\n",
    "        for k, v in prefix_state_dict.items():\n",
    "            if k.startswith(\"transformer.prefix_encoder.\"):\n",
    "                new_prefix_state_dict[k[len(\"transformer.prefix_encoder.\"):]] = v\n",
    "        model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)\n",
    "\n",
    "    elif os.environ[\"MODEL_TYPE\"] == \"full turning\":\n",
    "        print(\"====================load full turning=================\") \n",
    "        config = AutoConfig.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True, pre_seq_len=128)\n",
    "        model = AutoModel.from_pretrained(\"./tmp/model/\",config=config, trust_remote_code=True)\n",
    "    else:\n",
    "        print(\"====================load normal ======================\")\n",
    "        config = AutoConfig.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True, pre_seq_len=128)\n",
    "        model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).half().cuda()\n",
    "\n",
    "    #model = model.to(\"cuda\")\n",
    "    model = model.quantize(4)\n",
    "    model = model.half().cuda()\n",
    "    model.transformer.prefix_encoder.float()\n",
    "    model = model.eval()\n",
    "    print(\"=================model_fn_End=================\")\n",
    "    return model\n",
    "\n",
    "model = model_fn(\"s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed/adgen-chatglm-6b-ft/\")\n",
    "result=answer(input_data['ask'], model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cbcac2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 设置模型推理参数\n",
    "1. model_type : p-turning or full tuning\n",
    "2. model_s3_path: fine tuned model s3 url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8b8a09a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"chatglm-inference\"\n",
    "entry_point = 'inference-chatglm.py'\n",
    "framework_version = '1.13.1'\n",
    "py_version = 'py39'\n",
    "model_environment = {\n",
    "    'SAGEMAKER_MODEL_SERVER_TIMEOUT':'600', \n",
    "    'SAGEMAKER_MODEL_SERVER_WORKERS': '1', \n",
    "    #'MODEL_TYPE'                    : 'ptuning',\n",
    "    #'MODEL_TYPE'                    : 'full turning',\n",
    "    'MODEL_TYPE'                    : 'normal',\n",
    "    #'MODEL_S3_PATH'                 : 's3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/'\n",
    "    'MODEL_S3_PATH'                 : 's3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b7d03288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "#model_data = \"s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-simple-2023-05-06-1-2023-05-06-14-24-12-728/output/model.tar.gz\"\n",
    "\n",
    "model = PyTorchModel(\n",
    "    name = model_name,\n",
    "    model_data = model_data,\n",
    "    entry_point = entry_point,\n",
    "    source_dir = './code',\n",
    "    role = role,\n",
    "    framework_version = framework_version, \n",
    "    py_version = py_version,\n",
    "    env = model_environment\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e130efd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 部署 SageMaker Endpoint\n",
    "部署推理服务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4253e7d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "endpoint_name = None\n",
    "instance_type = 'ml.g5.8xlarge'\n",
    "instance_count = 1\n",
    "\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "predictor = model.deploy(\n",
    "    endpoint_name = endpoint_name,\n",
    "    instance_type = instance_type, \n",
    "    initial_instance_count = instance_count,\n",
    "    serializer = JSONSerializer(),\n",
    "    deserializer = JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f59e3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ChatGLM 测试\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ada7d8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#休眠2分钟,确保模型可以完全加载\n",
    "import time\n",
    "time.sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "01eb2968",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！请问有什么需要帮助的吗？\n",
      "以下建议有助于改善晚上睡不着的情况：\n",
      "\n",
      "1. 保持规律的睡眠时间：尽量在同一时间上床睡觉，建立规律的睡眠时间，让身体适应固定的睡眠时间。\n",
      "\n",
      "2. 创造舒适的睡眠环境：保持卧室安静、凉爽、黑暗、舒适，使用舒适的床垫和枕头，确保睡眠环境符合个人需求。\n",
      "\n",
      "3. 放松身心：在睡前进行放松活动，如冥想、深呼吸、温水泡脚、听轻柔的音乐等，放松\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs= {\n",
    "    \"ask\": \"你好!\"\n",
    "\n",
    "}\n",
    "\n",
    "response = predictor.predict(inputs)\n",
    "print(response[\"answer\"])\n",
    "\n",
    "inputs= {\n",
    "    \"ask\": \"晚上睡不着应该怎么办\"\n",
    "\n",
    "}\n",
    "\n",
    "response = predictor.predict(inputs)\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "90edc6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以下是一些年夜饭好意头的菜肴以及其寓意：\n",
      "\n",
      "1. 鱼：年年有余，表示希望家中财源广进。在菜肴中以鲤鱼最常见，但也有人用鲈鱼、鲳鱼等鱼类制作。\n",
      "\n",
      "2. 年糕：年年高升，寓意着新年里生活越来越好，前程似锦。\n",
      "\n",
      "3. 香菇发菜：发财发福，吉祥如意，是象征财源广进的重要食材\n",
      "人工智能课程的教案示例如下：\n",
      "\n",
      "主题：人工智能的基础\n",
      "\n",
      "目标：让学生了解人工智能的基本概念、原理和应用领域，掌握使用Python编程语言进行人工智能开发的基本技能。\n",
      "\n",
      "教学内容：\n",
      "\n",
      "1. 人工智能的基本概念\n",
      "\n",
      "- 什么是人工智能\n",
      "- 人工智能的应用领域\n",
      "- 人工智能的特点\n",
      "\n",
      "2. Python编程语言\n",
      "\n",
      "- Python编程语言的基本语法\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "inputs= {\n",
    "    \"ask\": \"列出一些年夜饭好意头的菜肴以及其寓意!\"\n",
    "\n",
    "}\n",
    "\n",
    "response = predictor.predict(inputs)\n",
    "print(response[\"answer\"])\n",
    "\n",
    "inputs= {\n",
    "    \"ask\": \"帮我写一篇人工智能课程的教案，1000字\"\n",
    "\n",
    "}\n",
    "\n",
    "response = predictor.predict(inputs)\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "80082a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "民法典第一百零二条 当事人可以依据本法典第一百二十七条、第一百二十八条的规定条的规定，就特定民事关系达成书面协议。\n",
      "\n",
      "当事人达成的书面协议，符合本法典第一百二十七条或者第一百二十八条的规定的，应当作为确定民事关系的有效协议。\n",
      "\n",
      "当事人未达成书面协议，或者达成的协议不符合本法典第一百二十七条或者第一百二十八条的规定的，可以按照本\n"
     ]
    }
   ],
   "source": [
    "#inputs= {\n",
    "#    \"ask\": \"怎么修改huggingface transformers的model cache位置\"\n",
    "#\n",
    "#}\n",
    "#\n",
    "#response = predictor.predict(inputs)\n",
    "#print(response[\"answer\"])\n",
    "\n",
    "inputs= {\n",
    "    \"ask\": \"民法典第一百零二条\"\n",
    "    #\"ask\": \"我租的房子被房东卖了，现在要把我赶出去，我怎么办？\"\n",
    "\n",
    "}\n",
    "\n",
    "response = predictor.predict(inputs)\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f3ddf260",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chatglm-inference-2023-05-09-10-08-10-446'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#我们来查看一下刚部署的这个ChatGLM模型对应的SageMaker inference endpoint\n",
    "sagemaker_endpoint_name = predictor.endpoint_name\n",
    "sagemaker_endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa257dda",
   "metadata": {},
   "source": [
    "利用已经在SageMaker real time inference endpoint部署的ChatGLM模型来模拟单轮对话和多轮对话。\n",
    "\n",
    "下面的代码建议在SageMaker Notebook上来运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "078035cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "def query_endpoint_with_json_payload(encoded_json):\n",
    "    response = client.invoke_endpoint(EndpointName=sagemaker_endpoint_name, ContentType='application/json', Body=encoded_json)\n",
    "    return response\n",
    "\n",
    "def parse_response_texts(query_response):\n",
    "    model_predictions = json.loads(query_response['Body'].read())\n",
    "    generated_text = model_predictions[\"answer\"]\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f8af6b2b-3293-4e0b-ab92-71abef0a70d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chatglm-inference-2023-05-09-10-08-10-446'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7922fd8",
   "metadata": {},
   "source": [
    "先简单测试一下ChatGLM针对单个问题的回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "511e4712-3159-43a4-b62d-a7b98490fd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#payload = {\"ask\": \"信息抽取：\\n2022年世界杯的冠军是阿根廷队伍，梅西是MVP\\n问题：国家名，人名\\n答案：\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "55a5b2ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BL\n"
     ]
    }
   ],
   "source": [
    "payload1 = {\"ask\": \"\"\"问题:GPU优化作用？\n",
    "                      回答:一般来说，GPU无法在检索数据同时执行这些计算。此外，现代GPU的计算性能远远高于每个操作(被称为GPU编程中的核)所需的内存传输速度。\n",
    "核融合是一种基于GPU计算的优化方法，通过在一次内核调用中执行多个连续操作。该方法提供了一种最小化数据传输的方法：中间结果留在GPU寄存器中，而不是复制到VRAM，从而节省开销。\n",
    "我们使用了Megatron-LM提供了几个定制化融合CUDA核。首先，我们使用一个优化核来执行LayerNorm，以及用核来融合各种缩放、掩码和softmax操作的各种组合。\n",
    "使用Pytorch的JIT功能将一个偏差项添加至GeLU激活中。作为一个使用融合核的例子，在GeLU操作中添加偏差项不会增加额外的时间，因为该操作受内存限制：与GPU VRAM和寄存器之间的数据传输相比，额外的计算可以忽略不计。\n",
    "因此融合这两个操作基本上减少了它们的运行时间\n",
    "                     问题：主题摘要,限制在10个字内\n",
    "           \"\"\"}\n",
    "\n",
    "payload2 ={ \"ask\": \"\"\"问题：如何优化BLOOM？\n",
    "                      回答：我们使用上表3中详细描述的超参数来训练BLOOM的6个尺寸变体。\n",
    "架构和超参数来自于我们的实验结果(Le Scao et al.)和先前的训练大语言模型(Brown et al.)。\n",
    "非176B模型的深度和宽度大致遵循先前的文献(Brown et al.)，偏离的3B和7.1B只是为了更容易适合我们训练设置。\n",
    "由于更大的多语言词表，BLOOM的embedding参数尺寸更大。在开发104B参数模型的过程中，我们使用了不同的Adam \n",
    "参数、权重衰减和梯度裁剪来对目标稳定性进行实验，但没有发现其有帮助。\n",
    "对于所有模型，我们在410B tokens使用cosine学习率衰减调度，在计算允许的情况下，将其作为训练长度的上限，并对375M tokens进行warmup。\n",
    "我们使用权重衰减、梯度裁剪，不使用dropout。ROOTS数据集包含341B tokens的文本。然而，基于训练期间发布的修订scaling laws，我们决定在重复数据上对大模型进行额外25B tokens的训练。\n",
    "由于warmup tokens + decay tokens大于总的token数量，所以学习率衰减始终未达到终点\n",
    "问题：主题摘要,限制在20个字内\n",
    "\"\"\"}\n",
    "query_response = query_endpoint_with_json_payload(json.dumps(payload2).encode('utf-8'))\n",
    "generated_texts = parse_response_texts(query_response)\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "75dba41a-725f-4f23-9379-cbaa5d6b81bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以上属于闲聊问题。\n"
     ]
    }
   ],
   "source": [
    "#payload={\"ask\":\"\"\"好累啊，想去米亚罗看红叶\n",
    "#问题：内容分类,以上属于闲聊还是专业问题？\"\"\"}\n",
    "\n",
    "payload={\"ask\":\"\"\"新加坡有什么好玩的么？\n",
    "问题：内容分类,以上是闲聊还是提问专业问题？\"\"\"}\n",
    "\n",
    "#payload={\"ask\":\"\"\"AWS是什么公司？\n",
    "#问题：内容分类,以上是闲聊还是提问专业问题？\"\"\"}\n",
    "query_response = query_endpoint_with_json_payload(json.dumps(payload).encode('utf-8'))\n",
    "generated_texts = parse_response_texts(query_response)\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81504f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 单轮对话：每个问题/query都是独立的，问题之间没有关联性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82254c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.首先需要一个简单的开场拍。\n",
    "print(\"用户：你好！\\nChatGLM：您好!我是ChatGLM。我可以回答您的问题、写文章、写作业、翻译，对于一些法律等领域的问题我也可以给你提供信息。\")\n",
    "\n",
    "#2.在同一个session中持续对话，但是每次对话之间没有什么关联。\n",
    "while True:\n",
    "    command = input(\"用户：\")\n",
    "    if command == 'quit':\n",
    "        break;\n",
    "    \n",
    "    #print(command)\n",
    "    payload = {\"ask\": \"\\n用户：\"+ command + \"\\n\"}\n",
    "    #print(payload[\"ask\"])\n",
    "    query_response = query_endpoint_with_json_payload(json.dumps(payload).encode('utf-8'))\n",
    "    generated_texts = \"ChatGLM：\" + parse_response_texts(query_response)\n",
    "    print(generated_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326def35",
   "metadata": {},
   "source": [
    "## 多轮对话模拟：我们这里来测试一下ChatGLM的多轮对话能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b169581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.首先需要开场拍来引导ChatGLM，其实就是给它一个上下文来启动所谓的对话session。\n",
    "payload = {\"ask\": \"用户：你好！\\nChatGLM：您好!我是ChatGLM。我可以回答您的问题、写文章、写作业、翻译，对于一些法律等领域的问题我也可以给你提供信息。\"}\n",
    "print(payload[\"ask\"])\n",
    "generated_texts = \"\"\n",
    "\n",
    "#在这里简单模拟多轮对话时，发送给SageMaker endpoint的payload会越来越大，这里对payload大致做一个限制。\n",
    "session_len = 10 * 1024 * 1024 \n",
    "\n",
    "#2.在同一个session中持续对话，为了有多轮对话的效果，把每一轮的信息(问题-回答对)都带上来告诉ChatGLM之前的上下文。\n",
    "while True:\n",
    "    command = input(\"用户：\")\n",
    "    if command == 'quit':\n",
    "        break;\n",
    "    \n",
    "    #print(command)\n",
    "    whole_context = payload[\"ask\"] + generated_texts + \"\\n用户：\"+ command + \"\\n\"\n",
    "    payload = {\"ask\": whole_context}\n",
    "    if len(whole_context) > session_len:\n",
    "        print(\"上下文信息太多了，当前对话session要退出了！\")\n",
    "        break;\n",
    "    #print(payload[\"ask\"])\n",
    "    query_response = query_endpoint_with_json_payload(json.dumps(payload).encode('utf-8'))\n",
    "    generated_texts = \"ChatGLM：\" + parse_response_texts(query_response)\n",
    "    print(generated_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2dcc6a",
   "metadata": {},
   "source": [
    "### 删除SageMaker  Endpoint\n",
    "删除推理服务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c329e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c512b41c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
