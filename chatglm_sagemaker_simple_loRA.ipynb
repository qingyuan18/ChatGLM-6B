{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "252de0de",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SageMaker fine tune ChatGLM\n",
    "\n",
    "#### 准备\n",
    "1. 升级boto3, sagemaker python sdk  \n",
    "2. 准备requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8f2c403",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (1.26.71)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.26.116-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3) (0.6.0)\n",
      "Collecting botocore<1.30.0,>=1.29.116\n",
      "  Downloading botocore-1.29.116-py3-none-any.whl (10.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.116->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.116->boto3) (1.26.8)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.116->boto3) (1.16.0)\n",
      "Installing collected packages: botocore, boto3\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.29.71\n",
      "    Uninstalling botocore-1.29.71:\n",
      "      Successfully uninstalled botocore-1.29.71\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.26.71\n",
      "    Uninstalling boto3-1.26.71:\n",
      "      Successfully uninstalled boto3-1.26.71\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.27.71 requires botocore==1.29.71, but you have botocore 1.29.116 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.26.116 botocore-1.29.116\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (2.132.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.148.0.tar.gz (743 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m743.3/743.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<23,>=20.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (22.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.28 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (1.26.116)\n",
      "Collecting cloudpickle==2.2.1\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (1.23.5)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (3.20.2)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (4.13.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (1.4.4)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML==5.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (5.4.1)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (3.2.0)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (2.6.2)\n",
      "Requirement already satisfied: tblib==1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.116 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.29.116)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from jsonschema->sagemaker) (0.19.3)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from jsonschema->sagemaker) (65.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pandas->sagemaker) (2022.7)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.116->boto3<2.0,>=1.26.28->sagemaker) (1.26.8)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.148.0-py2.py3-none-any.whl size=998496 sha256=60972853c9019b0256fc91cbb9a01495e4e968902662ea7a9093c45a60210476\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/66/36/07/56de705f4ad8ea09e40419f57f8478bc60aae5b1e095dda1f0\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: cloudpickle, sagemaker\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 2.2.0\n",
      "    Uninstalling cloudpickle-2.2.0:\n",
      "      Successfully uninstalled cloudpickle-2.2.0\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.132.0\n",
      "    Uninstalling sagemaker-2.132.0:\n",
      "      Successfully uninstalled sagemaker-2.132.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "distributed 2022.11.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cloudpickle-2.2.1 sagemaker-2.148.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade boto3\n",
    "!pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4a30f3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "arn:aws:iam::687912291502:role/service-role/AmazonSageMaker-ExecutionRole-20211013T113123\n",
      "sagemaker-us-west-2-687912291502\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "account = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(role)\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f59e3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### chatglm lora方式（单机单卡）\n",
    "1: 使用羊驼语料数据  \n",
    "2：语料处理，tokenization及label标注  \n",
    "3：HF trainer API  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9db2f288-0610-4b18-9409-c8ef5ee91ca0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ChatGLM-Tuning'...\n",
      "remote: Enumerating objects: 158, done.\u001b[K\n",
      "remote: Counting objects: 100% (123/123), done.\u001b[K\n",
      "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
      "remote: Total 158 (delta 83), reused 68 (delta 68), pack-reused 35 (from 1)\u001b[K\n",
      "Receiving objects: 100% (158/158), 8.02 MiB | 24.16 MiB/s, done.\n",
      "Resolving deltas: 100% (85/85), done.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./ChatGLM-Tuning\n",
    "!git clone https://github.com/qingyuan18/ChatGLM-Tuning.git\n",
    "!cp ./s5cmd ./ChatGLM-Tuning/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42e2794-2ed4-49f9-b1b0-054f1a0af54f",
   "metadata": {},
   "source": [
    "## prepare docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8aceb270-324f-445e-ae84-0da1d1b98adf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "## You should change below region code to the region you used, here sample is use us-west-2\n",
    "From 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04 \n",
    "#From pytorch/pytorch:1.5-cuda10.1-cudnn7-runtime\n",
    "\n",
    "ENV LANG=C.UTF-8\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
    "\n",
    "RUN mkdir -p /opt/\n",
    "COPY ./ChatGLM-Tuning/ /opt/ChatGLM-Tuning/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b8ee553",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "## You should change below region code to the region you used, here sample is use us-west-2\n",
    "!aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-west-2.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00b23c6-6e88-48f2-96dd-99140c147be5",
   "metadata": {},
   "source": [
    "**Build image and push to ECR.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa970133-14f1-40d4-963f-895154a43f94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## define repo name, should contain *sagemaker* in the name\n",
    "repo_name = \"sagemaker-chatglm-lora-demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51c2d43e-ff0f-4718-b772-555c95d6aaaf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  110.6MB\n",
      "Step 1/6 : From 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04\n",
      " ---> 5e32024fab41\n",
      "Step 2/6 : ENV LANG=C.UTF-8\n",
      " ---> Using cache\n",
      " ---> 342d0877edf9\n",
      "Step 3/6 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> 1d0e79caa085\n",
      "Step 4/6 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> a95c9716ab77\n",
      "Step 5/6 : RUN mkdir -p /opt/\n",
      " ---> Using cache\n",
      " ---> 876c58d28008\n",
      "Step 6/6 : COPY ./ChatGLM-Tuning/ /opt/ChatGLM-Tuning/\n",
      " ---> 978e526cc71d\n",
      "Successfully built 978e526cc71d\n",
      "Successfully tagged sagemaker-chatglm-lora-demo:latest\n",
      "The push refers to repository [687912291502.dkr.ecr.us-west-2.amazonaws.com/sagemaker-chatglm-lora-demo]\n",
      "74cf4fadd42f: Preparing\n",
      "879479285a9b: Preparing\n",
      "4b6461494ed0: Preparing\n",
      "144ed3c716a5: Preparing\n",
      "2132dc88592f: Preparing\n",
      "ef7125ccaa23: Preparing\n",
      "b838e4f50aeb: Preparing\n",
      "ca6fb9204b2d: Preparing\n",
      "89621c6faf10: Preparing\n",
      "9d5ea523f8b9: Preparing\n",
      "0ef91bf60058: Preparing\n",
      "3a7fc6bca624: Preparing\n",
      "1aa8e5c8acb3: Preparing\n",
      "4f06eba78c04: Preparing\n",
      "5e708cf41b4b: Preparing\n",
      "4f7718951951: Preparing\n",
      "589952c94fc4: Preparing\n",
      "55a67cca12ec: Preparing\n",
      "862ba9d4fce0: Preparing\n",
      "0788634e5e5b: Preparing\n",
      "e3b4362c8b5e: Preparing\n",
      "e825af28a219: Preparing\n",
      "7d6ef0e24b77: Preparing\n",
      "e5c670865646: Preparing\n",
      "3a7abe69699a: Preparing\n",
      "f858460f1f6b: Preparing\n",
      "9d5ea523f8b9: Waiting\n",
      "ba682ea6d164: Preparing\n",
      "0ef91bf60058: Waiting\n",
      "3a7fc6bca624: Waiting\n",
      "2a2e05ec49fc: Preparing\n",
      "1aa8e5c8acb3: Waiting\n",
      "17c8e4be725c: Preparing\n",
      "4f06eba78c04: Waiting\n",
      "5e708cf41b4b: Waiting\n",
      "fef7c202f114: Preparing\n",
      "2ab01898da8d: Preparing\n",
      "4f7718951951: Waiting\n",
      "589952c94fc4: Waiting\n",
      "1143f1a5dc76: Preparing\n",
      "55a67cca12ec: Waiting\n",
      "c73a44f1f9d6: Preparing\n",
      "862ba9d4fce0: Waiting\n",
      "0788634e5e5b: Waiting\n",
      "ed710019563e: Preparing\n",
      "e3b4362c8b5e: Waiting\n",
      "ee65472fc584: Preparing\n",
      "c325ef7e7607: Preparing\n",
      "2c0da3ee5341: Preparing\n",
      "cac3780e38e3: Preparing\n",
      "ad5cabf15c69: Preparing\n",
      "26e4a1d7380b: Preparing\n",
      "e825af28a219: Waiting\n",
      "2a2e05ec49fc: Waiting\n",
      "9843de4a4912: Preparing\n",
      "007aff51bc24: Preparing\n",
      "17c8e4be725c: Waiting\n",
      "fef7c202f114: Waiting\n",
      "8ad2fd5a3f1e: Preparing\n",
      "2ab01898da8d: Waiting\n",
      "a86626e42a96: Preparing\n",
      "1143f1a5dc76: Waiting\n",
      "e9d972ae6022: Preparing\n",
      "7d6ef0e24b77: Waiting\n",
      "c73a44f1f9d6: Waiting\n",
      "5c995fa0fd92: Preparing\n",
      "ed710019563e: Waiting\n",
      "e5c670865646: Waiting\n",
      "47d02c4e2a4a: Preparing\n",
      "f858460f1f6b: Waiting\n",
      "ee65472fc584: Waiting\n",
      "3a7abe69699a: Waiting\n",
      "55faa0189df1: Preparing\n",
      "c325ef7e7607: Waiting\n",
      "74b8bb426725: Preparing\n",
      "ba682ea6d164: Waiting\n",
      "01b2ce3312d7: Preparing\n",
      "ef7125ccaa23: Waiting\n",
      "b1d31bd8950c: Preparing\n",
      "b838e4f50aeb: Waiting\n",
      "6c3e7df31590: Preparing\n",
      "8ad2fd5a3f1e: Waiting\n",
      "2c0da3ee5341: Waiting\n",
      "26e4a1d7380b: Waiting\n",
      "ad5cabf15c69: Waiting\n",
      "a86626e42a96: Waiting\n",
      "ca6fb9204b2d: Waiting\n",
      "5c995fa0fd92: Waiting\n",
      "cac3780e38e3: Waiting\n",
      "47d02c4e2a4a: Waiting\n",
      "55faa0189df1: Waiting\n",
      "9843de4a4912: Waiting\n",
      "b1d31bd8950c: Waiting\n",
      "74b8bb426725: Waiting\n",
      "e9d972ae6022: Waiting\n",
      "01b2ce3312d7: Waiting\n",
      "007aff51bc24: Waiting\n",
      "6c3e7df31590: Waiting\n",
      "89621c6faf10: Waiting\n",
      "2132dc88592f: Layer already exists\n",
      "144ed3c716a5: Layer already exists\n",
      "879479285a9b: Layer already exists\n",
      "4b6461494ed0: Layer already exists\n",
      "ef7125ccaa23: Layer already exists\n",
      "b838e4f50aeb: Layer already exists\n",
      "89621c6faf10: Layer already exists\n",
      "ca6fb9204b2d: Layer already exists\n",
      "9d5ea523f8b9: Layer already exists\n",
      "3a7fc6bca624: Layer already exists\n",
      "1aa8e5c8acb3: Layer already exists\n",
      "0ef91bf60058: Layer already exists\n",
      "4f06eba78c04: Layer already exists\n",
      "5e708cf41b4b: Layer already exists\n",
      "4f7718951951: Layer already exists\n",
      "589952c94fc4: Layer already exists\n",
      "55a67cca12ec: Layer already exists\n",
      "862ba9d4fce0: Layer already exists\n",
      "0788634e5e5b: Layer already exists\n",
      "e3b4362c8b5e: Layer already exists\n",
      "7d6ef0e24b77: Layer already exists\n",
      "e5c670865646: Layer already exists\n",
      "e825af28a219: Layer already exists\n",
      "3a7abe69699a: Layer already exists\n",
      "f858460f1f6b: Layer already exists\n",
      "ba682ea6d164: Layer already exists\n",
      "17c8e4be725c: Layer already exists\n",
      "2a2e05ec49fc: Layer already exists\n",
      "fef7c202f114: Layer already exists\n",
      "2ab01898da8d: Layer already exists\n",
      "1143f1a5dc76: Layer already exists\n",
      "c73a44f1f9d6: Layer already exists\n",
      "ed710019563e: Layer already exists\n",
      "c325ef7e7607: Layer already exists\n",
      "ee65472fc584: Layer already exists\n",
      "2c0da3ee5341: Layer already exists\n",
      "cac3780e38e3: Layer already exists\n",
      "ad5cabf15c69: Layer already exists\n",
      "26e4a1d7380b: Layer already exists\n",
      "9843de4a4912: Layer already exists\n",
      "007aff51bc24: Layer already exists\n",
      "8ad2fd5a3f1e: Layer already exists\n",
      "a86626e42a96: Layer already exists\n",
      "e9d972ae6022: Layer already exists\n",
      "5c995fa0fd92: Layer already exists\n",
      "47d02c4e2a4a: Layer already exists\n",
      "74b8bb426725: Layer already exists\n",
      "55faa0189df1: Layer already exists\n",
      "01b2ce3312d7: Layer already exists\n",
      "b1d31bd8950c: Layer already exists\n",
      "6c3e7df31590: Layer already exists\n",
      "74cf4fadd42f: Pushed\n",
      "latest: digest: sha256:ab5d67018a27e85952ce5085167c274a39afe7bed5218d4083ffae6e036a18bc size: 11262\n"
     ]
    }
   ],
   "source": [
    "%%script env repo_name=$repo_name bash\n",
    "\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# This script shows how to build the Docker image and push it to ECR to be ready for use\n",
    "# by SageMaker.\n",
    "\n",
    "# The argument to this script is the image name. This will be used as the image on the local\n",
    "# machine and combined with the account and region to form the repository name for ECR.\n",
    "# The name of our algorithm\n",
    "algorithm_name=${repo_name}\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcb4cf6-4413-42b8-9ec1-9dcb9dcde793",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55cedcb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./ChatGLM-Tuning/train.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./ChatGLM-Tuning/train.sh\n",
    "#!/bin/bash\n",
    "\n",
    "chmod +x ./s5cmd\n",
    "pip install -r ./requirements.txt\n",
    "\n",
    "###转化alpaca数据集为jsonl\n",
    "python ./cover_alpaca2jsonl.py \\\n",
    "    --data_path ./data/alpaca_data.json \\\n",
    "    --save_path ./data/alpaca_data.jsonl \n",
    "\n",
    "#tokenization\n",
    "python ./tokenize_dataset_rows.py \\\n",
    "    --jsonl_path ./data/alpaca_data.jsonl \\\n",
    "    --save_path ./data/alpaca \\\n",
    "    --max_seq_length 200\n",
    "\n",
    "python ./finetune.py \\\n",
    "    --dataset_path ./data/alpaca \\\n",
    "    --lora_rank 8 \\\n",
    "    --per_device_train_batch_size 6 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --max_steps 1000 \\\n",
    "    --save_steps 1000 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --fp16 \\\n",
    "    --remove_unused_columns false \\\n",
    "    --logging_steps 50 \\\n",
    "    --output_dir /tmp/output\n",
    "\n",
    "./s5cmd sync /tmp/output/ s3://$MODEL_S3_BUCKET/models/chatglm-lora/output/$(date +%Y-%m-%d-%H-%M-%S)/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91dc3c13-ba5c-41b9-9682-11dd61cc8382",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'687912291502.dkr.ecr.us-west-2.amazonaws.com/sagemaker-chatglm-lora-demo:latest'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The image uri which is build and pushed above\n",
    "image_uri = \"{}.dkr.ecr.{}.amazonaws.com/{}:latest\".format(account, region, repo_name)\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab36100",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: chatglm-lora-demo-2024-09-25-11-19-30-765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-25 11:19:38 Starting - Starting the training job\n",
      "2024-09-25 11:19:38 Pending - Training job waiting for capacity......\n",
      "2024-09-25 11:20:20 Pending - Preparing the instances for training...\n",
      "2024-09-25 11:21:04 Downloading - Downloading input data...\n",
      "2024-09-25 11:21:19 Downloading - Downloading the training image.....................\n",
      "2024-09-25 11:25:01 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m2024-09-25 11:25:18,456 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-09-25 11:25:18,516 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-09-25 11:25:18,531 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-09-25 11:25:18,534 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-09-25 11:25:20,661 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-09-25 11:25:20,756 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-09-25 11:25:20,850 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-09-25 11:25:20,874 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"chatglm-lora-demo-2024-09-25-11-19-30-765\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-687912291502/chatglm-lora-demo-2024-09-25-11-19-30-765/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.sh\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.sh\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train.sh\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-687912291502/chatglm-lora-demo-2024-09-25-11-19-30-765/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"is_smddprun_installed\":true,\"job_name\":\"chatglm-lora-demo-2024-09-25-11-19-30-765\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-687912291502/chatglm-lora-demo-2024-09-25-11-19-30-765/source/sourcedir.tar.gz\",\"module_name\":\"train.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.sh\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/bin/sh -c \"./train.sh \"\u001b[0m\n",
      "\u001b[34m2024-09-25 11:25:20,911 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mCollecting git+https://github.com/huggingface/peft.git (from -r ./requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mCloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-q3f83yo0\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-q3f83yo0\u001b[0m\n",
      "\u001b[34mResolved https://github.com/huggingface/peft.git to commit f0b066eae888d5dea598e756b7e6d3401d0708e7\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes==0.37.1 (from -r ./requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.37.1-py3-none-any.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from -r ./requirements.txt (line 3)) (0.33.0)\u001b[0m\n",
      "\u001b[34mCollecting protobuf<3.20.1,>=3.19.5 (from -r ./requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading protobuf-3.20.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (698 bytes)\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.27.1 (from -r ./requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.27.1-py3-none-any.whl.metadata (106 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.7/106.7 kB 4.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting icetk (from -r ./requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading icetk-0.0.7-py3-none-any.whl.metadata (2.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting cpm_kernels==1.0.11 (from -r ./requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading cpm_kernels-1.0.11-py3-none-any.whl.metadata (1.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.13.1 in /opt/conda/lib/python3.9/site-packages (from -r ./requirements.txt (line 10)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard (from -r ./requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets in /opt/conda/lib/python3.9/site-packages (from -r ./requirements.txt (line 14)) (2.16.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r ./requirements.txt (line 7)) (3.15.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r ./requirements.txt (line 7)) (0.24.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r ./requirements.txt (line 7)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r ./requirements.txt (line 7)) (24.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r ./requirements.txt (line 7)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r ./requirements.txt (line 7)) (2024.7.24)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r ./requirements.txt (line 7)) (2.32.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r ./requirements.txt (line 7)) (0.13.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r ./requirements.txt (line 7)) (4.66.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate->-r ./requirements.txt (line 3)) (5.9.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.9/site-packages (from accelerate->-r ./requirements.txt (line 3)) (0.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision in /opt/conda/lib/python3.9/site-packages (from icetk->-r ./requirements.txt (line 8)) (0.14.1+cu117)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.9/site-packages (from icetk->-r ./requirements.txt (line 8)) (0.2.0)\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of icetk to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mCollecting icetk (from -r ./requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading icetk-0.0.6-py3-none-any.whl.metadata (2.0 kB)\u001b[0m\n",
      "\u001b[34mDownloading icetk-0.0.5-py3-none-any.whl.metadata (2.0 kB)\u001b[0m\n",
      "\u001b[34mDownloading icetk-0.0.4-py3-none-any.whl.metadata (2.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.13.1->-r ./requirements.txt (line 10)) (4.12.2)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4 (from tensorboard->-r ./requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.48.2 (from tensorboard->-r ./requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.66.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r ./requirements.txt (line 11)) (3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r ./requirements.txt (line 11)) (71.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r ./requirements.txt (line 11)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->-r ./requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r ./requirements.txt (line 11)) (3.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets->-r ./requirements.txt (line 14)) (14.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.9/site-packages (from datasets->-r ./requirements.txt (line 14)) (0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets->-r ./requirements.txt (line 14)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets->-r ./requirements.txt (line 14)) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets->-r ./requirements.txt (line 14)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets->-r ./requirements.txt (line 14)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.9/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets->-r ./requirements.txt (line 14)) (2023.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets->-r ./requirements.txt (line 14)) (3.10.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r ./requirements.txt (line 14)) (2.3.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r ./requirements.txt (line 14)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r ./requirements.txt (line 14)) (23.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r ./requirements.txt (line 14)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r ./requirements.txt (line 14)) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r ./requirements.txt (line 14)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r ./requirements.txt (line 14)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard->-r ./requirements.txt (line 11)) (6.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.1->-r ./requirements.txt (line 7)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.1->-r ./requirements.txt (line 7)) (3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.1->-r ./requirements.txt (line 7)) (1.26.19)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.1->-r ./requirements.txt (line 7)) (2024.7.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard->-r ./requirements.txt (line 11)) (2.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->-r ./requirements.txt (line 14)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->-r ./requirements.txt (line 14)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->-r ./requirements.txt (line 14)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.9/site-packages (from torchvision->icetk->-r ./requirements.txt (line 8)) (10.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r ./requirements.txt (line 11)) (3.19.2)\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.37.1-py3-none-any.whl (76.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.3/76.3 MB 26.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.7/6.7 MB 94.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 416.6/416.6 kB 35.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading protobuf-3.20.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 66.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading icetk-0.0.4-py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 99.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.7/133.7 kB 23.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.66.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.8/5.8 MB 102.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 103.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: peft\u001b[0m\n",
      "\u001b[34mBuilding wheel for peft (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for peft (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for peft: filename=peft-0.13.0-py3-none-any.whl size=320662 sha256=6d0916a0c9c62f88005a6b81b04192ebf489175ff52eca97b3da6306d77ee89b\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-xsx_fjag/wheels/2d/60/1b/0edd9dc0f0c489738b1166bc1b0b560ee368f7721f89d06e3a\u001b[0m\n",
      "\u001b[34mSuccessfully built peft\u001b[0m\n",
      "\u001b[34mInstalling collected packages: cpm_kernels, bitsandbytes, tensorboard-data-server, protobuf, grpcio, absl-py, transformers, tensorboard, icetk, peft\u001b[0m\n",
      "\u001b[34mAttempting uninstall: protobuf\u001b[0m\n",
      "\u001b[34mFound existing installation: protobuf 3.20.2\u001b[0m\n",
      "\u001b[34mUninstalling protobuf-3.20.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled protobuf-3.20.2\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.26.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.26.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.26.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-2.1.0 bitsandbytes-0.37.1 cpm_kernels-1.0.11 grpcio-1.66.1 icetk-0.0.4 peft-0.13.0 protobuf-3.20.0 tensorboard-2.17.1 tensorboard-data-server-0.7.2 transformers-4.27.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 24.1.2 -> 24.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mformatting..:   0%|          | 0/52002 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mformatting..:  33%|███▎      | 17206/52002 [00:00<00:00, 172048.27it/s]\u001b[0m\n",
      "\u001b[34mformatting..:  67%|██████▋   | 34851/52002 [00:00<00:00, 174628.49it/s]\u001b[0m\n",
      "\u001b[34mformatting..: 100%|██████████| 52002/52002 [00:00<00:00, 175195.30it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mExplicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mExplicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m0%|          | 0/52002 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 1 examples [00:02,  2.68s/ examples]\u001b[0m\n",
      "\u001b[34m0%|          | 137/52002 [00:00<00:37, 1369.50it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 141 examples [00:02, 71.13 examples/s]\u001b[0m\n",
      "\u001b[34m1%|          | 289/52002 [00:00<00:35, 1455.70it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 293 examples [00:02, 168.67 examples/s]\u001b[0m\n",
      "\u001b[34m1%|          | 437/52002 [00:00<00:35, 1464.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 441 examples [00:02, 283.50 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 575 examples [00:03, 400.71 examples/s]\u001b[0m\n",
      "\u001b[34m1%|          | 584/52002 [00:00<00:36, 1413.34it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 717 examples [00:03, 539.85 examples/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 726/52002 [00:00<00:36, 1407.59it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 864 examples [00:03, 691.76 examples/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 874/52002 [00:00<00:35, 1431.38it/s]#033[A\u001b[0m\n",
      "\u001b[34m2%|▏         | 1018/52002 [00:00<00:37, 1376.71it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 1076 examples [00:03, 868.27 examples/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 1174/52002 [00:00<00:35, 1431.49it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 1229 examples [00:03, 997.72 examples/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 1322/52002 [00:00<00:35, 1443.34it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 1375 examples [00:03, 1095.34 examples/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 1468/52002 [00:01<00:34, 1448.05it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 1528 examples [00:03, 1194.89 examples/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 1614/52002 [00:01<00:35, 1413.23it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 1729 examples [00:03, 1240.43 examples/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 1757/52002 [00:01<00:35, 1416.59it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 1872 examples [00:03, 1284.53 examples/s]\u001b[0m\n",
      "\u001b[34m4%|▎         | 1900/52002 [00:01<00:35, 1417.77it/s]#033[A\u001b[0m\n",
      "\u001b[34m4%|▍         | 2042/52002 [00:01<00:36, 1368.31it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 2073 examples [00:04, 1297.46 examples/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 2195/52002 [00:01<00:35, 1414.73it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 2226 examples [00:04, 1351.48 examples/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 2353/52002 [00:01<00:33, 1460.97it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 2380 examples [00:04, 1397.31 examples/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 2500/52002 [00:01<00:34, 1428.75it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 2596 examples [00:04, 1405.61 examples/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 2645/52002 [00:01<00:34, 1429.98it/s]#033[A\u001b[0m\n",
      "\u001b[34m5%|▌         | 2789/52002 [00:02<00:39, 1231.92it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 2793 examples [00:04, 1253.90 examples/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 2929/52002 [00:02<00:38, 1275.31it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 2935 examples [00:04, 1287.45 examples/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 3061/52002 [00:02<00:38, 1282.14it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 3070 examples [00:04, 1278.37 examples/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 3205/52002 [00:02<00:36, 1324.78it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 3219 examples [00:05, 1328.76 examples/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 3340/52002 [00:02<00:36, 1323.73it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 3427 examples [00:05, 1345.38 examples/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 3480/52002 [00:02<00:36, 1344.07it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 3564 examples [00:05, 1348.78 examples/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 3616/52002 [00:02<00:36, 1331.87it/s]#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 3763/52002 [00:02<00:35, 1370.01it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 3774 examples [00:05, 1361.70 examples/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 3910/52002 [00:02<00:34, 1395.51it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 3917 examples [00:05, 1376.18 examples/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 4051/52002 [00:02<00:36, 1322.52it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 4102 examples [00:05, 1321.49 examples/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 4196/52002 [00:03<00:35, 1358.80it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 4256 examples [00:05, 1373.58 examples/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 4349/52002 [00:03<00:33, 1407.39it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 4400 examples [00:05, 1388.24 examples/s]\u001b[0m\n",
      "\u001b[34m9%|▊         | 4491/52002 [00:03<00:33, 1401.80it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 4545 examples [00:05, 1403.88 examples/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 4633/52002 [00:03<00:33, 1405.94it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 4758 examples [00:06, 1408.24 examples/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 4781/52002 [00:03<00:33, 1426.33it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 4901 examples [00:06, 1409.83 examples/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 4924/52002 [00:03<00:33, 1413.16it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m10%|▉         | 5066/52002 [00:03<00:34, 1377.51it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 5103 examples [00:06, 1382.86 examples/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 5205/52002 [00:03<00:33, 1378.81it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 5312 examples [00:06, 1383.40 examples/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 5344/52002 [00:03<00:34, 1363.77it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 5456 examples [00:06, 1395.52 examples/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 5499/52002 [00:03<00:32, 1416.49it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 5611 examples [00:06, 1432.57 examples/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 5650/52002 [00:04<00:32, 1443.51it/s]#033[A\u001b[0m\n",
      "\u001b[34m11%|█         | 5795/52002 [00:04<00:32, 1420.48it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 5825 examples [00:06, 1424.58 examples/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 5938/52002 [00:04<00:33, 1380.88it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 6005 examples [00:07, 1345.90 examples/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 6077/52002 [00:04<00:34, 1341.58it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 6147 examples [00:07, 1361.12 examples/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 6224/52002 [00:04<00:33, 1377.26it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 6288 examples [00:07, 1371.01 examples/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 6363/52002 [00:04<00:33, 1370.69it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 6430 examples [00:07, 1382.82 examples/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 6501/52002 [00:04<00:33, 1373.26it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 6636 examples [00:07, 1374.57 examples/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 6639/52002 [00:04<00:33, 1369.29it/s]#033[A\u001b[0m\n",
      "\u001b[34m13%|█▎        | 6777/52002 [00:04<00:33, 1367.46it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 6836 examples [00:07, 1354.16 examples/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 6914/52002 [00:05<00:33, 1358.57it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 6977 examples [00:07, 1364.21 examples/s]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 7050/52002 [00:05<00:34, 1297.15it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 7162 examples [00:07, 1315.47 examples/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 7183/52002 [00:05<00:34, 1303.88it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 7303 examples [00:07, 1336.34 examples/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 7328/52002 [00:05<00:33, 1341.66it/s]#033[A\u001b[0m\n",
      "\u001b[34m14%|█▍        | 7463/52002 [00:05<00:33, 1317.26it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 7498 examples [00:08, 1318.99 examples/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 7610/52002 [00:05<00:32, 1361.27it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 7645 examples [00:08, 1354.12 examples/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 7752/52002 [00:05<00:32, 1373.43it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 7786 examples [00:08, 1365.41 examples/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 7890/52002 [00:05<00:33, 1323.30it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 7988 examples [00:08, 1355.96 examples/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 8023/52002 [00:05<00:33, 1305.63it/s]#033[A\u001b[0m\n",
      "\u001b[34m16%|█▌        | 8168/52002 [00:05<00:32, 1343.79it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 8182 examples [00:08, 1333.33 examples/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 8303/52002 [00:06<00:32, 1342.73it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 8376 examples [00:08, 1319.30 examples/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 8438/52002 [00:06<00:32, 1337.98it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 8521 examples [00:08, 1348.20 examples/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 8590/52002 [00:06<00:31, 1390.13it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 8663 examples [00:08, 1364.68 examples/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 8730/52002 [00:06<00:31, 1384.09it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 8812 examples [00:09, 1396.35 examples/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 8888/52002 [00:06<00:29, 1440.71it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 8971 examples [00:09, 1444.69 examples/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 9033/52002 [00:06<00:31, 1385.34it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 9154 examples [00:09, 1356.70 examples/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 9173/52002 [00:06<00:31, 1361.23it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 9304 examples [00:09, 1390.80 examples/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 9323/52002 [00:06<00:30, 1399.04it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 9460 examples [00:09, 1435.14 examples/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 9482/52002 [00:06<00:29, 1454.50it/s]#033[A\u001b[0m\n",
      "\u001b[34m19%|█▊        | 9628/52002 [00:06<00:30, 1380.09it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 9651 examples [00:09, 1370.16 examples/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 9768/52002 [00:07<00:30, 1369.37it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 9848 examples [00:09, 1345.52 examples/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 9906/52002 [00:07<00:31, 1341.69it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 9990 examples [00:09, 1362.55 examples/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 10041/52002 [00:07<00:31, 1322.71it/s]#033[A\u001b[0m\n",
      "\u001b[34m20%|█▉        | 10180/52002 [00:07<00:31, 1341.95it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 10186 examples [00:10, 1339.00 examples/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 10324/52002 [00:07<00:30, 1367.92it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 10330 examples [00:10, 1362.69 examples/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 10462/52002 [00:07<00:30, 1368.58it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 10539 examples [00:10, 1371.93 examples/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 10607/52002 [00:07<00:29, 1391.55it/s]#033[A\u001b[0m\n",
      "\u001b[34m21%|██        | 10747/52002 [00:07<00:29, 1381.05it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 10750 examples [00:10, 1381.65 examples/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 10887/52002 [00:07<00:29, 1382.07it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 10889 examples [00:10, 1380.38 examples/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 11026/52002 [00:08<00:30, 1349.21it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 11085 examples [00:10, 1351.48 examples/s]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 11162/52002 [00:08<00:30, 1331.89it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 11273 examples [00:10, 1316.84 examples/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 11296/52002 [00:08<00:31, 1308.08it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 11423 examples [00:11, 1357.47 examples/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 11447/52002 [00:08<00:29, 1363.14it/s]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 11584/52002 [00:08<00:30, 1341.47it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 11622 examples [00:11, 1344.84 examples/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 11728/52002 [00:08<00:29, 1369.60it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 11764 examples [00:11, 1357.99 examples/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 11866/52002 [00:08<00:29, 1344.64it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 11964 examples [00:11, 1344.48 examples/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 12001/52002 [00:08<00:30, 1311.93it/s]#033[A\u001b[0m\n",
      "\u001b[34m23%|██▎       | 12136/52002 [00:08<00:30, 1321.33it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 12157 examples [00:11, 1322.92 examples/s]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 12282/52002 [00:08<00:29, 1359.87it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 12303 examples [00:11, 1354.25 examples/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 12423/52002 [00:09<00:28, 1372.74it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 12444 examples [00:11, 1365.33 examples/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 12567/52002 [00:09<00:28, 1390.07it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 12585 examples [00:11, 1374.27 examples/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 12707/52002 [00:09<00:28, 1383.93it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 12724 examples [00:11, 1376.19 examples/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 12846/52002 [00:09<00:28, 1378.62it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 12863 examples [00:12, 1375.98 examples/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 12984/52002 [00:09<00:28, 1377.99it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 13065 examples [00:12, 1333.95 examples/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 13122/52002 [00:09<00:28, 1353.94it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 13215 examples [00:12, 1373.22 examples/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 13268/52002 [00:09<00:28, 1382.80it/s]#033[A\u001b[0m\n",
      "\u001b[34m26%|██▌       | 13407/52002 [00:09<00:28, 1353.85it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 13420 examples [00:12, 1367.51 examples/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 13557/52002 [00:09<00:27, 1395.69it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 13569 examples [00:12, 1396.11 examples/s]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 13697/52002 [00:09<00:28, 1362.89it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 13769 examples [00:12, 1366.09 examples/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 13838/52002 [00:10<00:27, 1375.07it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 13916 examples [00:12, 1389.02 examples/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 13976/52002 [00:10<00:27, 1363.13it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 14095 examples [00:12, 1315.78 examples/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 14113/52002 [00:10<00:29, 1302.01it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 14229 examples [00:13, 1320.96 examples/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 14247/52002 [00:10<00:28, 1309.68it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 14385 examples [00:13, 1381.29 examples/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 14405/52002 [00:10<00:27, 1387.50it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 14527 examples [00:13, 1388.96 examples/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 14545/52002 [00:10<00:27, 1383.66it/s]#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 14684/52002 [00:10<00:27, 1375.42it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 14732 examples [00:13, 1377.64 examples/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 14822/52002 [00:10<00:27, 1362.38it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 14937 examples [00:13, 1369.94 examples/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 14962/52002 [00:10<00:26, 1372.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▉       | 15100/52002 [00:11<00:27, 1341.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 15133 examples [00:13, 1334.29 examples/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 15237/52002 [00:11<00:27, 1348.78it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 15274 examples [00:13, 1350.05 examples/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 15373/52002 [00:11<00:27, 1327.21it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 15476 examples [00:13, 1345.20 examples/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 15514/52002 [00:11<00:27, 1349.40it/s]#033[A\u001b[0m\n",
      "\u001b[34m30%|███       | 15650/52002 [00:11<00:27, 1336.47it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 15677 examples [00:14, 1337.92 examples/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 15784/52002 [00:11<00:27, 1324.76it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 15813 examples [00:14, 1342.61 examples/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 15917/52002 [00:11<00:27, 1322.15it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 16009 examples [00:14, 1327.81 examples/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 16053/52002 [00:11<00:26, 1331.79it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 16155 examples [00:14, 1359.23 examples/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 16194/52002 [00:11<00:26, 1353.68it/s]#033[A\u001b[0m\n",
      "\u001b[34m31%|███▏      | 16337/52002 [00:11<00:25, 1375.79it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 16362 examples [00:14, 1362.13 examples/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 16475/52002 [00:12<00:25, 1366.55it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 16575 examples [00:14, 1378.73 examples/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 16623/52002 [00:12<00:25, 1398.38it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 16723 examples [00:14, 1398.79 examples/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 16775/52002 [00:12<00:24, 1432.58it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 16867 examples [00:14, 1404.22 examples/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 16919/52002 [00:12<00:25, 1402.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m33%|███▎      | 17060/52002 [00:12<00:25, 1354.68it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 17066 examples [00:15, 1357.37 examples/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 17200/52002 [00:12<00:25, 1367.12it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 17205 examples [00:15, 1362.87 examples/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 17348/52002 [00:12<00:24, 1398.94it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 17355 examples [00:15, 1395.27 examples/s]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 17489/52002 [00:12<00:24, 1391.85it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 17570 examples [00:15, 1404.89 examples/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 17630/52002 [00:12<00:24, 1396.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m34%|███▍      | 17770/52002 [00:12<00:24, 1393.49it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 17777 examples [00:15, 1391.70 examples/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 17910/52002 [00:13<00:24, 1388.84it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 17981 examples [00:15, 1379.12 examples/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 18049/52002 [00:13<00:25, 1348.41it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 18179 examples [00:15, 1358.48 examples/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 18186/52002 [00:13<00:25, 1349.41it/s]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▌      | 18322/52002 [00:13<00:25, 1317.19it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 18367 examples [00:16, 1321.37 examples/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 18454/52002 [00:13<00:25, 1302.10it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 18562 examples [00:16, 1311.08 examples/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 18588/52002 [00:13<00:25, 1311.32it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 18707 examples [00:16, 1339.59 examples/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 18733/52002 [00:13<00:24, 1350.68it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 18852 examples [00:16, 1364.12 examples/s]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 18869/52002 [00:13<00:24, 1352.37it/s]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 19005/52002 [00:13<00:24, 1327.40it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 19041 examples [00:16, 1326.32 examples/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 19146/52002 [00:14<00:24, 1350.83it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 19182 examples [00:16, 1344.68 examples/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 19294/52002 [00:14<00:23, 1387.68it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 19326 examples [00:16, 1363.04 examples/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 19433/52002 [00:14<00:24, 1332.02it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 19523 examples [00:16, 1339.62 examples/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 19570/52002 [00:14<00:24, 1342.97it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 19663 examples [00:17, 1353.77 examples/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 19708/52002 [00:14<00:23, 1350.82it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 19803 examples [00:17, 1362.51 examples/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 19852/52002 [00:14<00:23, 1374.27it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 19942 examples [00:17, 1365.68 examples/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 19991/52002 [00:14<00:23, 1376.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m39%|███▊      | 20129/52002 [00:14<00:23, 1340.88it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 20144 examples [00:17, 1352.70 examples/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 20271/52002 [00:14<00:23, 1363.13it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 20283 examples [00:17, 1359.78 examples/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 20413/52002 [00:14<00:22, 1378.19it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 20424 examples [00:17, 1370.25 examples/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 20552/52002 [00:15<00:23, 1355.95it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 20627 examples [00:17, 1358.89 examples/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 20705/52002 [00:15<00:22, 1402.81it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 20788 examples [00:17, 1422.91 examples/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 20856/52002 [00:15<00:21, 1429.12it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 21000 examples [00:18, 1395.73 examples/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 21000/52002 [00:15<00:22, 1390.79it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 21153 examples [00:18, 1427.74 examples/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 21153/52002 [00:15<00:21, 1429.51it/s]#033[A\u001b[0m\n",
      "\u001b[34m41%|████      | 21298/52002 [00:15<00:21, 1434.44it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 21299 examples [00:18, 1432.31 examples/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 21442/52002 [00:15<00:21, 1410.06it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 21511 examples [00:18, 1422.00 examples/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 21584/52002 [00:15<00:21, 1396.23it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 21721 examples [00:18, 1411.79 examples/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 21731/52002 [00:15<00:21, 1416.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▏     | 21873/52002 [00:15<00:21, 1411.33it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 21923 examples [00:18, 1385.19 examples/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 22015/52002 [00:16<00:22, 1342.19it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 22125 examples [00:18, 1369.71 examples/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 22156/52002 [00:16<00:21, 1360.48it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 22265 examples [00:18, 1374.40 examples/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 22298/52002 [00:16<00:21, 1375.33it/s]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 22436/52002 [00:16<00:21, 1367.80it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 22471 examples [00:19, 1371.67 examples/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 22575/52002 [00:16<00:21, 1370.33it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 22676 examples [00:19, 1366.02 examples/s]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 22713/52002 [00:16<00:21, 1356.18it/s]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 22849/52002 [00:16<00:21, 1353.63it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 22877 examples [00:19, 1353.93 examples/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 22990/52002 [00:16<00:21, 1367.24it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 23080 examples [00:19, 1350.04 examples/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 23127/52002 [00:16<00:21, 1344.66it/s]#033[A\u001b[0m\n",
      "\u001b[34m45%|████▍     | 23263/52002 [00:16<00:21, 1347.40it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 23289 examples [00:19, 1356.12 examples/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 23412/52002 [00:17<00:20, 1388.42it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 23439 examples [00:19, 1385.11 examples/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 23554/52002 [00:17<00:20, 1396.57it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 23582 examples [00:19, 1395.09 examples/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 23703/52002 [00:17<00:19, 1422.87it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 23730 examples [00:19, 1414.93 examples/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 23846/52002 [00:17<00:19, 1415.96it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 23942 examples [00:20, 1412.38 examples/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 23994/52002 [00:17<00:19, 1432.15it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 24132 examples [00:20, 1353.67 examples/s]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 24138/52002 [00:17<00:20, 1335.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m47%|████▋     | 24275/52002 [00:17<00:20, 1345.01it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 24345 examples [00:20, 1371.93 examples/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 24421/52002 [00:17<00:20, 1376.30it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 24486 examples [00:20, 1375.35 examples/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 24565/52002 [00:17<00:19, 1393.10it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 24630 examples [00:20, 1387.90 examples/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 24710/52002 [00:18<00:19, 1404.73it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 24772 examples [00:20, 1392.24 examples/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 24851/52002 [00:18<00:19, 1386.04it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 24982 examples [00:20, 1392.06 examples/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 24993/52002 [00:18<00:19, 1394.91it/s]#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 25133/52002 [00:18<00:19, 1356.28it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 25178 examples [00:21, 1358.13 examples/s]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 25273/52002 [00:18<00:19, 1368.03it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 25382 examples [00:21, 1354.83 examples/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 25411/52002 [00:18<00:19, 1356.18it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 25533 examples [00:21, 1388.42 examples/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 25560/52002 [00:18<00:18, 1394.09it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 25676 examples [00:21, 1395.74 examples/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 25700/52002 [00:18<00:18, 1387.93it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|████▉     | 25839/52002 [00:18<00:19, 1374.94it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 25874 examples [00:21, 1361.14 examples/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 25977/52002 [00:18<00:19, 1351.25it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 26082 examples [00:21, 1348.20 examples/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 26113/52002 [00:19<00:19, 1339.69it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 26228 examples [00:21, 1370.55 examples/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 26257/52002 [00:19<00:18, 1368.42it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 26370 examples [00:21, 1379.52 examples/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 26396/52002 [00:19<00:18, 1374.04it/s]#033[A\u001b[0m\n",
      "\u001b[34m51%|█████     | 26534/52002 [00:19<00:18, 1368.77it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 26580 examples [00:22, 1384.09 examples/s]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 26676/52002 [00:19<00:18, 1381.08it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 26772 examples [00:22, 1346.74 examples/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 26815/52002 [00:19<00:18, 1331.94it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 26908 examples [00:22, 1348.09 examples/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 26955/52002 [00:19<00:18, 1350.71it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 27091/52002 [00:19<00:18, 1318.09it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 27104 examples [00:22, 1330.76 examples/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 27238/52002 [00:19<00:18, 1358.60it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 27248 examples [00:22, 1353.62 examples/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 27376/52002 [00:19<00:18, 1363.40it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 27386 examples [00:22, 1356.85 examples/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 27524/52002 [00:20<00:17, 1396.26it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 27535 examples [00:22, 1390.44 examples/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 27664/52002 [00:20<00:17, 1370.61it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 27739 examples [00:22, 1373.44 examples/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 27802/52002 [00:20<00:17, 1356.04it/s]#033[A\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 27942/52002 [00:20<00:17, 1368.00it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 27943 examples [00:23, 1361.85 examples/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 28079/52002 [00:20<00:18, 1313.07it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 28138 examples [00:23, 1316.54 examples/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 28211/52002 [00:20<00:18, 1308.63it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 28335 examples [00:23, 1311.62 examples/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 28343/52002 [00:20<00:18, 1297.77it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 28469 examples [00:23, 1314.88 examples/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 28478/52002 [00:20<00:17, 1310.91it/s]#033[A\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 28610/52002 [00:20<00:17, 1303.10it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 28665 examples [00:23, 1309.57 examples/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 28751/52002 [00:21<00:17, 1334.11it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 28812 examples [00:23, 1346.20 examples/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 28890/52002 [00:21<00:17, 1344.54it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 29001 examples [00:23, 1313.96 examples/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 29025/52002 [00:21<00:17, 1330.15it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 29144 examples [00:24, 1337.48 examples/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 29160/52002 [00:21<00:17, 1334.81it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 29285 examples [00:24, 1353.96 examples/s]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 29302/52002 [00:21<00:16, 1359.76it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 29424 examples [00:24, 1360.94 examples/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 29440/52002 [00:21<00:16, 1365.52it/s]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 29577/52002 [00:21<00:16, 1364.26it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 29634 examples [00:24, 1371.55 examples/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 29719/52002 [00:21<00:16, 1379.42it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 29835 examples [00:24, 1356.77 examples/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 29857/52002 [00:21<00:16, 1347.45it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 29980 examples [00:24, 1376.29 examples/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 30000/52002 [00:21<00:16, 1335.75it/s]#033[A\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 30138/52002 [00:22<00:16, 1345.63it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 30168 examples [00:24, 1331.88 examples/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 30279/52002 [00:22<00:15, 1363.14it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 30317 examples [00:24, 1370.33 examples/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 30416/52002 [00:22<00:15, 1363.50it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 30505 examples [00:25, 1326.93 examples/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 30553/52002 [00:22<00:16, 1315.49it/s]#033[A\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 30691/52002 [00:22<00:15, 1333.02it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 30709 examples [00:25, 1331.78 examples/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 30836/52002 [00:22<00:15, 1364.48it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 30856 examples [00:25, 1362.09 examples/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 30977/52002 [00:22<00:15, 1375.88it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 30994 examples [00:25, 1358.47 examples/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 31115/52002 [00:22<00:15, 1328.19it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 31184 examples [00:25, 1321.49 examples/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 31249/52002 [00:22<00:15, 1329.56it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 31326 examples [00:25, 1344.79 examples/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 31387/52002 [00:22<00:15, 1342.04it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 31467 examples [00:25, 1360.11 examples/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 31529/52002 [00:23<00:15, 1364.38it/s]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████    | 31670/52002 [00:23<00:14, 1377.34it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 31678 examples [00:25, 1372.71 examples/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 31818/52002 [00:23<00:14, 1406.90it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 31827 examples [00:25, 1399.85 examples/s]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 31975/52002 [00:23<00:13, 1455.09it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 31987 examples [00:26, 1450.49 examples/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 32121/52002 [00:23<00:14, 1403.46it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 32190 examples [00:26, 1409.25 examples/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 32262/52002 [00:23<00:14, 1392.88it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 32387 examples [00:26, 1371.22 examples/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 32402/52002 [00:23<00:14, 1381.21it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 32536 examples [00:26, 1398.23 examples/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 32549/52002 [00:23<00:13, 1403.43it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 32682 examples [00:26, 1411.85 examples/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 32690/52002 [00:23<00:13, 1402.94it/s]#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 32831/52002 [00:24<00:13, 1391.49it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 32885 examples [00:26, 1386.02 examples/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 32972/52002 [00:24<00:13, 1394.92it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 33090 examples [00:26, 1376.89 examples/s]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 33112/52002 [00:24<00:13, 1365.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 33253/52002 [00:24<00:13, 1376.22it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 33294 examples [00:27, 1366.53 examples/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 33391/52002 [00:24<00:13, 1369.08it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 33440 examples [00:27, 1387.09 examples/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 33533/52002 [00:24<00:13, 1382.87it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 33584 examples [00:27, 1396.53 examples/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 33679/52002 [00:24<00:13, 1405.50it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 33792 examples [00:27, 1388.81 examples/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 33820/52002 [00:24<00:13, 1397.79it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 33937 examples [00:27, 1402.33 examples/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 33962/52002 [00:24<00:12, 1401.13it/s]#033[A\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 34103/52002 [00:24<00:13, 1347.93it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 34138 examples [00:27, 1353.27 examples/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 34244/52002 [00:25<00:13, 1364.13it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 34278 examples [00:27, 1363.54 examples/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 34381/52002 [00:25<00:12, 1360.32it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 34480 examples [00:27, 1352.07 examples/s]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 34519/52002 [00:25<00:12, 1364.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 34621 examples [00:27, 1363.71 examples/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 34657/52002 [00:25<00:12, 1368.28it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 34767 examples [00:28, 1382.87 examples/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 34795/52002 [00:25<00:12, 1370.77it/s]#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 34935/52002 [00:25<00:12, 1375.63it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 34970 examples [00:28, 1368.92 examples/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 35073/52002 [00:25<00:12, 1331.09it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 35166 examples [00:28, 1346.87 examples/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 35216/52002 [00:25<00:12, 1357.07it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 35307 examples [00:28, 1351.26 examples/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 35353/52002 [00:25<00:12, 1356.90it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 35451 examples [00:28, 1371.79 examples/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 35501/52002 [00:25<00:11, 1391.91it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 35597 examples [00:28, 1389.42 examples/s]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 35641/52002 [00:26<00:11, 1390.75it/s]#033[A\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 35781/52002 [00:26<00:11, 1384.18it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 35798 examples [00:28, 1368.39 examples/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 35920/52002 [00:26<00:12, 1334.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 35986 examples [00:29, 1325.49 examples/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 36054/52002 [00:26<00:12, 1254.25it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 36176 examples [00:29, 1301.50 examples/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 36193/52002 [00:26<00:12, 1289.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 36328/52002 [00:26<00:12, 1306.11it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 36373 examples [00:29, 1301.35 examples/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 36464/52002 [00:26<00:11, 1321.60it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 36508 examples [00:29, 1310.46 examples/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 36613/52002 [00:26<00:11, 1370.40it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 36661 examples [00:29, 1360.97 examples/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 36751/52002 [00:26<00:11, 1371.55it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 36806 examples [00:29, 1383.48 examples/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 36907/52002 [00:26<00:10, 1426.20it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 36962 examples [00:29, 1429.79 examples/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 37050/52002 [00:27<00:10, 1372.08it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 37158 examples [00:29, 1381.53 examples/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 37196/52002 [00:27<00:10, 1397.08it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 37309 examples [00:29, 1408.71 examples/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 37340/52002 [00:27<00:10, 1408.45it/s]#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 37482/52002 [00:27<00:10, 1401.07it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 37515 examples [00:30, 1392.19 examples/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 37623/52002 [00:27<00:10, 1366.47it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 37725 examples [00:30, 1389.88 examples/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 37771/52002 [00:27<00:10, 1398.14it/s]#033[A\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 37912/52002 [00:27<00:10, 1400.98it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 37936 examples [00:30, 1390.70 examples/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 38053/52002 [00:27<00:10, 1364.59it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 38080 examples [00:30, 1382.37 examples/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 38203/52002 [00:27<00:09, 1402.38it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 38295 examples [00:30, 1396.13 examples/s]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 38344/52002 [00:28<00:09, 1388.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 38493/52002 [00:28<00:09, 1416.32it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 38510 examples [00:30, 1405.35 examples/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 38635/52002 [00:28<00:09, 1399.98it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 38711 examples [00:30, 1381.77 examples/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 38776/52002 [00:28<00:09, 1366.77it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 38917 examples [00:31, 1374.43 examples/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 38917/52002 [00:28<00:09, 1376.40it/s]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 39055/52002 [00:28<00:09, 1316.24it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 39108 examples [00:31, 1340.62 examples/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 39194/52002 [00:28<00:09, 1336.28it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 39245 examples [00:31, 1346.15 examples/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 39342/52002 [00:28<00:09, 1375.85it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 39395 examples [00:31, 1383.35 examples/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 39485/52002 [00:28<00:09, 1389.75it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 39602 examples [00:31, 1376.39 examples/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 39625/52002 [00:28<00:08, 1379.52it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 39744 examples [00:31, 1381.36 examples/s]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 39766/52002 [00:29<00:08, 1387.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 39883 examples [00:31, 1379.16 examples/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 39905/52002 [00:29<00:08, 1384.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 40044/52002 [00:29<00:08, 1365.09it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 40080 examples [00:31, 1354.24 examples/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 40181/52002 [00:29<00:08, 1359.34it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 40220 examples [00:32, 1364.90 examples/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 40324/52002 [00:29<00:08, 1377.91it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 40419 examples [00:32, 1348.65 examples/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 40462/52002 [00:29<00:08, 1337.42it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 40565 examples [00:32, 1373.96 examples/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 40612/52002 [00:29<00:08, 1384.17it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 40711 examples [00:32, 1390.64 examples/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 40753/52002 [00:29<00:08, 1390.06it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 40855 examples [00:32, 1402.62 examples/s]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 40898/52002 [00:29<00:07, 1407.54it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 41000 examples [00:32, 1384.66 examples/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 41039/52002 [00:29<00:07, 1389.62it/s]#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 41179/52002 [00:30<00:07, 1378.78it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 41208 examples [00:32, 1384.41 examples/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 41324/52002 [00:30<00:07, 1397.96it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 41354 examples [00:32, 1400.29 examples/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 41464/52002 [00:30<00:07, 1387.78it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 41569 examples [00:33, 1409.49 examples/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 41610/52002 [00:30<00:07, 1406.37it/s]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 41751/52002 [00:30<00:07, 1352.42it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 41759 examples [00:33, 1357.63 examples/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 41888/52002 [00:30<00:07, 1356.40it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 41898 examples [00:33, 1363.26 examples/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 42024/52002 [00:30<00:07, 1309.50it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 42088 examples [00:33, 1327.40 examples/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 42162/52002 [00:30<00:07, 1328.15it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 42224 examples [00:33, 1330.93 examples/s]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 42299/52002 [00:30<00:07, 1337.51it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 42358 examples [00:33, 1329.38 examples/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 42436/52002 [00:31<00:07, 1346.03it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 42504 examples [00:33, 1363.83 examples/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 42584/52002 [00:31<00:06, 1383.55it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 42651 examples [00:33, 1392.07 examples/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 42725/52002 [00:31<00:06, 1387.57it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 42844 examples [00:34, 1346.52 examples/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 42864/52002 [00:31<00:06, 1331.09it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 42982 examples [00:34, 1352.18 examples/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 43000/52002 [00:31<00:06, 1307.85it/s]#033[A\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 43146/52002 [00:31<00:06, 1351.28it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 43177 examples [00:34, 1329.37 examples/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 43282/52002 [00:31<00:06, 1317.48it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 43364 examples [00:34, 1296.47 examples/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 43415/52002 [00:31<00:06, 1295.19it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 43504 examples [00:34, 1319.27 examples/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 43554/52002 [00:31<00:06, 1322.02it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 43647 examples [00:34, 1346.00 examples/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 43698/52002 [00:31<00:06, 1356.01it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 43793 examples [00:34, 1373.02 examples/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 43846/52002 [00:32<00:05, 1392.31it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 43936 examples [00:34, 1387.36 examples/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 43990/52002 [00:32<00:05, 1404.70it/s]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 44131/52002 [00:32<00:05, 1341.31it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 44138 examples [00:34, 1345.35 examples/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 44268/52002 [00:32<00:05, 1349.25it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 44276 examples [00:35, 1353.95 examples/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 44408/52002 [00:32<00:05, 1363.33it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 44416 examples [00:35, 1363.98 examples/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 44554/52002 [00:32<00:05, 1390.10it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 44564 examples [00:35, 1394.40 examples/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 44702/52002 [00:32<00:05, 1416.44it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 44712 examples [00:35, 1417.02 examples/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 44846/52002 [00:32<00:05, 1423.11it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 44929 examples [00:35, 1423.84 examples/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 44989/52002 [00:32<00:04, 1409.95it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 45121 examples [00:35, 1368.71 examples/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 45131/52002 [00:33<00:05, 1346.05it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 45263 examples [00:35, 1376.23 examples/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 45275/52002 [00:33<00:04, 1372.79it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 45402 examples [00:35, 1377.28 examples/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 45413/52002 [00:33<00:04, 1374.49it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 45543 examples [00:35, 1384.64 examples/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 45556/52002 [00:33<00:04, 1388.89it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 45700 examples [00:36, 1435.12 examples/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 45714/52002 [00:33<00:04, 1444.34it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 45849 examples [00:36, 1444.78 examples/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 45859/52002 [00:33<00:04, 1434.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 46003/52002 [00:33<00:04, 1344.75it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 46028 examples [00:36, 1345.17 examples/s]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 46139/52002 [00:33<00:04, 1319.57it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 46227 examples [00:36, 1331.18 examples/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 46278/52002 [00:33<00:04, 1336.99it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 46369 examples [00:36, 1349.86 examples/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 46421/52002 [00:33<00:04, 1361.97it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 46516 examples [00:36, 1377.63 examples/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 46568/52002 [00:34<00:03, 1393.35it/s]#033[A\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 46708/52002 [00:34<00:03, 1382.45it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 46724 examples [00:36, 1376.17 examples/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 46852/52002 [00:34<00:03, 1396.45it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 46869 examples [00:36, 1390.27 examples/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 46996/52002 [00:34<00:03, 1406.83it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 47075 examples [00:37, 1368.66 examples/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 47137/52002 [00:34<00:03, 1358.64it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 47213 examples [00:37, 1366.98 examples/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 47275/52002 [00:34<00:03, 1362.51it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 47353 examples [00:37, 1372.11 examples/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 47415/52002 [00:34<00:03, 1372.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 47553/52002 [00:34<00:03, 1366.22it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 47559 examples [00:37, 1367.23 examples/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 47693/52002 [00:34<00:03, 1372.61it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 47699 examples [00:37, 1370.31 examples/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 47831/52002 [00:34<00:03, 1352.61it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 47891 examples [00:37, 1335.93 examples/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 47967/52002 [00:35<00:03, 1336.64it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 48095 examples [00:37, 1339.02 examples/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 48101/52002 [00:35<00:02, 1327.50it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 48232 examples [00:37, 1344.09 examples/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 48239/52002 [00:35<00:02, 1341.54it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 48371 examples [00:38, 1351.00 examples/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 48375/52002 [00:35<00:02, 1344.49it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 48507 examples [00:38, 1350.02 examples/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 48512/52002 [00:35<00:02, 1349.74it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 48652 examples [00:38, 1373.01 examples/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 48655/52002 [00:35<00:02, 1372.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 48793/52002 [00:35<00:02, 1365.11it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 48864 examples [00:38, 1385.01 examples/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 48940/52002 [00:35<00:02, 1395.49it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 49066 examples [00:38, 1345.52 examples/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 49080/52002 [00:35<00:02, 1333.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 49218/52002 [00:35<00:02, 1344.44it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 49275 examples [00:38, 1359.63 examples/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 49360/52002 [00:36<00:01, 1364.06it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 49416 examples [00:38, 1368.92 examples/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 49497/52002 [00:36<00:01, 1361.43it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 49610 examples [00:38, 1341.17 examples/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 49634/52002 [00:36<00:01, 1332.01it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 49759 examples [00:39, 1374.58 examples/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 49789/52002 [00:36<00:01, 1392.91it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 49899 examples [00:39, 1377.25 examples/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 49929/52002 [00:36<00:01, 1371.40it/s]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 50067/52002 [00:36<00:01, 1305.97it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 50084 examples [00:39, 1321.11 examples/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 50210/52002 [00:36<00:01, 1341.06it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 50227 examples [00:39, 1345.38 examples/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 50347/52002 [00:36<00:01, 1347.84it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 50426 examples [00:39, 1334.92 examples/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 50483/52002 [00:36<00:01, 1332.74it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 50570 examples [00:39, 1358.48 examples/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 50622/52002 [00:37<00:01, 1348.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 50758/52002 [00:37<00:00, 1346.83it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 50770 examples [00:39, 1344.84 examples/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 50913/52002 [00:37<00:00, 1406.62it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 50925 examples [00:39, 1393.01 examples/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 51054/52002 [00:37<00:00, 1355.04it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 51116 examples [00:40, 1347.98 examples/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 51191/52002 [00:37<00:00, 1356.71it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 51261 examples [00:40, 1369.53 examples/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 51337/52002 [00:37<00:00, 1385.63it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 51400 examples [00:40, 1365.18 examples/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 51476/52002 [00:37<00:00, 1372.86it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 51540 examples [00:40, 1371.62 examples/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 51619/52002 [00:37<00:00, 1387.38it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 51687 examples [00:40, 1395.44 examples/s]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 51760/52002 [00:37<00:00, 1392.78it/s]#033[A\u001b[0m\n",
      "\u001b[34mGenerating train split: 51828 examples [00:40, 1395.61 examples/s]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 51900/52002 [00:37<00:00, 1389.62it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 52002/52002 [00:38<00:00, 1366.78it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 52002 examples [00:40, 1253.15 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 52002 examples [00:40, 1276.16 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (0/1 shards):   0%|          | 0/52002 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (1/1 shards): 100%|██████████| 52002/52002 [00:00<00:00, 2276388.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (1/1 shards): 100%|██████████| 52002/52002 [00:00<00:00, 2258591.66 examples/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mExplicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mExplicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mExplicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mExplicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mExplicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mExplicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\u001b[0m\n",
      "\u001b[34mOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  12%|█▎        | 1/8 [00:07<00:49,  7.06s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 2/8 [00:14<00:42,  7.06s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  38%|███▊      | 3/8 [00:20<00:34,  6.90s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 4/8 [00:27<00:26,  6.63s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  62%|██████▎   | 5/8 [00:33<00:19,  6.55s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 6/8 [00:39<00:13,  6.54s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  88%|████████▊ | 7/8 [00:43<00:05,  5.65s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [00:47<00:00,  5.03s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [00:47<00:00,  5.94s/it]\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /opt/conda/lib/python3.9/site-packages/smdistributed/dataparallel/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/lib did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/v2/credentials/proxy-afb7369b904559a4a792ecd925da3259518d2119c4563327dc9786a3e512164d-customer')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/opt/ml/output/intermediate')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('$(dirname $(which conda))/..')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/opt/conda/lib/python39.zip')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('\"sagemaker_pytorch_container.training'), PosixPath('[\"algo-1\"],\"instance_group_name\"'), PosixPath('main\",\"hosts\"'), PosixPath('\"algo-1\",\"model_dir\"'), PosixPath('{\"hosts\"'), PosixPath('{},\"channel_input_dirs\"'), PosixPath('[],\"framework_module\"'), PosixPath('\"algo-1\",\"current_instance_group\"'), PosixPath('4,\"num_neurons\"'), PosixPath('\"ml.g5.12xlarge\"}},\"is_hetero\"'), PosixPath('false,\"is_master\"'), PosixPath('\"eth0\",\"num_cpus\"'), PosixPath('\"homogeneousCluster\",\"current_instance_group_hosts\"'), PosixPath('\"homogeneousCluster\",\"current_host\"'), PosixPath('\"ml.g5.12xlarge\"}],\"network_interface_name\"'), PosixPath('true,\"is_modelparallel_enabled\"'), PosixPath('[],\"distribution_instance_groups\"'), PosixPath('\"/opt/ml/output/intermediate\",\"resource_config\"'), PosixPath('//sagemaker-us-west-2-687912291502/chatglm-lora-demo-2024-09-25-11-19-30-765/source/sourcedir.tar.gz\",\"module_name\"'), PosixPath('{},\"input_dir\"'), PosixPath('{\"current_group_name\"'), PosixPath('\"/opt/ml/input/config\",\"input_data_config\"'), PosixPath('\"/opt/ml/input\",\"instance_groups\"'), PosixPath('{\"additional_framework_parameters\"'), PosixPath('\"chatglm-lora-demo-2024-09-25-11-19-30-765\",\"log_level\"'), PosixPath('\"/opt/ml/output/data\",\"output_dir\"'), PosixPath('\"ml.g5.12xlarge\",\"distribution_hosts\"'), PosixPath('[\"algo-1\"],\"current_instance_type\"'), PosixPath('48,\"num_gpus\"'), PosixPath('true,\"job_name\"'), PosixPath('[\"homogeneousCluster\"],\"instance_groups_dict\"'), PosixPath('\"ml.g5.12xlarge\",\"hosts\"'), PosixPath('20,\"master_hostname\"'), PosixPath('0,\"output_data_dir\"'), PosixPath('{},\"current_host\"'), PosixPath('[\"algo-1\"],\"instance_groups\"'), PosixPath('\"train.sh\",\"network_interface_name\"'), PosixPath('\"eth0\"},\"user_entry_point\"'), PosixPath('true,\"is_smddprun_installed\"'), PosixPath('\"s3'), PosixPath('\"algo-1\",\"current_instance_type\"'), PosixPath('\"/opt/ml/model\",\"module_dir\"'), PosixPath('[{\"hosts\"'), PosixPath('\"train.sh\"}'), PosixPath('[\"algo-1\"],\"hyperparameters\"'), PosixPath('{\"homogeneousCluster\"'), PosixPath('\"/opt/ml/output\",\"output_intermediate_dir\"'), PosixPath('null,\"is_smddpmprun_installed\"'), PosixPath('{},\"input_config_dir\"'), PosixPath('\"homogeneousCluster\",\"instance_type\"')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('s3'), PosixPath('//sagemaker-us-west-2-687912291502/chatglm-lora-demo-2024-09-25-11-19-30-765/source/sourcedir.tar.gz')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('us-west-2'), PosixPath('arn'), PosixPath('687912291502'), PosixPath('training-job/chatglm-lora-demo-2024-09-25-11-19-30-765'), PosixPath('sagemaker'), PosixPath('aws')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 117\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:07,  1.14s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 2/8 [00:02<00:07,  1.20s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  38%|███▊      | 3/8 [00:03<00:06,  1.25s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 4/8 [00:04<00:05,  1.25s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:06<00:03,  1.25s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:07<00:02,  1.25s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:08<00:01,  1.08s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:08<00:00,  1.03it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:08<00:00,  1.11s/it]\u001b[0m\n",
      "\u001b[34mlen(dataset)=52002\u001b[0m\n",
      "\u001b[34mYou are adding a <class 'transformers.integrations.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\u001b[0m\n",
      "\u001b[34m:DefaultFlowCallback\u001b[0m\n",
      "\u001b[34mTensorBoardCallback\u001b[0m\n",
      "\u001b[34mYou are adding a <class 'transformers.integrations.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\u001b[0m\n",
      "\u001b[34m:DefaultFlowCallback\u001b[0m\n",
      "\u001b[34mTensorBoardCallback\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m0%|          | 0/1000 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2024-09-25 11:27:27.881 algo-1:254 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-09-25 11:27:27.916 algo-1:254 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\u001b[0m\n",
      "\u001b[34m0%|          | 1/1000 [00:03<52:01,  3.12s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/1000 [00:04<37:37,  2.26s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 3/1000 [00:06<29:57,  1.80s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 4/1000 [00:07<26:18,  1.58s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 5/1000 [00:09<29:33,  1.78s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 6/1000 [00:11<28:35,  1.73s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 7/1000 [00:12<27:59,  1.69s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 8/1000 [00:14<30:20,  1.84s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 9/1000 [00:16<27:09,  1.64s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 10/1000 [00:17<26:01,  1.58s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 11/1000 [00:19<28:46,  1.75s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 12/1000 [00:21<27:14,  1.65s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 13/1000 [00:22<26:03,  1.58s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 14/1000 [00:23<24:03,  1.46s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 15/1000 [00:25<24:40,  1.50s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 16/1000 [00:27<27:46,  1.69s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 17/1000 [00:28<26:18,  1.61s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 18/1000 [00:30<27:24,  1.67s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 19/1000 [00:32<26:10,  1.60s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 20/1000 [00:33<25:27,  1.56s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 21/1000 [00:34<24:45,  1.52s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 22/1000 [00:37<27:45,  1.70s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 23/1000 [00:38<26:17,  1.61s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 24/1000 [00:39<25:38,  1.58s/it]\u001b[0m\n",
      "\u001b[34m2%|▎         | 25/1000 [00:41<25:55,  1.59s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 26/1000 [00:43<25:46,  1.59s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 27/1000 [00:45<28:26,  1.75s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 28/1000 [00:47<29:38,  1.83s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 29/1000 [00:48<27:48,  1.72s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 30/1000 [00:50<28:56,  1.79s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 31/1000 [00:52<27:27,  1.70s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 32/1000 [00:53<26:23,  1.64s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 33/1000 [00:55<27:23,  1.70s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 34/1000 [00:57<26:15,  1.63s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 35/1000 [00:58<26:04,  1.62s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 36/1000 [00:59<24:24,  1.52s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 37/1000 [01:01<22:56,  1.43s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 38/1000 [01:02<21:02,  1.31s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 39/1000 [01:03<23:30,  1.47s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 40/1000 [01:06<26:33,  1.66s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 41/1000 [01:07<26:08,  1.64s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 42/1000 [01:09<28:30,  1.79s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 43/1000 [01:11<26:55,  1.69s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 44/1000 [01:13<27:40,  1.74s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 45/1000 [01:15<29:30,  1.85s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 46/1000 [01:16<27:39,  1.74s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 47/1000 [01:18<26:10,  1.65s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 48/1000 [01:19<25:22,  1.60s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 49/1000 [01:21<26:11,  1.65s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 50/1000 [01:22<24:59,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9984, 'learning_rate': 9.53e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m5%|▌         | 50/1000 [01:22<24:59,  1.58s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 51/1000 [01:24<25:26,  1.61s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 52/1000 [01:25<24:27,  1.55s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 53/1000 [01:28<27:09,  1.72s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 54/1000 [01:30<29:01,  1.84s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 55/1000 [01:32<29:47,  1.89s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 56/1000 [01:34<30:52,  1.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 57/1000 [01:35<27:37,  1.76s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 58/1000 [01:37<26:12,  1.67s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 59/1000 [01:38<25:53,  1.65s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 60/1000 [01:40<27:14,  1.74s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 61/1000 [01:42<28:08,  1.80s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 62/1000 [01:44<27:15,  1.74s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 63/1000 [01:45<23:26,  1.50s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 64/1000 [01:47<26:25,  1.69s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 65/1000 [01:49<27:11,  1.74s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 66/1000 [01:50<26:32,  1.70s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 67/1000 [01:52<28:32,  1.84s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 68/1000 [01:54<29:52,  1.92s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 69/1000 [01:56<29:16,  1.89s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 70/1000 [01:58<27:50,  1.80s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 71/1000 [02:00<28:29,  1.84s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 72/1000 [02:02<29:46,  1.93s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 73/1000 [02:04<29:56,  1.94s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 74/1000 [02:06<30:49,  2.00s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 75/1000 [02:07<27:09,  1.76s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 76/1000 [02:09<28:48,  1.87s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 77/1000 [02:10<25:13,  1.64s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 78/1000 [02:12<24:26,  1.59s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 79/1000 [02:13<23:44,  1.55s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 80/1000 [02:15<23:15,  1.52s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 81/1000 [02:16<21:05,  1.38s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 82/1000 [02:18<22:14,  1.45s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 83/1000 [02:20<25:18,  1.66s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 84/1000 [02:22<27:27,  1.80s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 85/1000 [02:23<26:40,  1.75s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 86/1000 [02:25<27:10,  1.78s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 87/1000 [02:27<27:51,  1.83s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 88/1000 [02:29<29:10,  1.92s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 89/1000 [02:31<26:55,  1.77s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 90/1000 [02:32<25:37,  1.69s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 91/1000 [02:34<27:37,  1.82s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 92/1000 [02:37<28:58,  1.92s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 93/1000 [02:39<29:56,  1.98s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 94/1000 [02:40<27:38,  1.83s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 95/1000 [02:42<25:59,  1.72s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 96/1000 [02:43<26:13,  1.74s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 97/1000 [02:46<27:55,  1.86s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 98/1000 [02:48<29:07,  1.94s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 99/1000 [02:49<26:07,  1.74s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 100/1000 [02:50<23:58,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6314, 'learning_rate': 9.030000000000001e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m10%|█         | 100/1000 [02:50<23:58,  1.60s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 101/1000 [02:51<22:23,  1.49s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 102/1000 [02:53<22:19,  1.49s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 103/1000 [02:54<21:13,  1.42s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 104/1000 [02:56<22:18,  1.49s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 105/1000 [02:58<23:38,  1.58s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 106/1000 [03:00<26:02,  1.75s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 107/1000 [03:01<24:44,  1.66s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 108/1000 [03:03<23:41,  1.59s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 109/1000 [03:05<26:03,  1.76s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 110/1000 [03:06<25:12,  1.70s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 111/1000 [03:08<26:30,  1.79s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 112/1000 [03:10<26:34,  1.80s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 113/1000 [03:12<27:36,  1.87s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 114/1000 [03:14<25:52,  1.75s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 115/1000 [03:15<23:37,  1.60s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 116/1000 [03:17<24:28,  1.66s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 117/1000 [03:19<26:00,  1.77s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 118/1000 [03:21<26:11,  1.78s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 119/1000 [03:23<27:43,  1.89s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 120/1000 [03:24<25:39,  1.75s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 121/1000 [03:25<22:42,  1.55s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 122/1000 [03:27<22:58,  1.57s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 123/1000 [03:29<25:22,  1.74s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 124/1000 [03:31<25:05,  1.72s/it]\u001b[0m\n",
      "\u001b[34m12%|█▎        | 125/1000 [03:33<26:01,  1.78s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 126/1000 [03:35<27:26,  1.88s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 127/1000 [03:36<24:35,  1.69s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 128/1000 [03:38<26:27,  1.82s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 129/1000 [03:40<27:49,  1.92s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 130/1000 [03:42<28:43,  1.98s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 131/1000 [03:44<29:22,  2.03s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 132/1000 [03:46<29:17,  2.03s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 133/1000 [03:49<29:37,  2.05s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 134/1000 [03:51<29:19,  2.03s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 135/1000 [03:53<29:39,  2.06s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 136/1000 [03:54<27:51,  1.93s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 137/1000 [03:56<25:34,  1.78s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 138/1000 [03:57<23:56,  1.67s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 139/1000 [03:59<24:43,  1.72s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 140/1000 [04:01<23:40,  1.65s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 141/1000 [04:02<24:24,  1.71s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 142/1000 [04:04<26:13,  1.83s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 143/1000 [04:06<24:44,  1.73s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 144/1000 [04:08<24:59,  1.75s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 145/1000 [04:09<23:40,  1.66s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 146/1000 [04:11<22:29,  1.58s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 147/1000 [04:12<21:03,  1.48s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 148/1000 [04:13<21:24,  1.51s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 149/1000 [04:15<20:14,  1.43s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 150/1000 [04:16<21:11,  1.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.681, 'learning_rate': 8.53e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 150/1000 [04:16<21:11,  1.50s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 151/1000 [04:18<20:50,  1.47s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 152/1000 [04:19<20:40,  1.46s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 153/1000 [04:21<21:27,  1.52s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 154/1000 [04:22<21:08,  1.50s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 155/1000 [04:24<22:16,  1.58s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 156/1000 [04:26<23:52,  1.70s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 157/1000 [04:28<24:11,  1.72s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 158/1000 [04:30<24:45,  1.76s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 159/1000 [04:32<26:18,  1.88s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 160/1000 [04:33<24:33,  1.75s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 161/1000 [04:35<26:08,  1.87s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 162/1000 [04:37<25:04,  1.80s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 163/1000 [04:38<22:43,  1.63s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 164/1000 [04:40<22:07,  1.59s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 165/1000 [04:41<22:24,  1.61s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 166/1000 [04:43<21:50,  1.57s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 167/1000 [04:44<20:36,  1.48s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 168/1000 [04:46<22:08,  1.60s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 169/1000 [04:47<20:31,  1.48s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 170/1000 [04:49<22:01,  1.59s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 171/1000 [04:51<21:31,  1.56s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 172/1000 [04:52<21:00,  1.52s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 173/1000 [04:54<22:17,  1.62s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 174/1000 [04:56<24:17,  1.76s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 175/1000 [04:58<25:45,  1.87s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 176/1000 [05:00<26:47,  1.95s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 177/1000 [05:02<25:30,  1.86s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 178/1000 [05:04<26:34,  1.94s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 179/1000 [05:05<24:26,  1.79s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 180/1000 [05:08<25:49,  1.89s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 181/1000 [05:09<23:14,  1.70s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 182/1000 [05:10<20:47,  1.53s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 183/1000 [05:11<20:24,  1.50s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 184/1000 [05:13<22:25,  1.65s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 185/1000 [05:15<21:20,  1.57s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 186/1000 [05:17<22:26,  1.65s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 187/1000 [05:18<20:05,  1.48s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 188/1000 [05:20<21:32,  1.59s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 189/1000 [05:21<22:38,  1.68s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 190/1000 [05:23<21:48,  1.61s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 191/1000 [05:25<22:25,  1.66s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 192/1000 [05:27<24:14,  1.80s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 193/1000 [05:28<22:36,  1.68s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 194/1000 [05:29<20:56,  1.56s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 195/1000 [05:31<20:36,  1.54s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 196/1000 [05:32<20:13,  1.51s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 197/1000 [05:35<22:41,  1.70s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 198/1000 [05:37<24:23,  1.82s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 199/1000 [05:39<25:34,  1.92s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 200/1000 [05:41<26:25,  1.98s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6044, 'learning_rate': 8.030000000000001e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m20%|██        | 200/1000 [05:41<26:25,  1.98s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 201/1000 [05:43<27:02,  2.03s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 202/1000 [05:44<23:56,  1.80s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 203/1000 [05:46<23:07,  1.74s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 204/1000 [05:48<22:26,  1.69s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 205/1000 [05:49<19:58,  1.51s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 206/1000 [05:51<21:54,  1.65s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 207/1000 [05:52<21:09,  1.60s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 208/1000 [05:54<23:12,  1.76s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 209/1000 [05:56<22:28,  1.70s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 210/1000 [05:58<24:00,  1.82s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 211/1000 [06:00<24:38,  1.87s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 212/1000 [06:01<22:59,  1.75s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 213/1000 [06:02<20:12,  1.54s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 214/1000 [06:04<19:00,  1.45s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 215/1000 [06:06<21:39,  1.66s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 216/1000 [06:08<23:30,  1.80s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 217/1000 [06:09<21:54,  1.68s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 218/1000 [06:11<21:46,  1.67s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 219/1000 [06:13<23:33,  1.81s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 220/1000 [06:15<24:21,  1.87s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 221/1000 [06:17<24:06,  1.86s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 222/1000 [06:19<25:13,  1.95s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 223/1000 [06:20<21:57,  1.70s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 224/1000 [06:22<23:03,  1.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▎       | 225/1000 [06:24<22:17,  1.73s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 226/1000 [06:25<20:17,  1.57s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 227/1000 [06:27<22:23,  1.74s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 228/1000 [06:29<23:52,  1.86s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 229/1000 [06:31<24:52,  1.94s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 230/1000 [06:33<23:32,  1.83s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 231/1000 [06:35<23:18,  1.82s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 232/1000 [06:36<22:30,  1.76s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 233/1000 [06:38<20:17,  1.59s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 234/1000 [06:39<20:28,  1.60s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 235/1000 [06:41<21:06,  1.66s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 236/1000 [06:43<22:55,  1.80s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 237/1000 [06:44<21:26,  1.69s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 238/1000 [06:46<21:23,  1.68s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 239/1000 [06:47<19:38,  1.55s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 240/1000 [06:49<19:06,  1.51s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 241/1000 [06:50<18:40,  1.48s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 242/1000 [06:51<17:03,  1.35s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 243/1000 [06:53<18:57,  1.50s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 244/1000 [06:55<20:02,  1.59s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 245/1000 [06:57<21:25,  1.70s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 246/1000 [06:59<21:59,  1.75s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 247/1000 [07:01<23:26,  1.87s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 248/1000 [07:03<24:22,  1.94s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 249/1000 [07:05<22:58,  1.84s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 250/1000 [07:06<20:37,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6292, 'learning_rate': 7.53e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 250/1000 [07:06<20:37,  1.65s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 251/1000 [07:07<19:44,  1.58s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 252/1000 [07:08<17:28,  1.40s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 253/1000 [07:10<17:33,  1.41s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 254/1000 [07:11<16:56,  1.36s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 255/1000 [07:13<19:46,  1.59s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 256/1000 [07:15<21:42,  1.75s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 257/1000 [07:17<21:47,  1.76s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 258/1000 [07:19<21:16,  1.72s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 259/1000 [07:20<20:20,  1.65s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 260/1000 [07:22<21:49,  1.77s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 261/1000 [07:24<22:09,  1.80s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 262/1000 [07:26<22:10,  1.80s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 263/1000 [07:27<21:24,  1.74s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 264/1000 [07:29<21:37,  1.76s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 265/1000 [07:31<21:13,  1.73s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 266/1000 [07:33<22:39,  1.85s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 267/1000 [07:34<19:40,  1.61s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 268/1000 [07:35<18:26,  1.51s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 269/1000 [07:37<19:35,  1.61s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 270/1000 [07:38<18:18,  1.50s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 271/1000 [07:40<18:49,  1.55s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 272/1000 [07:42<19:36,  1.62s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 273/1000 [07:43<17:44,  1.46s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 274/1000 [07:45<20:07,  1.66s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 275/1000 [07:47<21:47,  1.80s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 276/1000 [07:48<18:29,  1.53s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 277/1000 [07:49<17:24,  1.45s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 278/1000 [07:51<19:52,  1.65s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 279/1000 [07:53<19:07,  1.59s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 280/1000 [07:54<17:45,  1.48s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 281/1000 [07:56<18:17,  1.53s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 282/1000 [07:58<20:25,  1.71s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 283/1000 [08:00<20:42,  1.73s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 284/1000 [08:01<20:08,  1.69s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 285/1000 [08:03<19:57,  1.67s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 286/1000 [08:05<21:35,  1.82s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 287/1000 [08:07<20:42,  1.74s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 288/1000 [08:09<21:36,  1.82s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 289/1000 [08:11<22:40,  1.91s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 290/1000 [08:13<23:22,  1.98s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 291/1000 [08:14<21:54,  1.85s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 292/1000 [08:16<20:33,  1.74s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 293/1000 [08:18<20:48,  1.77s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 294/1000 [08:20<22:02,  1.87s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 295/1000 [08:22<21:11,  1.80s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 296/1000 [08:23<21:17,  1.82s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 297/1000 [08:25<20:02,  1.71s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 298/1000 [08:27<21:28,  1.84s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 299/1000 [08:29<21:30,  1.84s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 300/1000 [08:30<19:55,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5904, 'learning_rate': 7.03e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m30%|███       | 300/1000 [08:30<19:55,  1.71s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 301/1000 [08:32<19:00,  1.63s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 302/1000 [08:34<20:44,  1.78s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 303/1000 [08:36<20:45,  1.79s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 304/1000 [08:37<18:12,  1.57s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 305/1000 [08:39<19:09,  1.65s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 306/1000 [08:40<18:19,  1.58s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 307/1000 [08:41<17:39,  1.53s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 308/1000 [08:43<18:36,  1.61s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 309/1000 [08:45<20:21,  1.77s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 310/1000 [08:47<21:16,  1.85s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 311/1000 [08:49<21:16,  1.85s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 312/1000 [08:51<20:28,  1.79s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 313/1000 [08:53<21:39,  1.89s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 314/1000 [08:55<22:27,  1.96s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 315/1000 [08:57<23:00,  2.02s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 316/1000 [08:59<21:29,  1.89s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 317/1000 [09:01<22:18,  1.96s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 318/1000 [09:03<22:20,  1.97s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 319/1000 [09:04<19:46,  1.74s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 320/1000 [09:06<19:53,  1.76s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 321/1000 [09:07<18:09,  1.60s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 322/1000 [09:09<18:44,  1.66s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 323/1000 [09:11<20:19,  1.80s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 324/1000 [09:12<18:27,  1.64s/it]\u001b[0m\n",
      "\u001b[34m32%|███▎      | 325/1000 [09:14<19:44,  1.76s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 326/1000 [09:17<20:57,  1.87s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 327/1000 [09:19<21:49,  1.95s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 328/1000 [09:20<20:53,  1.87s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 329/1000 [09:22<20:10,  1.80s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 330/1000 [09:23<18:47,  1.68s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 331/1000 [09:25<18:36,  1.67s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 332/1000 [09:27<18:25,  1.66s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 333/1000 [09:28<18:49,  1.69s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 334/1000 [09:31<20:12,  1.82s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 335/1000 [09:33<20:44,  1.87s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 336/1000 [09:35<21:34,  1.95s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 337/1000 [09:36<20:58,  1.90s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 338/1000 [09:38<21:15,  1.93s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 339/1000 [09:40<20:19,  1.84s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 340/1000 [09:42<19:27,  1.77s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 341/1000 [09:44<20:33,  1.87s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 342/1000 [09:45<19:00,  1.73s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 343/1000 [09:47<19:14,  1.76s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 344/1000 [09:49<20:22,  1.86s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 345/1000 [09:51<20:18,  1.86s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 346/1000 [09:52<18:20,  1.68s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 347/1000 [09:54<17:26,  1.60s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 348/1000 [09:56<19:07,  1.76s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 349/1000 [09:57<17:16,  1.59s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 350/1000 [09:59<19:01,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6375, 'learning_rate': 6.53e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 350/1000 [09:59<19:01,  1.76s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 351/1000 [10:01<19:10,  1.77s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 352/1000 [10:03<20:16,  1.88s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 353/1000 [10:05<21:02,  1.95s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 354/1000 [10:07<21:32,  2.00s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 355/1000 [10:09<18:57,  1.76s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 356/1000 [10:11<20:04,  1.87s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 357/1000 [10:13<20:52,  1.95s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 358/1000 [10:15<20:32,  1.92s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 359/1000 [10:17<21:07,  1.98s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 360/1000 [10:19<20:32,  1.93s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 361/1000 [10:20<19:03,  1.79s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 362/1000 [10:22<18:28,  1.74s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 363/1000 [10:24<19:37,  1.85s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 364/1000 [10:25<18:47,  1.77s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 365/1000 [10:27<17:00,  1.61s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 366/1000 [10:28<17:36,  1.67s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 367/1000 [10:30<17:58,  1.70s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 368/1000 [10:32<18:50,  1.79s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 369/1000 [10:34<19:54,  1.89s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 370/1000 [10:36<20:34,  1.96s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 371/1000 [10:38<19:31,  1.86s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 372/1000 [10:39<18:00,  1.72s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 373/1000 [10:41<18:41,  1.79s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 374/1000 [10:43<18:37,  1.79s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 375/1000 [10:45<19:03,  1.83s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 376/1000 [10:46<17:04,  1.64s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 377/1000 [10:48<18:32,  1.79s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 378/1000 [10:51<19:34,  1.89s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 379/1000 [10:52<18:12,  1.76s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 380/1000 [10:54<17:42,  1.71s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 381/1000 [10:56<18:56,  1.84s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 382/1000 [10:57<17:31,  1.70s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 383/1000 [10:59<16:45,  1.63s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 384/1000 [11:00<16:13,  1.58s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 385/1000 [11:02<17:33,  1.71s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 386/1000 [11:03<16:38,  1.63s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 387/1000 [11:06<18:10,  1.78s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 388/1000 [11:08<19:11,  1.88s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 389/1000 [11:09<17:04,  1.68s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 390/1000 [11:10<15:20,  1.51s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 391/1000 [11:12<15:39,  1.54s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 392/1000 [11:13<15:18,  1.51s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 393/1000 [11:15<16:55,  1.67s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 394/1000 [11:17<16:38,  1.65s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 395/1000 [11:18<16:06,  1.60s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 396/1000 [11:20<16:00,  1.59s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 397/1000 [11:21<14:29,  1.44s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 398/1000 [11:23<16:31,  1.65s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 399/1000 [11:24<15:18,  1.53s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 400/1000 [11:26<15:09,  1.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6072, 'learning_rate': 6.03e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m40%|████      | 400/1000 [11:26<15:09,  1.52s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 401/1000 [11:27<15:31,  1.56s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 402/1000 [11:29<16:19,  1.64s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 403/1000 [11:31<16:52,  1.70s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 404/1000 [11:33<17:09,  1.73s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 405/1000 [11:35<17:22,  1.75s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 406/1000 [11:37<18:12,  1.84s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 407/1000 [11:38<16:50,  1.70s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 408/1000 [11:40<16:05,  1.63s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 409/1000 [11:41<14:54,  1.51s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 410/1000 [11:43<16:40,  1.70s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 411/1000 [11:44<16:01,  1.63s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 412/1000 [11:46<14:56,  1.52s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 413/1000 [11:48<16:15,  1.66s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 414/1000 [11:50<17:08,  1.75s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 415/1000 [11:52<18:10,  1.86s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 416/1000 [11:54<18:53,  1.94s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 417/1000 [11:56<19:11,  1.98s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 418/1000 [11:58<19:22,  2.00s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 419/1000 [12:00<19:41,  2.03s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 420/1000 [12:01<17:21,  1.80s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 421/1000 [12:03<17:16,  1.79s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 422/1000 [12:05<16:21,  1.70s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 423/1000 [12:06<16:11,  1.68s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 424/1000 [12:08<16:00,  1.67s/it]\u001b[0m\n",
      "\u001b[34m42%|████▎     | 425/1000 [12:09<14:46,  1.54s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 426/1000 [12:11<15:31,  1.62s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 427/1000 [12:13<16:53,  1.77s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 428/1000 [12:15<17:09,  1.80s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 429/1000 [12:17<16:34,  1.74s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 430/1000 [12:18<15:48,  1.66s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 431/1000 [12:20<16:42,  1.76s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 432/1000 [12:21<15:05,  1.59s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 433/1000 [12:22<14:09,  1.50s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 434/1000 [12:24<14:32,  1.54s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 435/1000 [12:26<16:08,  1.71s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 436/1000 [12:28<17:14,  1.83s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 437/1000 [12:30<18:02,  1.92s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 438/1000 [12:32<17:17,  1.85s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 439/1000 [12:34<18:03,  1.93s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 440/1000 [12:36<17:39,  1.89s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 441/1000 [12:37<15:49,  1.70s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 442/1000 [12:39<16:57,  1.82s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 443/1000 [12:41<15:51,  1.71s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 444/1000 [12:42<15:11,  1.64s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 445/1000 [12:44<16:30,  1.79s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 446/1000 [12:47<17:26,  1.89s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 447/1000 [12:48<15:39,  1.70s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 448/1000 [12:49<15:25,  1.68s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 449/1000 [12:51<15:37,  1.70s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 450/1000 [12:53<14:57,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6392, 'learning_rate': 5.530000000000001e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 450/1000 [12:53<14:57,  1.63s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 451/1000 [12:54<14:56,  1.63s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 452/1000 [12:56<14:51,  1.63s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 453/1000 [12:58<14:56,  1.64s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 454/1000 [12:59<14:22,  1.58s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 455/1000 [13:01<15:50,  1.74s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 456/1000 [13:03<14:54,  1.64s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 457/1000 [13:04<14:15,  1.58s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 458/1000 [13:05<13:44,  1.52s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 459/1000 [13:07<13:30,  1.50s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 460/1000 [13:09<14:39,  1.63s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 461/1000 [13:10<14:04,  1.57s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 462/1000 [13:12<15:31,  1.73s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 463/1000 [13:14<14:45,  1.65s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 464/1000 [13:15<13:39,  1.53s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 465/1000 [13:16<12:22,  1.39s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 466/1000 [13:18<14:17,  1.61s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 467/1000 [13:19<13:14,  1.49s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 468/1000 [13:22<14:54,  1.68s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 469/1000 [13:23<15:17,  1.73s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 470/1000 [13:25<15:59,  1.81s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 471/1000 [13:28<16:49,  1.91s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 472/1000 [13:29<15:59,  1.82s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 473/1000 [13:31<16:45,  1.91s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 474/1000 [13:33<15:30,  1.77s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 475/1000 [13:35<16:24,  1.87s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 476/1000 [13:36<14:48,  1.70s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 477/1000 [13:38<15:53,  1.82s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 478/1000 [13:40<16:38,  1.91s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 479/1000 [13:42<14:55,  1.72s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 480/1000 [13:43<14:41,  1.69s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 481/1000 [13:45<14:06,  1.63s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 482/1000 [13:47<15:20,  1.78s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 483/1000 [13:49<16:06,  1.87s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 484/1000 [13:50<15:18,  1.78s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 485/1000 [13:52<13:55,  1.62s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 486/1000 [13:53<13:26,  1.57s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 487/1000 [13:54<12:33,  1.47s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 488/1000 [13:56<13:33,  1.59s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 489/1000 [13:57<12:21,  1.45s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 490/1000 [13:59<13:40,  1.61s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 491/1000 [14:02<14:58,  1.77s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 492/1000 [14:03<15:24,  1.82s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 493/1000 [14:05<14:45,  1.75s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 494/1000 [14:06<13:31,  1.60s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 495/1000 [14:08<13:27,  1.60s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 496/1000 [14:09<13:03,  1.55s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 497/1000 [14:11<12:17,  1.47s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 498/1000 [14:13<13:56,  1.67s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 499/1000 [14:15<14:37,  1.75s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 500/1000 [14:16<12:52,  1.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6307, 'learning_rate': 5.03e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m50%|█████     | 500/1000 [14:16<12:52,  1.55s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 501/1000 [14:18<14:17,  1.72s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 502/1000 [14:19<13:56,  1.68s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 503/1000 [14:22<15:00,  1.81s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 504/1000 [14:24<15:20,  1.86s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 505/1000 [14:25<13:47,  1.67s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 506/1000 [14:27<14:47,  1.80s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 507/1000 [14:28<13:59,  1.70s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 508/1000 [14:31<15:01,  1.83s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 509/1000 [14:32<13:59,  1.71s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 510/1000 [14:33<13:12,  1.62s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 511/1000 [14:35<13:11,  1.62s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 512/1000 [14:37<14:24,  1.77s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 513/1000 [14:39<13:38,  1.68s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 514/1000 [14:41<14:35,  1.80s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 515/1000 [14:42<12:28,  1.54s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 516/1000 [14:44<13:51,  1.72s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 517/1000 [14:45<13:01,  1.62s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 518/1000 [14:47<13:31,  1.68s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 519/1000 [14:48<12:54,  1.61s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 520/1000 [14:49<11:31,  1.44s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 521/1000 [14:51<11:48,  1.48s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 522/1000 [14:53<12:35,  1.58s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 523/1000 [14:55<13:50,  1.74s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 524/1000 [14:56<13:11,  1.66s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▎    | 525/1000 [14:58<13:38,  1.72s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 526/1000 [15:00<14:16,  1.81s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 527/1000 [15:01<12:49,  1.63s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 528/1000 [15:03<12:50,  1.63s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 529/1000 [15:05<13:55,  1.77s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 530/1000 [15:07<13:35,  1.73s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 531/1000 [15:08<13:12,  1.69s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 532/1000 [15:10<13:46,  1.77s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 533/1000 [15:13<14:33,  1.87s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 534/1000 [15:14<14:47,  1.90s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 535/1000 [15:16<14:57,  1.93s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 536/1000 [15:18<12:55,  1.67s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 537/1000 [15:20<13:55,  1.80s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 538/1000 [15:21<13:09,  1.71s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 539/1000 [15:23<12:37,  1.64s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 540/1000 [15:24<12:32,  1.64s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 541/1000 [15:26<12:51,  1.68s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 542/1000 [15:27<12:11,  1.60s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 543/1000 [15:29<12:15,  1.61s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 544/1000 [15:31<13:01,  1.71s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 545/1000 [15:32<11:57,  1.58s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 546/1000 [15:33<10:53,  1.44s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 547/1000 [15:35<11:12,  1.48s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 548/1000 [15:37<12:14,  1.62s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 549/1000 [15:39<12:12,  1.63s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 550/1000 [15:40<12:12,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6029, 'learning_rate': 4.53e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 550/1000 [15:40<12:12,  1.63s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 551/1000 [15:42<12:05,  1.62s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 552/1000 [15:43<12:07,  1.62s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 553/1000 [15:45<11:59,  1.61s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 554/1000 [15:46<10:45,  1.45s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 555/1000 [15:48<11:00,  1.49s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 556/1000 [15:50<12:24,  1.68s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 557/1000 [15:51<11:54,  1.61s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 558/1000 [15:53<11:54,  1.62s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 559/1000 [15:55<13:00,  1.77s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 560/1000 [15:56<12:14,  1.67s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 561/1000 [15:59<13:13,  1.81s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 562/1000 [16:01<13:52,  1.90s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 563/1000 [16:03<14:00,  1.92s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 564/1000 [16:04<13:19,  1.83s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 565/1000 [16:06<12:20,  1.70s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 566/1000 [16:08<12:53,  1.78s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 567/1000 [16:09<12:12,  1.69s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 568/1000 [16:11<12:05,  1.68s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 569/1000 [16:12<11:12,  1.56s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 570/1000 [16:14<11:41,  1.63s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 571/1000 [16:16<11:43,  1.64s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 572/1000 [16:18<12:43,  1.78s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 573/1000 [16:19<11:24,  1.60s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 574/1000 [16:20<11:28,  1.62s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▊    | 575/1000 [16:22<10:36,  1.50s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 576/1000 [16:24<11:53,  1.68s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 577/1000 [16:26<12:47,  1.82s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 578/1000 [16:28<12:50,  1.83s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 579/1000 [16:29<12:24,  1.77s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 580/1000 [16:31<12:24,  1.77s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 581/1000 [16:33<12:01,  1.72s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 582/1000 [16:34<11:20,  1.63s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 583/1000 [16:35<10:33,  1.52s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 584/1000 [16:37<09:56,  1.43s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 585/1000 [16:39<11:21,  1.64s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 586/1000 [16:40<11:20,  1.64s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 587/1000 [16:43<12:17,  1.79s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 588/1000 [16:45<12:56,  1.88s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 589/1000 [16:47<12:43,  1.86s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 590/1000 [16:48<11:28,  1.68s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 591/1000 [16:49<11:17,  1.66s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 592/1000 [16:51<12:03,  1.77s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 593/1000 [16:53<11:43,  1.73s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 594/1000 [16:55<12:30,  1.85s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 595/1000 [16:56<11:13,  1.66s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 596/1000 [16:59<12:03,  1.79s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 597/1000 [17:00<11:40,  1.74s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 598/1000 [17:02<11:25,  1.70s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 599/1000 [17:03<10:55,  1.64s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 600/1000 [17:05<10:57,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.643, 'learning_rate': 4.0300000000000004e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m60%|██████    | 600/1000 [17:05<10:57,  1.64s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 601/1000 [17:07<11:53,  1.79s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 602/1000 [17:09<11:16,  1.70s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 603/1000 [17:10<11:32,  1.74s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 604/1000 [17:11<10:11,  1.54s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 605/1000 [17:13<10:35,  1.61s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 606/1000 [17:15<11:35,  1.77s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 607/1000 [17:17<10:56,  1.67s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 608/1000 [17:18<09:50,  1.51s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 609/1000 [17:19<09:14,  1.42s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 610/1000 [17:21<09:36,  1.48s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 611/1000 [17:23<10:48,  1.67s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 612/1000 [17:24<10:18,  1.59s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 613/1000 [17:26<10:16,  1.59s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 614/1000 [17:27<09:54,  1.54s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 615/1000 [17:29<10:22,  1.62s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 616/1000 [17:31<10:58,  1.71s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 617/1000 [17:33<11:04,  1.74s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 618/1000 [17:34<10:46,  1.69s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 619/1000 [17:36<10:58,  1.73s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 620/1000 [17:38<11:02,  1.74s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 621/1000 [17:40<11:43,  1.86s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 622/1000 [17:42<10:57,  1.74s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 623/1000 [17:43<10:40,  1.70s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 624/1000 [17:44<09:48,  1.57s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 625/1000 [17:46<09:29,  1.52s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 626/1000 [17:48<10:36,  1.70s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 627/1000 [17:50<10:19,  1.66s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 628/1000 [17:52<11:08,  1.80s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 629/1000 [17:53<10:51,  1.76s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 630/1000 [17:55<10:36,  1.72s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 631/1000 [17:57<10:29,  1.71s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 632/1000 [17:59<11:14,  1.83s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 633/1000 [18:00<10:23,  1.70s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 634/1000 [18:02<10:17,  1.69s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 635/1000 [18:03<09:44,  1.60s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 636/1000 [18:05<10:40,  1.76s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 637/1000 [18:07<11:19,  1.87s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 638/1000 [18:09<10:05,  1.67s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 639/1000 [18:10<09:20,  1.55s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 640/1000 [18:12<09:22,  1.56s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 641/1000 [18:14<10:21,  1.73s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 642/1000 [18:16<11:01,  1.85s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 643/1000 [18:17<09:57,  1.67s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 644/1000 [18:19<10:31,  1.77s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 645/1000 [18:20<09:03,  1.53s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 646/1000 [18:21<08:33,  1.45s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 647/1000 [18:23<09:43,  1.65s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 648/1000 [18:25<10:31,  1.79s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 649/1000 [18:27<09:55,  1.70s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 650/1000 [18:29<09:46,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5594, 'learning_rate': 3.53e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 650/1000 [18:29<09:46,  1.67s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 651/1000 [18:30<09:38,  1.66s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 652/1000 [18:32<10:12,  1.76s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 653/1000 [18:34<09:56,  1.72s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 654/1000 [18:36<10:08,  1.76s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 655/1000 [18:38<10:15,  1.78s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 656/1000 [18:39<10:15,  1.79s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 657/1000 [18:41<10:01,  1.75s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 658/1000 [18:43<09:51,  1.73s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 659/1000 [18:44<09:14,  1.63s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 660/1000 [18:46<09:44,  1.72s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 661/1000 [18:48<10:23,  1.84s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 662/1000 [18:50<09:39,  1.72s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 663/1000 [18:52<10:20,  1.84s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 664/1000 [18:54<10:47,  1.93s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 665/1000 [18:55<09:23,  1.68s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 666/1000 [18:56<08:25,  1.51s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 667/1000 [18:57<07:41,  1.39s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 668/1000 [18:59<08:51,  1.60s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 669/1000 [19:01<09:42,  1.76s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 670/1000 [19:03<10:12,  1.86s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 671/1000 [19:05<09:27,  1.73s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 672/1000 [19:07<10:03,  1.84s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 673/1000 [19:09<10:29,  1.93s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 674/1000 [19:11<10:00,  1.84s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 675/1000 [19:12<08:55,  1.65s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 676/1000 [19:14<09:30,  1.76s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 677/1000 [19:15<08:41,  1.61s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 678/1000 [19:17<08:42,  1.62s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 679/1000 [19:19<09:01,  1.69s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 680/1000 [19:21<09:41,  1.82s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 681/1000 [19:23<10:09,  1.91s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 682/1000 [19:24<09:22,  1.77s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 683/1000 [19:26<08:45,  1.66s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 684/1000 [19:27<08:26,  1.60s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 685/1000 [19:29<08:31,  1.62s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 686/1000 [19:30<08:15,  1.58s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 687/1000 [19:33<09:06,  1.75s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 688/1000 [19:34<08:39,  1.66s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 689/1000 [19:36<08:32,  1.65s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 690/1000 [19:37<07:52,  1.52s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 691/1000 [19:39<08:15,  1.60s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 692/1000 [19:40<07:41,  1.50s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 693/1000 [19:41<07:33,  1.48s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 694/1000 [19:43<08:31,  1.67s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 695/1000 [19:46<09:12,  1.81s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 696/1000 [19:47<09:16,  1.83s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 697/1000 [19:50<09:41,  1.92s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 698/1000 [19:52<09:43,  1.93s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 699/1000 [19:53<08:21,  1.67s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 700/1000 [19:54<08:13,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6216, 'learning_rate': 3.03e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m70%|███████   | 700/1000 [19:54<08:13,  1.64s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 701/1000 [19:56<08:28,  1.70s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 702/1000 [19:57<07:33,  1.52s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 703/1000 [19:59<07:40,  1.55s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 704/1000 [20:00<07:45,  1.57s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 705/1000 [20:02<07:28,  1.52s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 706/1000 [20:04<08:05,  1.65s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 707/1000 [20:06<08:46,  1.80s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 708/1000 [20:08<08:49,  1.81s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 709/1000 [20:10<08:51,  1.83s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 710/1000 [20:11<08:50,  1.83s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 711/1000 [20:13<08:43,  1.81s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 712/1000 [20:15<08:22,  1.75s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 713/1000 [20:16<08:10,  1.71s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 714/1000 [20:18<07:49,  1.64s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 715/1000 [20:19<07:34,  1.59s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 716/1000 [20:21<07:47,  1.65s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 717/1000 [20:23<08:27,  1.79s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 718/1000 [20:25<07:56,  1.69s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 719/1000 [20:27<08:32,  1.82s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 720/1000 [20:28<08:12,  1.76s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 721/1000 [20:30<08:16,  1.78s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 722/1000 [20:32<07:50,  1.69s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 723/1000 [20:33<07:09,  1.55s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 724/1000 [20:35<07:12,  1.57s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▎  | 725/1000 [20:36<06:43,  1.47s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 726/1000 [20:38<07:11,  1.58s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 727/1000 [20:40<07:42,  1.69s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 728/1000 [20:41<07:36,  1.68s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 729/1000 [20:43<07:46,  1.72s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 730/1000 [20:44<07:17,  1.62s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 731/1000 [20:47<07:56,  1.77s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 732/1000 [20:48<07:14,  1.62s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 733/1000 [20:50<07:17,  1.64s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 734/1000 [20:51<07:33,  1.71s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 735/1000 [20:53<07:26,  1.68s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 736/1000 [20:55<07:07,  1.62s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 737/1000 [20:57<07:45,  1.77s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 738/1000 [20:59<08:12,  1.88s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 739/1000 [21:00<07:20,  1.69s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 740/1000 [21:01<06:59,  1.61s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 741/1000 [21:03<07:25,  1.72s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 742/1000 [21:05<07:05,  1.65s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 743/1000 [21:06<06:18,  1.47s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 744/1000 [21:08<07:07,  1.67s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 745/1000 [21:10<07:40,  1.80s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 746/1000 [21:12<08:01,  1.90s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 747/1000 [21:14<08:05,  1.92s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 748/1000 [21:16<08:06,  1.93s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 749/1000 [21:18<07:57,  1.90s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 750/1000 [21:20<07:48,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5465, 'learning_rate': 2.5300000000000002e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 750/1000 [21:20<07:48,  1.87s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 751/1000 [21:22<07:53,  1.90s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 752/1000 [21:23<07:18,  1.77s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 753/1000 [21:25<06:34,  1.60s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 754/1000 [21:26<06:04,  1.48s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 755/1000 [21:27<05:58,  1.46s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 756/1000 [21:29<06:20,  1.56s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 757/1000 [21:30<05:43,  1.42s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 758/1000 [21:32<05:55,  1.47s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 759/1000 [21:33<05:54,  1.47s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 760/1000 [21:35<06:18,  1.58s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 761/1000 [21:37<06:35,  1.65s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 762/1000 [21:39<07:07,  1.80s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 763/1000 [21:40<06:43,  1.70s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 764/1000 [21:42<07:12,  1.83s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 765/1000 [21:44<06:43,  1.72s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 766/1000 [21:46<06:34,  1.68s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "environment = {\n",
    "              'MODEL_S3_BUCKET': bucket # The bucket to store pretrained model and fine-tune model\n",
    "}\n",
    "\n",
    "base_job_name = 'chatglm-lora-demo'         \n",
    "\n",
    "instance_type = 'ml.g5.12xlarge'\n",
    "\n",
    "estimator = Estimator(role=role,\n",
    "                      entry_point='train.sh',\n",
    "                      source_dir='./ChatGLM-Tuning/',\n",
    "                      base_job_name=base_job_name,\n",
    "                      instance_count=1,\n",
    "                      instance_type=instance_type,\n",
    "                      image_uri=image_uri,\n",
    "                      environment=environment,\n",
    "                      script_mode=True,\n",
    "                      disable_profiler=True,\n",
    "                      debugger_hook_config=False,\n",
    "                      max_run=24*60*60*2)\n",
    "\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6740ffb-6951-46a2-a9da-8677261f9147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbea5eca-ff85-4226-95fb-3d0f979f5e83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
