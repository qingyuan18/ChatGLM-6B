{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "252de0de",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SageMaker fine tune ChatGLM\n",
    "\n",
    "#### 准备\n",
    "1. 升级boto3, sagemaker python sdk  \n",
    "2. 准备requirements.txt\n",
    "3. 准备s5cmd utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8f2c403",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (1.26.121)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.26.127-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3) (0.6.0)\n",
      "Collecting botocore<1.30.0,>=1.29.127\n",
      "  Downloading botocore-1.29.127-py3-none-any.whl (10.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.127->boto3) (1.26.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.127->boto3) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.127->boto3) (1.16.0)\n",
      "Installing collected packages: botocore, boto3\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.29.121\n",
      "    Uninstalling botocore-1.29.121:\n",
      "      Successfully uninstalled botocore-1.29.121\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.26.121\n",
      "    Uninstalling boto3-1.26.121:\n",
      "      Successfully uninstalled boto3-1.26.121\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.27.71 requires botocore==1.29.71, but you have botocore 1.29.127 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.26.127 botocore-1.29.127\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (2.150.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.152.0.tar.gz (751 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.1/751.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<23,>=20.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (22.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.28 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (1.26.127)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (1.23.5)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (3.20.2)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (4.13.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (1.4.4)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML==5.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (5.4.1)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (3.2.0)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (2.6.2)\n",
      "Requirement already satisfied: tblib==1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.127 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.29.127)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from jsonschema->sagemaker) (0.19.3)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from jsonschema->sagemaker) (65.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pandas->sagemaker) (2022.7)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.127->boto3<2.0,>=1.26.28->sagemaker) (1.26.8)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.152.0-py2.py3-none-any.whl size=1007493 sha256=9271f5144f6a7667fe5a008ea558812e5c6ae0e1ee18b78dc3013dc8aebeb1b1\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/d4/b1/6b/71a4277c52cfca15ce8e3f602504a610cb6a7c60fae434fd0e\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.150.0\n",
      "    Uninstalling sagemaker-2.150.0:\n",
      "      Successfully uninstalled sagemaker-2.150.0\n",
      "Successfully installed sagemaker-2.152.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade boto3\n",
    "!pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9436071c-6e98-4b38-aff8-507193445382",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 4176k  100 4176k    0     0  9255k      0 --:--:-- --:--:-- --:--:-- 9255k\n"
     ]
    }
   ],
   "source": [
    "#print('s3://{}/llm/models/'.format(sagemaker_session.default_bucket()))\n",
    "#!aws s3 ls s3://sagemaker-us-west-2-687912291502/llm/models/\n",
    "!curl -L https://github.com/peak/s5cmd/releases/download/v2.0.0/s5cmd_2.0.0_Linux-64bit.tar.gz | tar -xz && mv s5cmd ChatGLM-6B/ptuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4a30f3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::687912291502:role/service-role/AmazonSageMaker-ExecutionRole-20211013T113123\n",
      "sagemaker-us-west-2-687912291502\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region_name = boto3.session.Session().region_name\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(role)\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cbcac2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### chatglm 官方P-tuning v2方式（单机单卡）\n",
    "1:安装依赖lib   \n",
    "2:准备数据集(本例以ADGEN 文本生成数据集为例，将解压后的 AdvertiseGen 目录放到本目录  \n",
    "3:修改并bash运行 train.sh  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b07fa0a4-8e21-4441-a9b4-3a038f6c5cd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install rouge_chinese nltk jieba datasets\n",
    "#!git clone https://github.com/THUDM/ChatGLM-6B.git\n",
    "#!pip install -r ChatGLM-6B/requirements.txt\n",
    "!cp ChatGLM-6B/requirements.txt ChatGLM-6B/ptuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ca76f3d-e961-4c4c-84cc-6ea297711a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp ChatGLM-6B/ptuning/AdvertiseGen/dev.json s3://sagemaker-us-west-2-687912291502/llm/chatglm/datasets/dev.json\n",
      "cp ChatGLM-6B/ptuning/AdvertiseGen/train.json s3://sagemaker-us-west-2-687912291502/llm/chatglm/datasets/train.json\n"
     ]
    }
   ],
   "source": [
    "#!cd ChatGLM-6B/ptuning/ && wget \"https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1\"\n",
    "#!cd ChatGLM-6B/ptuning/ && mv \"index.html?dl=1\" dataset.tar.gz\n",
    "#!cd ChatGLM-6B/ptuning/ && tar -xvf dataset.tar.gz\n",
    "!./ChatGLM-6B/ptuning/s5cmd sync ChatGLM-6B/ptuning/AdvertiseGen/ s3://{bucket}/llm/chatglm/datasets/ \n",
    "#!rm -rf cd ChatGLM-6B/ptuning/dataset.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0d9e0f9-e025-4529-aadf-2f9d798b018b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-chatglm-simple-2023-05-06-1-2023-05-06-14-24-12-728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-06 14:24:38 Starting - Starting the training job...\n",
      "2023-05-06 14:24:53 Starting - Preparing the instances for training......\n",
      "2023-05-06 14:26:04 Downloading - Downloading input data\n",
      "2023-05-06 14:26:04 Training - Downloading the training image..................\n",
      "2023-05-06 14:28:55 Training - Training image download completed. Training in progress...\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-05-06 14:29:21,359 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-05-06 14:29:21,377 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-06 14:29:21,388 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-05-06 14:29:21,390 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-05-06 14:29:25,803 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (3.20.2)\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.27.1\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.7/6.7 MB 22.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting cpm_kernels\u001b[0m\n",
      "\u001b[34mDownloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 416.6/416.6 kB 37.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mCollecting gradio\u001b[0m\n",
      "\u001b[34mDownloading gradio-3.28.3-py3-none-any.whl (17.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 69.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting mdtex2html\u001b[0m\n",
      "\u001b[34mDownloading mdtex2html-1.2.0-py3-none-any.whl (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (0.1.97)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (0.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (2.9.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface\u001b[0m\n",
      "\u001b[34mDownloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting jieba\u001b[0m\n",
      "\u001b[34mDownloading jieba-0.42.1.tar.gz (19.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/19.2 MB 80.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting rouge_chinese\u001b[0m\n",
      "\u001b[34mDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\u001b[0m\n",
      "\u001b[34mCollecting nltk\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 100.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting deepspeed==0.8.0\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.8.0.tar.gz (749 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 749.9/749.9 kB 84.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (0.13.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (2022.10.31)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (3.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.0->-r requirements.txt (line 14)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.0->-r requirements.txt (line 14)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.0->-r requirements.txt (line 14)) (5.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.0->-r requirements.txt (line 14)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.0->-r requirements.txt (line 14)) (1.10.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.10->-r requirements.txt (line 4)) (4.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (1.5.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments>=2.12.0 in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (2.14.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.11.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 43.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting markdown-it-py[linkify]>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.5/84.5 kB 28.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting python-multipart\u001b[0m\n",
      "\u001b[34mDownloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.7/45.7 kB 15.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting orjson\u001b[0m\n",
      "\u001b[34mDownloading orjson-3.8.11-cp39-cp39-manylinux_2_28_x86_64.whl (135 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.6/135.6 kB 32.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markupsafe in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (2.1.2)\u001b[0m\n",
      "\u001b[34mCollecting fastapi\u001b[0m\n",
      "\u001b[34mDownloading fastapi-0.95.1-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.0/57.0 kB 18.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting semantic-version\u001b[0m\n",
      "\u001b[34mDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mCollecting gradio-client>=0.1.3\u001b[0m\n",
      "\u001b[34mDownloading gradio_client-0.2.0-py3-none-any.whl (287 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 287.9/287.9 kB 57.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (3.1.2)\u001b[0m\n",
      "\u001b[34mCollecting mdit-py-plugins<=0.3.3\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.5/50.5 kB 13.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting uvicorn\u001b[0m\n",
      "\u001b[34mDownloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 17.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (9.4.0)\u001b[0m\n",
      "\u001b[34mCollecting websockets>=10.0\u001b[0m\n",
      "\u001b[34mDownloading websockets-11.0.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.7/129.7 kB 34.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting httpx\u001b[0m\n",
      "\u001b[34mDownloading httpx-0.24.0-py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.3/75.3 kB 26.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiofiles\u001b[0m\n",
      "\u001b[34mDownloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mCollecting ffmpy\u001b[0m\n",
      "\u001b[34mDownloading ffmpy-0.3.0.tar.gz (4.8 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting altair>=4.2.0\u001b[0m\n",
      "\u001b[34mDownloading altair-4.2.2-py3-none-any.whl (813 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 813.6/813.6 kB 81.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (3.6.3)\u001b[0m\n",
      "\u001b[34mCollecting pydub\u001b[0m\n",
      "\u001b[34mDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\u001b[0m\n",
      "\u001b[34mCollecting latex2mathml\u001b[0m\n",
      "\u001b[34mDownloading latex2mathml-3.75.3-py3-none-any.whl (73 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.2/73.2 kB 25.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown in /opt/conda/lib/python3.9/site-packages (from mdtex2html->-r requirements.txt (line 6)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (11.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (2023.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from rouge_chinese->-r requirements.txt (line 12)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 13)) (8.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 13)) (1.2.0)\u001b[0m\n",
      "\u001b[34mCollecting entrypoints\u001b[0m\n",
      "\u001b[34mDownloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: toolz in /opt/conda/lib/python3.9/site-packages (from altair>=4.2.0->gradio->-r requirements.txt (line 5)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.9/site-packages (from altair>=4.2.0->gradio->-r requirements.txt (line 5)) (4.17.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (1.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (1.3.3)\u001b[0m\n",
      "\u001b[34mCollecting mdurl~=0.1\u001b[0m\n",
      "\u001b[34mDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting linkify-it-py<3,>=1\u001b[0m\n",
      "\u001b[34mDownloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->gradio->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->gradio->-r requirements.txt (line 5)) (2022.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 2)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 2)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 2)) (3.4)\u001b[0m\n",
      "\u001b[34mCollecting starlette<0.27.0,>=0.26.1\u001b[0m\n",
      "\u001b[34mDownloading starlette-0.26.1-py3-none-any.whl (66 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.9/66.9 kB 18.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting sniffio\u001b[0m\n",
      "\u001b[34mDownloading sniffio-1.3.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting httpcore<0.18.0,>=0.15.0\u001b[0m\n",
      "\u001b[34mDownloading httpcore-0.17.0-py3-none-any.whl (70 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 70.6/70.6 kB 26.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown->mdtex2html->-r requirements.txt (line 6)) (4.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 5)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 5)) (0.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 5)) (1.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 5)) (1.0.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 5)) (4.38.0)\u001b[0m\n",
      "\u001b[34mCollecting h11>=0.8\u001b[0m\n",
      "\u001b[34mDownloading h11-0.14.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 20.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting anyio<5.0,>=3.0\u001b[0m\n",
      "\u001b[34mDownloading anyio-3.6.2-py3-none-any.whl (80 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.6/80.6 kB 25.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown->mdtex2html->-r requirements.txt (line 6)) (3.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio->-r requirements.txt (line 5)) (0.19.3)\u001b[0m\n",
      "\u001b[34mCollecting uc-micro-py\u001b[0m\n",
      "\u001b[34mDownloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed, jieba, ffmpy\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.8.0-py3-none-any.whl size=752128 sha256=d77b66cd81963f544d7d2b7c11563960c188fc0a21eaa421b6bededbc67ee419\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/ba/27/48/edac683ddf3d54d4a619ca460375bd226bf1db4a627d681b7c\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=4c39388489ead8e4957ede8fdd1371c61ebe29fe64ec6a19c341c5777527fdaa\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/7d/74/cf/08c94db4b784e2c1ef675a600b7b5b281fd25240dcb954ee7e\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4693 sha256=043a6b6cb3fafeccb222ec0e771ad5afd878ba53b5d750deb2277a4196b1783d\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/91/e2/96/f676aa08bfd789328c6576cd0f1fde4a3d686703bb0c247697\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed jieba ffmpy\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pydub, jieba, huggingface, ffmpy, cpm_kernels, websockets, uc-micro-py, sniffio, semantic-version, rouge_chinese, python-multipart, orjson, nltk, mdurl, latex2mathml, h11, entrypoints, aiofiles, uvicorn, markdown-it-py, linkify-it-py, huggingface-hub, deepspeed, anyio, transformers, starlette, mdtex2html, mdit-py-plugins, httpcore, altair, httpx, fastapi, gradio-client, gradio\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.12.0\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.12.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.12.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+06f2048\u001b[0m\n",
      "\u001b[34mUninstalling deepspeed-0.6.1+06f2048:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled deepspeed-0.6.1+06f2048\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.26.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.26.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.26.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiofiles-23.1.0 altair-4.2.2 anyio-3.6.2 cpm_kernels-1.0.11 deepspeed-0.8.0 entrypoints-0.4 fastapi-0.95.1 ffmpy-0.3.0 gradio-3.28.3 gradio-client-0.2.0 h11-0.14.0 httpcore-0.17.0 httpx-0.24.0 huggingface-0.0.1 huggingface-hub-0.14.1 jieba-0.42.1 latex2mathml-3.75.3 linkify-it-py-2.0.2 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 mdtex2html-1.2.0 mdurl-0.1.2 nltk-3.8.1 orjson-3.8.11 pydub-0.25.1 python-multipart-0.0.6 rouge_chinese-1.0.3 semantic-version-2.10.0 sniffio-1.3.0 starlette-0.26.1 transformers-4.27.1 uc-micro-py-1.0.2 uvicorn-0.22.0 websockets-11.0.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.1.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-05-06 14:29:53,887 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-05-06 14:29:53,887 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-05-06 14:29:53,910 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-06 14:29:53,942 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-06 14:29:53,974 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-06 14:29:53,986 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"AdvertiseGen\": \"/opt/ml/input/data/AdvertiseGen\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"AdvertiseGen\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-chatglm-simple-2023-05-06-1-2023-05-06-14-24-12-728\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-simple-2023-05-06-1-2023-05-06-14-24-12-728/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"start_simple\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"start_simple.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=start_simple.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"AdvertiseGen\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"AdvertiseGen\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=start_simple\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-simple-2023-05-06-1-2023-05-06-14-24-12-728/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"AdvertiseGen\":\"/opt/ml/input/data/AdvertiseGen\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"AdvertiseGen\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-chatglm-simple-2023-05-06-1-2023-05-06-14-24-12-728\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-simple-2023-05-06-1-2023-05-06-14-24-12-728/source/sourcedir.tar.gz\",\"module_name\":\"start_simple\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"start_simple.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_ADVERTISEGEN=/opt/ml/input/data/AdvertiseGen\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 start_simple.py\u001b[0m\n",
      "\u001b[34m[2023-05-06 14:29:58.513: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.27.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-05-06 14:29:58,518 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-05-06 14:29:58,542 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m05/06/2023 14:30:03 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m05/06/2023 14:30:03 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=False,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdo_eval=False,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=no,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgeneration_max_length=None,\u001b[0m\n",
      "\u001b[34mgeneration_num_beams=None,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=16,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=False,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=0.02,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=-1,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/opt/ml/model/adgen-chatglm-6b-ft/runs/May06_14-30-03_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=10,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=linear,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=100,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=3.0,\u001b[0m\n",
      "\u001b[34moptim=adamw_hf,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moutput_dir=/opt/ml/model/adgen-chatglm-6b-ft,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=True,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=1,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=1,\u001b[0m\n",
      "\u001b[34mpredict_with_generate=True,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/opt/ml/model/adgen-chatglm-6b-ft,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_steps=100,\u001b[0m\n",
      "\u001b[34msave_strategy=steps,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=42,\u001b[0m\n",
      "\u001b[34msharded_ddp=[],\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34msortish_sampler=False,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,\u001b[0m\n",
      "\u001b[34mxpu_backend=None,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m05/06/2023 14:30:03 - WARNING - datasets.builder - Using custom data configuration default-e141d551345fe989\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-e141d551345fe989/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 2/2 [00:00<00:00, 8830.11it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 2/2 [00:00<00:00, 1727.47it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 111750 examples [00:00, 908986.57 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating validation split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-e141d551345fe989/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 541.17it/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/773 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 773/773 [00:00<00:00, 129kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:668] 2023-05-06 14:30:04,090 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/658202d88ac4bb782b99e99ac3adff58b4d0b813/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:668] 2023-05-06 14:30:04,090 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/658202d88ac4bb782b99e99ac3adff58b4d0b813/config.json\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:905] 2023-05-06 14:30:04,091 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:905] 2023-05-06 14:30:04,091 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mDownloading (…)iguration_chatglm.py:   0%|          | 0.00/4.28k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)iguration_chatglm.py: 100%|██████████| 4.28k/4.28k [00:00<00:00, 2.52MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:668] 2023-05-06 14:30:04,551 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/658202d88ac4bb782b99e99ac3adff58b4d0b813/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:668] 2023-05-06 14:30:04,551 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/658202d88ac4bb782b99e99ac3adff58b4d0b813/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:720] 2023-05-06 14:30:04,552 >> Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm-6b\",\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\"\n",
      "  },\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"gmask_token_id\": 130001,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"inner_hidden_size\": 16384,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"mask_token_id\": 130000,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_encoding_2d\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 130528\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:720] 2023-05-06 14:30:04,552 >> Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm-6b\",\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\"\n",
      "  },\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"gmask_token_id\": 130001,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"inner_hidden_size\": 16384,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"mask_token_id\": 130000,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_encoding_2d\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 130528\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json:   0%|          | 0.00/441 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json: 100%|██████████| 441/441 [00:00<00:00, 84.9kB/s]\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:652] 2023-05-06 14:30:04,738 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:652] 2023-05-06 14:30:04,738 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mDownloading (…)enization_chatglm.py:   0%|          | 0.00/16.7k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)enization_chatglm.py: 100%|██████████| 16.7k/16.7k [00:00<00:00, 9.67MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ice_text.model:   0%|          | 0.00/2.71M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ice_text.model: 100%|██████████| 2.71M/2.71M [00:00<00:00, 53.7MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-05-06 14:30:05,468 >> loading file ice_text.model from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/658202d88ac4bb782b99e99ac3adff58b4d0b813/ice_text.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-05-06 14:30:05,468 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-05-06 14:30:05,468 >> loading file special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-05-06 14:30:05,468 >> loading file ice_text.model from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/658202d88ac4bb782b99e99ac3adff58b4d0b813/ice_text.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-05-06 14:30:05,468 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-05-06 14:30:05,468 >> loading file special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-05-06 14:30:05,468 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/658202d88ac4bb782b99e99ac3adff58b4d0b813/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-05-06 14:30:05,468 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/658202d88ac4bb782b99e99ac3adff58b4d0b813/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-06 14:30:05,695 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-06 14:30:05,695 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mDownloading (…)/modeling_chatglm.py:   0%|          | 0.00/57.6k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/modeling_chatglm.py: 100%|██████████| 57.6k/57.6k [00:00<00:00, 9.68MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)main/quantization.py:   0%|          | 0.00/15.1k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)main/quantization.py: 100%|██████████| 15.1k/15.1k [00:00<00:00, 8.94MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)model.bin.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)model.bin.index.json: 100%|██████████| 33.4k/33.4k [00:00<00:00, 6.34MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2403] 2023-05-06 14:30:06,985 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/658202d88ac4bb782b99e99ac3adff58b4d0b813/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2403] 2023-05-06 14:30:06,985 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/658202d88ac4bb782b99e99ac3adff58b4d0b813/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   0%|          | 0.00/1.74G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   1%|          | 10.5M/1.74G [00:00<00:21, 80.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   1%|          | 21.0M/1.74G [00:00<00:19, 87.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   2%|▏         | 31.5M/1.74G [00:00<00:18, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   2%|▏         | 41.9M/1.74G [00:00<00:20, 82.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   3%|▎         | 52.4M/1.74G [00:00<00:20, 84.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   4%|▎         | 62.9M/1.74G [00:00<00:19, 87.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   4%|▍         | 73.4M/1.74G [00:00<00:19, 84.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   5%|▍         | 83.9M/1.74G [00:00<00:19, 83.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   5%|▌         | 94.4M/1.74G [00:01<00:19, 86.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   6%|▌         | 105M/1.74G [00:01<00:18, 88.2MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   7%|▋         | 115M/1.74G [00:01<00:17, 90.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   7%|▋         | 126M/1.74G [00:01<00:17, 91.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   8%|▊         | 136M/1.74G [00:01<00:17, 92.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   8%|▊         | 147M/1.74G [00:01<00:17, 88.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   9%|▉         | 157M/1.74G [00:01<00:17, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  10%|▉         | 168M/1.74G [00:01<00:17, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  10%|█         | 178M/1.74G [00:02<00:18, 84.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  11%|█         | 189M/1.74G [00:02<00:18, 85.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  11%|█▏        | 199M/1.74G [00:02<00:19, 80.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  12%|█▏        | 210M/1.74G [00:02<00:18, 84.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  13%|█▎        | 220M/1.74G [00:02<00:17, 86.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  13%|█▎        | 231M/1.74G [00:02<00:17, 86.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  14%|█▍        | 241M/1.74G [00:02<00:17, 87.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  14%|█▍        | 252M/1.74G [00:02<00:16, 87.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  15%|█▌        | 262M/1.74G [00:03<00:16, 87.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  16%|█▌        | 273M/1.74G [00:03<00:16, 88.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  16%|█▋        | 283M/1.74G [00:03<00:16, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  17%|█▋        | 294M/1.74G [00:03<00:16, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  17%|█▋        | 304M/1.74G [00:03<00:15, 91.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  18%|█▊        | 315M/1.74G [00:03<00:15, 92.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  19%|█▊        | 325M/1.74G [00:03<00:15, 91.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  19%|█▉        | 336M/1.74G [00:03<00:15, 92.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  20%|█▉        | 346M/1.74G [00:03<00:15, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  20%|██        | 357M/1.74G [00:04<00:14, 92.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  21%|██        | 367M/1.74G [00:04<00:15, 90.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  22%|██▏       | 377M/1.74G [00:04<00:15, 90.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  22%|██▏       | 388M/1.74G [00:04<00:15, 88.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  23%|██▎       | 398M/1.74G [00:04<00:15, 86.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  23%|██▎       | 409M/1.74G [00:04<00:15, 87.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  24%|██▍       | 419M/1.74G [00:04<00:14, 88.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  25%|██▍       | 430M/1.74G [00:04<00:15, 86.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  25%|██▌       | 440M/1.74G [00:05<00:15, 86.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  26%|██▌       | 451M/1.74G [00:05<00:14, 88.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  27%|██▋       | 461M/1.74G [00:05<00:14, 89.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  27%|██▋       | 472M/1.74G [00:05<00:14, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  28%|██▊       | 482M/1.74G [00:05<00:13, 91.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  28%|██▊       | 493M/1.74G [00:05<00:13, 92.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  29%|██▉       | 503M/1.74G [00:05<00:13, 93.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  30%|██▉       | 514M/1.74G [00:05<00:13, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  30%|███       | 524M/1.74G [00:05<00:13, 88.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  31%|███       | 535M/1.74G [00:06<00:13, 88.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  31%|███▏      | 545M/1.74G [00:06<00:13, 89.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  32%|███▏      | 556M/1.74G [00:06<00:13, 86.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  33%|███▎      | 566M/1.74G [00:06<00:13, 87.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  33%|███▎      | 577M/1.74G [00:06<00:13, 89.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  34%|███▎      | 587M/1.74G [00:06<00:12, 90.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  34%|███▍      | 598M/1.74G [00:06<00:12, 88.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  35%|███▍      | 608M/1.74G [00:06<00:12, 89.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  36%|███▌      | 619M/1.74G [00:06<00:12, 90.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  36%|███▌      | 629M/1.74G [00:07<00:12, 89.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  37%|███▋      | 640M/1.74G [00:07<00:12, 90.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  37%|███▋      | 650M/1.74G [00:07<00:11, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  38%|███▊      | 661M/1.74G [00:07<00:11, 91.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  39%|███▊      | 671M/1.74G [00:07<00:11, 91.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  39%|███▉      | 682M/1.74G [00:07<00:11, 89.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  40%|███▉      | 692M/1.74G [00:07<00:11, 90.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  40%|████      | 703M/1.74G [00:07<00:11, 91.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  41%|████      | 713M/1.74G [00:08<00:11, 92.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  42%|████▏     | 724M/1.74G [00:08<00:11, 89.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  42%|████▏     | 734M/1.74G [00:08<00:11, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  43%|████▎     | 744M/1.74G [00:08<00:10, 90.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  43%|████▎     | 755M/1.74G [00:08<00:11, 88.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  44%|████▍     | 765M/1.74G [00:08<00:10, 89.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  45%|████▍     | 776M/1.74G [00:08<00:10, 89.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  45%|████▌     | 786M/1.74G [00:08<00:11, 86.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  46%|████▌     | 797M/1.74G [00:08<00:10, 87.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  46%|████▋     | 807M/1.74G [00:09<00:10, 86.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  47%|████▋     | 818M/1.74G [00:09<00:10, 85.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  48%|████▊     | 828M/1.74G [00:09<00:10, 87.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  48%|████▊     | 839M/1.74G [00:09<00:10, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  49%|████▉     | 849M/1.74G [00:09<00:09, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  49%|████▉     | 860M/1.74G [00:09<00:09, 91.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  50%|████▉     | 870M/1.74G [00:09<00:09, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  51%|█████     | 881M/1.74G [00:09<00:09, 92.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  51%|█████     | 891M/1.74G [00:10<00:09, 90.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  52%|█████▏    | 902M/1.74G [00:10<00:09, 90.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  52%|█████▏    | 912M/1.74G [00:10<00:09, 90.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  53%|█████▎    | 923M/1.74G [00:10<00:13, 62.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  54%|█████▎    | 933M/1.74G [00:10<00:11, 67.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  54%|█████▍    | 944M/1.74G [00:10<00:10, 73.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  55%|█████▍    | 954M/1.74G [00:10<00:10, 77.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  55%|█████▌    | 965M/1.74G [00:11<00:10, 76.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  56%|█████▌    | 975M/1.74G [00:11<00:10, 73.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  57%|█████▋    | 986M/1.74G [00:11<00:09, 76.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  57%|█████▋    | 996M/1.74G [00:11<00:10, 72.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  58%|█████▊    | 1.01G/1.74G [00:11<00:10, 72.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  58%|█████▊    | 1.02G/1.74G [00:11<00:10, 69.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  59%|█████▉    | 1.03G/1.74G [00:11<00:09, 73.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  60%|█████▉    | 1.04G/1.74G [00:12<00:09, 74.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  60%|██████    | 1.05G/1.74G [00:12<00:09, 76.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  61%|██████    | 1.06G/1.74G [00:12<00:08, 77.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  61%|██████▏   | 1.07G/1.74G [00:12<00:08, 75.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  62%|██████▏   | 1.08G/1.74G [00:12<00:08, 79.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  63%|██████▎   | 1.09G/1.74G [00:12<00:07, 82.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  63%|██████▎   | 1.10G/1.74G [00:12<00:07, 84.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  64%|██████▍   | 1.11G/1.74G [00:12<00:07, 86.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  64%|██████▍   | 1.12G/1.74G [00:13<00:07, 86.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  65%|██████▌   | 1.13G/1.74G [00:13<00:06, 87.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  66%|██████▌   | 1.14G/1.74G [00:13<00:06, 89.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  66%|██████▋   | 1.15G/1.74G [00:13<00:06, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  67%|██████▋   | 1.16G/1.74G [00:13<00:06, 91.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  67%|██████▋   | 1.17G/1.74G [00:13<00:06, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  68%|██████▊   | 1.18G/1.74G [00:13<00:06, 89.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  69%|██████▊   | 1.20G/1.74G [00:13<00:06, 85.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  69%|██████▉   | 1.21G/1.74G [00:13<00:06, 86.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  70%|██████▉   | 1.22G/1.74G [00:14<00:06, 87.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  70%|███████   | 1.23G/1.74G [00:14<00:06, 81.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  71%|███████   | 1.24G/1.74G [00:14<00:06, 78.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  72%|███████▏  | 1.25G/1.74G [00:14<00:05, 82.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  72%|███████▏  | 1.26G/1.74G [00:14<00:05, 84.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  73%|███████▎  | 1.27G/1.74G [00:14<00:05, 86.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  73%|███████▎  | 1.28G/1.74G [00:14<00:05, 88.7MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  74%|███████▍  | 1.29G/1.74G [00:14<00:05, 89.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  75%|███████▍  | 1.30G/1.74G [00:15<00:04, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  75%|███████▌  | 1.31G/1.74G [00:15<00:04, 89.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  76%|███████▌  | 1.32G/1.74G [00:15<00:04, 87.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  77%|███████▋  | 1.33G/1.74G [00:15<00:04, 88.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  77%|███████▋  | 1.34G/1.74G [00:15<00:04, 89.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  78%|███████▊  | 1.35G/1.74G [00:15<00:04, 89.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  78%|███████▊  | 1.36G/1.74G [00:15<00:04, 90.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  79%|███████▉  | 1.37G/1.74G [00:15<00:04, 91.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  80%|███████▉  | 1.38G/1.74G [00:16<00:03, 90.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  80%|████████  | 1.39G/1.74G [00:16<00:03, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  81%|████████  | 1.41G/1.74G [00:16<00:03, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  81%|████████▏ | 1.42G/1.74G [00:16<00:03, 89.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  82%|████████▏ | 1.43G/1.74G [00:16<00:03, 89.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  83%|████████▎ | 1.44G/1.74G [00:16<00:03, 90.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  83%|████████▎ | 1.45G/1.74G [00:16<00:03, 89.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  84%|████████▎ | 1.46G/1.74G [00:16<00:03, 90.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  84%|████████▍ | 1.47G/1.74G [00:16<00:03, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  85%|████████▍ | 1.48G/1.74G [00:17<00:03, 86.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  86%|████████▌ | 1.49G/1.74G [00:17<00:02, 85.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  86%|████████▌ | 1.50G/1.74G [00:17<00:02, 85.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  87%|████████▋ | 1.51G/1.74G [00:17<00:02, 80.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  87%|████████▋ | 1.52G/1.74G [00:17<00:02, 82.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  88%|████████▊ | 1.53G/1.74G [00:17<00:02, 79.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  89%|████████▊ | 1.54G/1.74G [00:17<00:02, 79.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  89%|████████▉ | 1.55G/1.74G [00:18<00:02, 81.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  90%|████████▉ | 1.56G/1.74G [00:18<00:02, 80.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  90%|█████████ | 1.57G/1.74G [00:18<00:02, 82.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  91%|█████████ | 1.58G/1.74G [00:18<00:01, 82.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  92%|█████████▏| 1.59G/1.74G [00:18<00:01, 84.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  92%|█████████▏| 1.60G/1.74G [00:18<00:01, 85.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  93%|█████████▎| 1.61G/1.74G [00:18<00:01, 85.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  93%|█████████▎| 1.63G/1.74G [00:18<00:01, 86.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  94%|█████████▍| 1.64G/1.74G [00:19<00:01, 84.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  95%|█████████▍| 1.65G/1.74G [00:19<00:01, 82.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  95%|█████████▌| 1.66G/1.74G [00:19<00:01, 83.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  96%|█████████▌| 1.67G/1.74G [00:19<00:00, 84.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  96%|█████████▋| 1.68G/1.74G [00:19<00:00, 85.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  97%|█████████▋| 1.69G/1.74G [00:19<00:00, 87.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  98%|█████████▊| 1.70G/1.74G [00:19<00:00, 84.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  98%|█████████▊| 1.71G/1.74G [00:19<00:00, 86.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  99%|█████████▉| 1.72G/1.74G [00:19<00:00, 84.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  99%|█████████▉| 1.73G/1.74G [00:20<00:00, 86.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin: 100%|█████████▉| 1.74G/1.74G [00:20<00:00, 88.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin: 100%|██████████| 1.74G/1.74G [00:20<00:00, 86.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  12%|█▎        | 1/8 [00:20<02:22, 20.33s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   1%|          | 10.5M/1.88G [00:00<00:23, 78.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   1%|          | 21.0M/1.88G [00:00<00:21, 85.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   2%|▏         | 31.5M/1.88G [00:00<00:20, 90.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   2%|▏         | 41.9M/1.88G [00:00<00:20, 91.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   3%|▎         | 52.4M/1.88G [00:00<00:19, 92.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   3%|▎         | 62.9M/1.88G [00:00<00:20, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   4%|▍         | 73.4M/1.88G [00:00<00:19, 90.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   4%|▍         | 83.9M/1.88G [00:00<00:19, 91.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   5%|▌         | 94.4M/1.88G [00:01<00:19, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   6%|▌         | 105M/1.88G [00:01<00:19, 91.1MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   6%|▌         | 115M/1.88G [00:01<00:29, 60.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   8%|▊         | 147M/1.88G [00:01<00:18, 94.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   8%|▊         | 157M/1.88G [00:01<00:18, 93.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   9%|▉         | 168M/1.88G [00:01<00:18, 93.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   9%|▉         | 178M/1.88G [00:02<00:18, 91.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  10%|█         | 189M/1.88G [00:02<00:18, 91.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  11%|█         | 199M/1.88G [00:02<00:18, 92.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  11%|█         | 210M/1.88G [00:02<00:17, 92.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  12%|█▏        | 220M/1.88G [00:02<00:18, 91.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  12%|█▏        | 231M/1.88G [00:02<00:17, 91.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  13%|█▎        | 241M/1.88G [00:02<00:17, 92.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  13%|█▎        | 252M/1.88G [00:02<00:17, 91.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  14%|█▍        | 262M/1.88G [00:02<00:17, 92.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  15%|█▍        | 273M/1.88G [00:03<00:17, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  15%|█▌        | 283M/1.88G [00:03<00:17, 88.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  16%|█▌        | 294M/1.88G [00:03<00:17, 90.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  16%|█▌        | 304M/1.88G [00:03<00:17, 91.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  17%|█▋        | 315M/1.88G [00:03<00:17, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  17%|█▋        | 325M/1.88G [00:03<00:17, 87.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  18%|█▊        | 336M/1.88G [00:03<00:17, 89.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  18%|█▊        | 346M/1.88G [00:03<00:17, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  19%|█▉        | 357M/1.88G [00:03<00:16, 91.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  20%|█▉        | 367M/1.88G [00:04<00:16, 92.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  20%|██        | 377M/1.88G [00:04<00:16, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  21%|██        | 388M/1.88G [00:04<00:16, 91.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  21%|██        | 398M/1.88G [00:04<00:16, 92.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  22%|██▏       | 409M/1.88G [00:04<00:15, 93.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  22%|██▏       | 419M/1.88G [00:04<00:15, 93.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  23%|██▎       | 430M/1.88G [00:04<00:15, 92.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  23%|██▎       | 440M/1.88G [00:04<00:16, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  24%|██▍       | 451M/1.88G [00:05<00:16, 88.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  25%|██▍       | 461M/1.88G [00:05<00:15, 90.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  25%|██▌       | 472M/1.88G [00:05<00:15, 90.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  26%|██▌       | 482M/1.88G [00:05<00:15, 92.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  26%|██▌       | 493M/1.88G [00:05<00:14, 92.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  27%|██▋       | 503M/1.88G [00:05<00:14, 92.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  27%|██▋       | 514M/1.88G [00:05<00:14, 92.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  28%|██▊       | 524M/1.88G [00:05<00:14, 92.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  28%|██▊       | 535M/1.88G [00:05<00:14, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  29%|██▉       | 545M/1.88G [00:06<00:15, 84.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  30%|██▉       | 556M/1.88G [00:06<00:15, 86.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  30%|███       | 566M/1.88G [00:06<00:15, 86.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  31%|███       | 577M/1.88G [00:06<00:14, 88.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  31%|███       | 587M/1.88G [00:06<00:15, 85.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  32%|███▏      | 598M/1.88G [00:06<00:14, 86.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  32%|███▏      | 608M/1.88G [00:06<00:14, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  33%|███▎      | 619M/1.88G [00:06<00:14, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  33%|███▎      | 629M/1.88G [00:07<00:14, 88.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  34%|███▍      | 640M/1.88G [00:07<00:13, 90.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  35%|███▍      | 650M/1.88G [00:07<00:13, 91.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  35%|███▌      | 661M/1.88G [00:07<00:13, 91.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  36%|███▌      | 671M/1.88G [00:07<00:13, 90.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  36%|███▋      | 682M/1.88G [00:07<00:13, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  37%|███▋      | 692M/1.88G [00:07<00:13, 87.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  37%|███▋      | 703M/1.88G [00:07<00:13, 86.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  38%|███▊      | 713M/1.88G [00:07<00:13, 85.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  38%|███▊      | 724M/1.88G [00:08<00:13, 87.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  39%|███▉      | 734M/1.88G [00:08<00:13, 87.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  40%|███▉      | 744M/1.88G [00:08<00:13, 86.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  40%|████      | 755M/1.88G [00:08<00:12, 88.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  41%|████      | 765M/1.88G [00:08<00:12, 88.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  41%|████▏     | 776M/1.88G [00:08<00:12, 90.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  42%|████▏     | 786M/1.88G [00:08<00:12, 88.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  42%|████▏     | 797M/1.88G [00:08<00:12, 89.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  43%|████▎     | 807M/1.88G [00:09<00:12, 88.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  44%|████▎     | 818M/1.88G [00:09<00:11, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  44%|████▍     | 828M/1.88G [00:09<00:11, 88.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  45%|████▍     | 839M/1.88G [00:09<00:11, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  45%|████▌     | 849M/1.88G [00:09<00:11, 91.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  46%|████▌     | 860M/1.88G [00:09<00:11, 91.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  46%|████▋     | 870M/1.88G [00:09<00:10, 92.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  47%|████▋     | 881M/1.88G [00:09<00:11, 90.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  47%|████▋     | 891M/1.88G [00:09<00:11, 88.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  48%|████▊     | 902M/1.88G [00:10<00:10, 90.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  49%|████▊     | 912M/1.88G [00:10<00:10, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  49%|████▉     | 923M/1.88G [00:10<00:10, 89.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  50%|████▉     | 933M/1.88G [00:10<00:10, 91.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  50%|█████     | 944M/1.88G [00:10<00:10, 90.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  51%|█████     | 954M/1.88G [00:10<00:10, 84.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  51%|█████▏    | 965M/1.88G [00:10<00:10, 83.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  52%|█████▏    | 975M/1.88G [00:10<00:10, 85.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  52%|█████▏    | 986M/1.88G [00:11<00:10, 87.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  53%|█████▎    | 996M/1.88G [00:11<00:10, 87.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  54%|█████▎    | 1.01G/1.88G [00:11<00:09, 90.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  54%|█████▍    | 1.02G/1.88G [00:11<00:09, 90.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  55%|█████▍    | 1.03G/1.88G [00:11<00:09, 91.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  55%|█████▌    | 1.04G/1.88G [00:11<00:09, 91.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  56%|█████▌    | 1.05G/1.88G [00:11<00:09, 90.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  56%|█████▋    | 1.06G/1.88G [00:11<00:09, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  57%|█████▋    | 1.07G/1.88G [00:11<00:09, 89.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  57%|█████▋    | 1.08G/1.88G [00:12<00:08, 89.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  58%|█████▊    | 1.09G/1.88G [00:12<00:08, 90.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  59%|█████▊    | 1.10G/1.88G [00:12<00:08, 91.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  59%|█████▉    | 1.11G/1.88G [00:12<00:08, 92.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  60%|█████▉    | 1.12G/1.88G [00:12<00:08, 93.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  60%|██████    | 1.13G/1.88G [00:12<00:08, 92.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  61%|██████    | 1.14G/1.88G [00:12<00:07, 94.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  61%|██████▏   | 1.15G/1.88G [00:12<00:07, 94.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  62%|██████▏   | 1.16G/1.88G [00:12<00:07, 93.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  62%|██████▏   | 1.17G/1.88G [00:13<00:07, 92.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  63%|██████▎   | 1.18G/1.88G [00:13<00:07, 93.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  64%|██████▎   | 1.20G/1.88G [00:13<00:07, 93.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  64%|██████▍   | 1.21G/1.88G [00:13<00:07, 94.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  65%|██████▍   | 1.22G/1.88G [00:13<00:07, 93.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  65%|██████▌   | 1.23G/1.88G [00:13<00:06, 93.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  66%|██████▌   | 1.24G/1.88G [00:13<00:06, 91.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  66%|██████▋   | 1.25G/1.88G [00:13<00:06, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  67%|██████▋   | 1.26G/1.88G [00:13<00:06, 91.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  67%|██████▋   | 1.27G/1.88G [00:14<00:06, 91.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  68%|██████▊   | 1.28G/1.88G [00:14<00:06, 91.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  69%|██████▊   | 1.29G/1.88G [00:14<00:06, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  69%|██████▉   | 1.30G/1.88G [00:14<00:06, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  70%|██████▉   | 1.31G/1.88G [00:14<00:06, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  70%|███████   | 1.32G/1.88G [00:14<00:06, 91.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  71%|███████   | 1.33G/1.88G [00:14<00:05, 92.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  71%|███████▏  | 1.34G/1.88G [00:14<00:05, 93.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  72%|███████▏  | 1.35G/1.88G [00:15<00:05, 91.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  73%|███████▎  | 1.36G/1.88G [00:15<00:05, 92.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  73%|███████▎  | 1.37G/1.88G [00:15<00:05, 93.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  74%|███████▎  | 1.38G/1.88G [00:15<00:05, 93.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  74%|███████▍  | 1.39G/1.88G [00:15<00:05, 87.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  75%|███████▍  | 1.41G/1.88G [00:15<00:05, 89.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  75%|███████▌  | 1.42G/1.88G [00:15<00:05, 90.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  76%|███████▌  | 1.43G/1.88G [00:15<00:05, 90.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  76%|███████▋  | 1.44G/1.88G [00:15<00:04, 89.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  77%|███████▋  | 1.45G/1.88G [00:16<00:04, 90.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  78%|███████▊  | 1.46G/1.88G [00:16<00:04, 86.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  78%|███████▊  | 1.47G/1.88G [00:16<00:04, 88.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  79%|███████▊  | 1.48G/1.88G [00:16<00:04, 89.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  79%|███████▉  | 1.49G/1.88G [00:16<00:04, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  80%|███████▉  | 1.50G/1.88G [00:16<00:04, 92.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  80%|████████  | 1.51G/1.88G [00:16<00:04, 92.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  81%|████████  | 1.52G/1.88G [00:16<00:03, 91.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  81%|████████▏ | 1.53G/1.88G [00:16<00:03, 92.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  82%|████████▏ | 1.54G/1.88G [00:17<00:03, 93.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  83%|████████▎ | 1.55G/1.88G [00:17<00:03, 92.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  83%|████████▎ | 1.56G/1.88G [00:17<00:03, 91.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  84%|████████▎ | 1.57G/1.88G [00:17<00:03, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  84%|████████▍ | 1.58G/1.88G [00:17<00:03, 91.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  85%|████████▍ | 1.59G/1.88G [00:17<00:03, 90.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  85%|████████▌ | 1.60G/1.88G [00:17<00:03, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  86%|████████▌ | 1.61G/1.88G [00:17<00:02, 92.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  86%|████████▋ | 1.63G/1.88G [00:17<00:02, 93.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  87%|████████▋ | 1.64G/1.88G [00:18<00:02, 93.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  88%|████████▊ | 1.65G/1.88G [00:18<00:02, 93.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  88%|████████▊ | 1.66G/1.88G [00:18<00:02, 92.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  89%|████████▊ | 1.67G/1.88G [00:18<00:02, 92.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  89%|████████▉ | 1.68G/1.88G [00:18<00:02, 92.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  90%|████████▉ | 1.69G/1.88G [00:18<00:02, 92.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  90%|█████████ | 1.70G/1.88G [00:18<00:02, 87.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  91%|█████████ | 1.71G/1.88G [00:18<00:01, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  91%|█████████▏| 1.72G/1.88G [00:19<00:01, 88.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  92%|█████████▏| 1.73G/1.88G [00:19<00:01, 87.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  93%|█████████▎| 1.74G/1.88G [00:19<00:01, 87.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  93%|█████████▎| 1.75G/1.88G [00:19<00:01, 87.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  94%|█████████▎| 1.76G/1.88G [00:19<00:01, 89.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  94%|█████████▍| 1.77G/1.88G [00:19<00:01, 90.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  95%|█████████▍| 1.78G/1.88G [00:19<00:01, 87.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  95%|█████████▌| 1.79G/1.88G [00:19<00:00, 88.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  96%|█████████▌| 1.80G/1.88G [00:19<00:00, 89.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  97%|█████████▋| 1.81G/1.88G [00:20<00:00, 65.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  98%|█████████▊| 1.84G/1.88G [00:20<00:00, 90.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  98%|█████████▊| 1.85G/1.88G [00:20<00:00, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  99%|█████████▊| 1.86G/1.88G [00:20<00:00, 89.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  99%|█████████▉| 1.87G/1.88G [00:20<00:00, 90.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin: 100%|█████████▉| 1.88G/1.88G [00:20<00:00, 88.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin: 100%|██████████| 1.88G/1.88G [00:20<00:00, 90.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 2/8 [00:41<02:04, 20.71s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   1%|          | 10.5M/1.98G [00:00<00:24, 81.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   1%|          | 21.0M/1.98G [00:00<00:24, 81.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   2%|▏         | 31.5M/1.98G [00:00<00:21, 89.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   2%|▏         | 41.9M/1.98G [00:00<00:21, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   3%|▎         | 52.4M/1.98G [00:00<00:22, 87.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   3%|▎         | 62.9M/1.98G [00:00<00:21, 87.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   4%|▎         | 73.4M/1.98G [00:00<00:21, 88.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   4%|▍         | 83.9M/1.98G [00:00<00:22, 84.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   5%|▍         | 94.4M/1.98G [00:01<00:22, 83.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   5%|▌         | 105M/1.98G [00:01<00:22, 85.1MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   6%|▌         | 115M/1.98G [00:01<00:22, 82.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   6%|▋         | 126M/1.98G [00:01<00:21, 84.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   7%|▋         | 136M/1.98G [00:01<00:21, 87.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   7%|▋         | 147M/1.98G [00:01<00:20, 88.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   8%|▊         | 157M/1.98G [00:01<00:20, 90.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   8%|▊         | 168M/1.98G [00:01<00:19, 91.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   9%|▉         | 178M/1.98G [00:02<00:19, 91.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  10%|▉         | 189M/1.98G [00:02<00:20, 89.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  10%|█         | 199M/1.98G [00:02<00:19, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  11%|█         | 210M/1.98G [00:02<00:19, 88.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  11%|█         | 220M/1.98G [00:02<00:19, 88.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  12%|█▏        | 231M/1.98G [00:02<00:19, 90.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  12%|█▏        | 241M/1.98G [00:02<00:19, 89.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  13%|█▎        | 252M/1.98G [00:02<00:19, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  13%|█▎        | 262M/1.98G [00:02<00:18, 91.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  14%|█▍        | 273M/1.98G [00:03<00:18, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  14%|█▍        | 283M/1.98G [00:03<00:18, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  15%|█▍        | 294M/1.98G [00:03<00:19, 87.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  15%|█▌        | 304M/1.98G [00:03<00:19, 87.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  16%|█▌        | 315M/1.98G [00:03<00:19, 85.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  16%|█▋        | 325M/1.98G [00:03<00:19, 86.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  17%|█▋        | 336M/1.98G [00:03<00:19, 85.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  17%|█▋        | 346M/1.98G [00:03<00:19, 85.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  18%|█▊        | 357M/1.98G [00:04<00:18, 87.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  19%|█▊        | 367M/1.98G [00:04<00:18, 86.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  19%|█▉        | 377M/1.98G [00:04<00:18, 85.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  20%|█▉        | 388M/1.98G [00:04<00:18, 87.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  20%|██        | 398M/1.98G [00:04<00:17, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  21%|██        | 409M/1.98G [00:04<00:17, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  21%|██        | 419M/1.98G [00:04<00:17, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  22%|██▏       | 430M/1.98G [00:04<00:17, 91.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  22%|██▏       | 440M/1.98G [00:05<00:17, 88.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  23%|██▎       | 451M/1.98G [00:05<00:17, 88.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  23%|██▎       | 461M/1.98G [00:05<00:17, 87.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  24%|██▍       | 472M/1.98G [00:05<00:17, 86.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  24%|██▍       | 482M/1.98G [00:05<00:17, 87.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  25%|██▍       | 493M/1.98G [00:05<00:16, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  25%|██▌       | 503M/1.98G [00:05<00:16, 88.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  26%|██▌       | 514M/1.98G [00:05<00:16, 89.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  26%|██▋       | 524M/1.98G [00:05<00:16, 87.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  27%|██▋       | 535M/1.98G [00:06<00:16, 89.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  28%|██▊       | 545M/1.98G [00:06<00:16, 87.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  28%|██▊       | 556M/1.98G [00:06<00:15, 89.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  29%|██▊       | 566M/1.98G [00:06<00:15, 88.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  29%|██▉       | 577M/1.98G [00:06<00:15, 89.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  30%|██▉       | 587M/1.98G [00:06<00:15, 87.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  30%|███       | 598M/1.98G [00:06<00:15, 88.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  31%|███       | 608M/1.98G [00:06<00:15, 89.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  31%|███       | 619M/1.98G [00:07<00:15, 89.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  32%|███▏      | 629M/1.98G [00:07<00:15, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  32%|███▏      | 640M/1.98G [00:07<00:14, 89.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  33%|███▎      | 650M/1.98G [00:07<00:16, 81.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  33%|███▎      | 661M/1.98G [00:07<00:15, 83.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  34%|███▍      | 671M/1.98G [00:07<00:15, 83.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  34%|███▍      | 682M/1.98G [00:07<00:15, 82.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  35%|███▍      | 692M/1.98G [00:07<00:15, 84.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  35%|███▌      | 703M/1.98G [00:08<00:15, 81.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  36%|███▌      | 713M/1.98G [00:08<00:15, 83.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  37%|███▋      | 724M/1.98G [00:08<00:14, 85.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  37%|███▋      | 734M/1.98G [00:08<00:14, 87.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  38%|███▊      | 744M/1.98G [00:08<00:14, 87.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  38%|███▊      | 755M/1.98G [00:08<00:14, 86.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  39%|███▊      | 765M/1.98G [00:08<00:18, 67.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  39%|███▉      | 776M/1.98G [00:08<00:16, 73.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  40%|███▉      | 786M/1.98G [00:09<00:15, 74.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  40%|████      | 797M/1.98G [00:09<00:14, 79.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  41%|████      | 807M/1.98G [00:09<00:14, 83.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  41%|████▏     | 818M/1.98G [00:09<00:13, 85.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  42%|████▏     | 828M/1.98G [00:09<00:13, 87.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  42%|████▏     | 839M/1.98G [00:09<00:13, 83.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  43%|████▎     | 849M/1.98G [00:09<00:13, 85.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  43%|████▎     | 860M/1.98G [00:09<00:12, 87.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  44%|████▍     | 870M/1.98G [00:10<00:12, 88.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  44%|████▍     | 881M/1.98G [00:10<00:12, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  45%|████▌     | 891M/1.98G [00:10<00:12, 85.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  46%|████▌     | 902M/1.98G [00:10<00:12, 86.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  46%|████▌     | 912M/1.98G [00:10<00:11, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  47%|████▋     | 923M/1.98G [00:10<00:12, 86.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  47%|████▋     | 933M/1.98G [00:10<00:11, 88.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  48%|████▊     | 944M/1.98G [00:10<00:11, 89.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  48%|████▊     | 954M/1.98G [00:10<00:11, 90.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  49%|████▊     | 965M/1.98G [00:11<00:10, 92.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  49%|████▉     | 975M/1.98G [00:11<00:10, 92.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  50%|████▉     | 986M/1.98G [00:11<00:10, 92.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  50%|█████     | 996M/1.98G [00:11<00:10, 93.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  51%|█████     | 1.01G/1.98G [00:11<00:10, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  51%|█████▏    | 1.02G/1.98G [00:11<00:10, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  52%|█████▏    | 1.03G/1.98G [00:11<00:11, 85.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  52%|█████▏    | 1.04G/1.98G [00:11<00:10, 86.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  53%|█████▎    | 1.05G/1.98G [00:12<00:10, 88.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  53%|█████▎    | 1.06G/1.98G [00:12<00:10, 89.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  54%|█████▍    | 1.07G/1.98G [00:12<00:10, 90.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  55%|█████▍    | 1.08G/1.98G [00:12<00:09, 90.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  55%|█████▌    | 1.09G/1.98G [00:12<00:09, 91.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  56%|█████▌    | 1.10G/1.98G [00:12<00:09, 91.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  56%|█████▌    | 1.11G/1.98G [00:12<00:09, 91.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  57%|█████▋    | 1.12G/1.98G [00:12<00:09, 91.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  57%|█████▋    | 1.13G/1.98G [00:12<00:09, 89.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  58%|█████▊    | 1.14G/1.98G [00:13<00:09, 89.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  58%|█████▊    | 1.15G/1.98G [00:13<00:09, 90.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  59%|█████▉    | 1.16G/1.98G [00:13<00:09, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  59%|█████▉    | 1.17G/1.98G [00:13<00:08, 90.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  60%|█████▉    | 1.18G/1.98G [00:13<00:08, 90.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  60%|██████    | 1.20G/1.98G [00:13<00:08, 90.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  61%|██████    | 1.21G/1.98G [00:13<00:08, 91.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  61%|██████▏   | 1.22G/1.98G [00:13<00:08, 91.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  62%|██████▏   | 1.23G/1.98G [00:14<00:08, 85.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  62%|██████▏   | 1.24G/1.98G [00:14<00:08, 87.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  63%|██████▎   | 1.25G/1.98G [00:14<00:08, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  64%|██████▎   | 1.26G/1.98G [00:14<00:08, 87.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  64%|██████▍   | 1.27G/1.98G [00:14<00:07, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  65%|██████▍   | 1.28G/1.98G [00:14<00:07, 91.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  65%|██████▌   | 1.29G/1.98G [00:14<00:07, 89.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  66%|██████▌   | 1.30G/1.98G [00:14<00:07, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  66%|██████▌   | 1.31G/1.98G [00:14<00:07, 88.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  67%|██████▋   | 1.32G/1.98G [00:15<00:07, 88.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  67%|██████▋   | 1.33G/1.98G [00:15<00:07, 89.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  68%|██████▊   | 1.34G/1.98G [00:15<00:07, 90.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  68%|██████▊   | 1.35G/1.98G [00:15<00:06, 90.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  69%|██████▉   | 1.36G/1.98G [00:15<00:06, 90.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  69%|██████▉   | 1.37G/1.98G [00:15<00:06, 90.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  70%|██████▉   | 1.38G/1.98G [00:15<00:06, 87.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  70%|███████   | 1.39G/1.98G [00:15<00:06, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  71%|███████   | 1.41G/1.98G [00:16<00:06, 90.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  71%|███████▏  | 1.42G/1.98G [00:16<00:06, 90.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  72%|███████▏  | 1.43G/1.98G [00:16<00:06, 90.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  73%|███████▎  | 1.44G/1.98G [00:16<00:06, 89.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  73%|███████▎  | 1.45G/1.98G [00:16<00:05, 90.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  74%|███████▎  | 1.46G/1.98G [00:16<00:05, 90.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  74%|███████▍  | 1.47G/1.98G [00:16<00:05, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  75%|███████▍  | 1.48G/1.98G [00:16<00:05, 87.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  75%|███████▌  | 1.49G/1.98G [00:16<00:05, 88.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  76%|███████▌  | 1.50G/1.98G [00:17<00:05, 90.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  76%|███████▌  | 1.51G/1.98G [00:17<00:05, 90.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  77%|███████▋  | 1.52G/1.98G [00:17<00:05, 90.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  77%|███████▋  | 1.53G/1.98G [00:17<00:04, 91.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  78%|███████▊  | 1.54G/1.98G [00:17<00:04, 91.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  78%|███████▊  | 1.55G/1.98G [00:17<00:04, 90.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  79%|███████▉  | 1.56G/1.98G [00:17<00:04, 88.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  79%|███████▉  | 1.57G/1.98G [00:17<00:04, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  80%|███████▉  | 1.58G/1.98G [00:17<00:04, 90.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  80%|████████  | 1.59G/1.98G [00:18<00:04, 84.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  81%|████████  | 1.60G/1.98G [00:18<00:04, 87.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  82%|████████▏ | 1.61G/1.98G [00:18<00:04, 87.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  82%|████████▏ | 1.63G/1.98G [00:18<00:04, 88.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  83%|████████▎ | 1.64G/1.98G [00:18<00:03, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  83%|████████▎ | 1.65G/1.98G [00:18<00:03, 91.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  84%|████████▎ | 1.66G/1.98G [00:18<00:03, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  84%|████████▍ | 1.67G/1.98G [00:18<00:03, 91.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  85%|████████▍ | 1.68G/1.98G [00:19<00:03, 91.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  85%|████████▌ | 1.69G/1.98G [00:19<00:03, 91.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  86%|████████▌ | 1.70G/1.98G [00:19<00:03, 92.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  86%|████████▋ | 1.71G/1.98G [00:19<00:02, 92.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  87%|████████▋ | 1.72G/1.98G [00:19<00:02, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  87%|████████▋ | 1.73G/1.98G [00:19<00:02, 91.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  88%|████████▊ | 1.74G/1.98G [00:19<00:02, 88.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  88%|████████▊ | 1.75G/1.98G [00:19<00:02, 87.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  89%|████████▉ | 1.76G/1.98G [00:19<00:02, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  89%|████████▉ | 1.77G/1.98G [00:20<00:02, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  90%|█████████ | 1.78G/1.98G [00:20<00:02, 90.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  91%|█████████ | 1.79G/1.98G [00:20<00:02, 91.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  91%|█████████ | 1.80G/1.98G [00:20<00:01, 92.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  92%|█████████▏| 1.81G/1.98G [00:20<00:01, 93.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  92%|█████████▏| 1.82G/1.98G [00:20<00:01, 94.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  93%|█████████▎| 1.84G/1.98G [00:20<00:01, 94.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  93%|█████████▎| 1.85G/1.98G [00:20<00:01, 91.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  94%|█████████▎| 1.86G/1.98G [00:21<00:01, 67.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  95%|█████████▍| 1.88G/1.98G [00:21<00:01, 82.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  95%|█████████▌| 1.89G/1.98G [00:21<00:01, 81.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  96%|█████████▌| 1.90G/1.98G [00:21<00:00, 84.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  96%|█████████▋| 1.91G/1.98G [00:21<00:00, 87.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  97%|█████████▋| 1.92G/1.98G [00:21<00:00, 88.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  97%|█████████▋| 1.93G/1.98G [00:21<00:00, 86.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  98%|█████████▊| 1.94G/1.98G [00:22<00:00, 88.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  98%|█████████▊| 1.95G/1.98G [00:22<00:00, 90.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  99%|█████████▉| 1.96G/1.98G [00:22<00:00, 91.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin: 100%|█████████▉| 1.97G/1.98G [00:22<00:00, 89.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin: 100%|██████████| 1.98G/1.98G [00:22<00:00, 88.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin: 100%|██████████| 1.98G/1.98G [00:22<00:00, 88.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  38%|███▊      | 3/8 [01:03<01:47, 21.57s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   0%|          | 0.00/1.91G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   1%|          | 10.5M/1.91G [00:00<00:23, 80.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   1%|          | 21.0M/1.91G [00:00<00:21, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   2%|▏         | 31.5M/1.91G [00:00<00:21, 88.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   2%|▏         | 41.9M/1.91G [00:00<00:20, 89.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   3%|▎         | 52.4M/1.91G [00:00<00:20, 89.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   3%|▎         | 62.9M/1.91G [00:00<00:20, 91.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   4%|▍         | 73.4M/1.91G [00:00<00:22, 81.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   4%|▍         | 83.9M/1.91G [00:00<00:22, 80.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   5%|▍         | 94.4M/1.91G [00:01<00:22, 82.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   5%|▌         | 105M/1.91G [00:01<00:21, 84.3MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   6%|▌         | 115M/1.91G [00:01<00:21, 84.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   7%|▋         | 126M/1.91G [00:01<00:21, 83.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   7%|▋         | 136M/1.91G [00:01<00:20, 86.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   8%|▊         | 147M/1.91G [00:01<00:19, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   8%|▊         | 157M/1.91G [00:01<00:19, 90.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   9%|▉         | 168M/1.91G [00:01<00:20, 83.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   9%|▉         | 178M/1.91G [00:02<00:20, 85.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  10%|▉         | 189M/1.91G [00:02<00:19, 88.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  10%|█         | 199M/1.91G [00:02<00:19, 87.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  11%|█         | 210M/1.91G [00:02<00:20, 83.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  12%|█▏        | 220M/1.91G [00:02<00:19, 86.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  12%|█▏        | 231M/1.91G [00:02<00:18, 89.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  13%|█▎        | 241M/1.91G [00:02<00:18, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  13%|█▎        | 252M/1.91G [00:02<00:17, 92.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  14%|█▎        | 262M/1.91G [00:03<00:17, 92.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  14%|█▍        | 273M/1.91G [00:03<00:17, 93.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  15%|█▍        | 283M/1.91G [00:03<00:17, 94.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  15%|█▌        | 294M/1.91G [00:03<00:17, 90.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  16%|█▌        | 304M/1.91G [00:03<00:17, 89.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  16%|█▋        | 315M/1.91G [00:03<00:17, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  17%|█▋        | 325M/1.91G [00:03<00:17, 93.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  18%|█▊        | 336M/1.91G [00:03<00:16, 94.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  18%|█▊        | 346M/1.91G [00:03<00:16, 93.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  19%|█▊        | 357M/1.91G [00:04<00:16, 93.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  19%|█▉        | 367M/1.91G [00:04<00:16, 94.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  20%|█▉        | 377M/1.91G [00:04<00:17, 89.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  20%|██        | 388M/1.91G [00:04<00:16, 91.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  21%|██        | 398M/1.91G [00:04<00:16, 91.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  21%|██▏       | 409M/1.91G [00:04<00:16, 92.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  22%|██▏       | 419M/1.91G [00:04<00:16, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  22%|██▏       | 430M/1.91G [00:04<00:16, 90.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  23%|██▎       | 440M/1.91G [00:04<00:16, 86.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  24%|██▎       | 451M/1.91G [00:05<00:16, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  24%|██▍       | 461M/1.91G [00:05<00:16, 89.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  25%|██▍       | 472M/1.91G [00:05<00:16, 89.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  25%|██▌       | 482M/1.91G [00:05<00:16, 87.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  26%|██▌       | 493M/1.91G [00:05<00:16, 87.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  26%|██▋       | 503M/1.91G [00:05<00:15, 89.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  27%|██▋       | 514M/1.91G [00:05<00:15, 88.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  27%|██▋       | 524M/1.91G [00:05<00:15, 89.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  28%|██▊       | 535M/1.91G [00:06<00:15, 91.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  28%|██▊       | 545M/1.91G [00:06<00:14, 91.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  29%|██▉       | 556M/1.91G [00:06<00:14, 93.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  30%|██▉       | 566M/1.91G [00:06<00:15, 85.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  30%|███       | 577M/1.91G [00:06<00:15, 87.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  31%|███       | 587M/1.91G [00:06<00:15, 87.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  31%|███       | 598M/1.91G [00:06<00:14, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  32%|███▏      | 608M/1.91G [00:06<00:14, 91.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  32%|███▏      | 619M/1.91G [00:06<00:14, 91.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  33%|███▎      | 629M/1.91G [00:07<00:14, 89.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  33%|███▎      | 640M/1.91G [00:07<00:15, 83.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  34%|███▍      | 650M/1.91G [00:07<00:14, 85.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  35%|███▍      | 661M/1.91G [00:07<00:14, 86.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  35%|███▌      | 671M/1.91G [00:07<00:15, 78.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  36%|███▌      | 682M/1.91G [00:07<00:16, 75.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  36%|███▌      | 692M/1.91G [00:07<00:15, 79.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  37%|███▋      | 703M/1.91G [00:08<00:15, 78.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  37%|███▋      | 713M/1.91G [00:08<00:15, 77.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  38%|███▊      | 724M/1.91G [00:08<00:14, 81.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  38%|███▊      | 734M/1.91G [00:08<00:14, 83.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  39%|███▉      | 744M/1.91G [00:08<00:13, 86.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  39%|███▉      | 755M/1.91G [00:08<00:13, 86.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  40%|████      | 765M/1.91G [00:08<00:13, 87.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  41%|████      | 776M/1.91G [00:08<00:13, 86.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  41%|████      | 786M/1.91G [00:08<00:13, 86.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  42%|████▏     | 797M/1.91G [00:09<00:12, 87.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  42%|████▏     | 807M/1.91G [00:09<00:12, 86.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  43%|████▎     | 818M/1.91G [00:09<00:12, 88.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  43%|████▎     | 828M/1.91G [00:09<00:12, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  44%|████▍     | 839M/1.91G [00:09<00:11, 90.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  44%|████▍     | 849M/1.91G [00:09<00:11, 88.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  45%|████▍     | 860M/1.91G [00:09<00:11, 90.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  45%|████▌     | 870M/1.91G [00:09<00:11, 88.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  46%|████▌     | 881M/1.91G [00:10<00:11, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  47%|████▋     | 891M/1.91G [00:10<00:11, 90.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  47%|████▋     | 902M/1.91G [00:10<00:11, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  48%|████▊     | 912M/1.91G [00:10<00:10, 91.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  48%|████▊     | 923M/1.91G [00:10<00:10, 91.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  49%|████▉     | 933M/1.91G [00:10<00:10, 92.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  49%|████▉     | 944M/1.91G [00:10<00:10, 92.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  50%|████▉     | 954M/1.91G [00:10<00:10, 87.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  50%|█████     | 965M/1.91G [00:10<00:10, 87.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  51%|█████     | 975M/1.91G [00:11<00:10, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  52%|█████▏    | 986M/1.91G [00:11<00:10, 89.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  52%|█████▏    | 996M/1.91G [00:11<00:10, 91.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  53%|█████▎    | 1.01G/1.91G [00:11<00:09, 91.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  53%|█████▎    | 1.02G/1.91G [00:11<00:09, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  54%|█████▎    | 1.03G/1.91G [00:11<00:09, 91.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  54%|█████▍    | 1.04G/1.91G [00:11<00:09, 89.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  55%|█████▍    | 1.05G/1.91G [00:11<00:09, 90.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  55%|█████▌    | 1.06G/1.91G [00:11<00:09, 90.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  56%|█████▌    | 1.07G/1.91G [00:12<00:09, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  56%|█████▋    | 1.08G/1.91G [00:12<00:09, 92.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  57%|█████▋    | 1.09G/1.91G [00:12<00:09, 90.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  58%|█████▊    | 1.10G/1.91G [00:12<00:09, 88.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  58%|█████▊    | 1.11G/1.91G [00:12<00:09, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  59%|█████▊    | 1.12G/1.91G [00:12<00:08, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  59%|█████▉    | 1.13G/1.91G [00:12<00:08, 88.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  60%|█████▉    | 1.14G/1.91G [00:12<00:08, 90.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  60%|██████    | 1.15G/1.91G [00:13<00:08, 87.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  61%|██████    | 1.16G/1.91G [00:13<00:08, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  61%|██████▏   | 1.17G/1.91G [00:13<00:08, 88.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  62%|██████▏   | 1.18G/1.91G [00:13<00:08, 90.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  62%|██████▏   | 1.20G/1.91G [00:13<00:07, 90.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  63%|██████▎   | 1.21G/1.91G [00:13<00:07, 89.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  64%|██████▎   | 1.22G/1.91G [00:13<00:08, 87.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  64%|██████▍   | 1.23G/1.91G [00:13<00:07, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  65%|██████▍   | 1.24G/1.91G [00:13<00:07, 91.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  65%|██████▌   | 1.25G/1.91G [00:14<00:07, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  66%|██████▌   | 1.26G/1.91G [00:14<00:07, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  66%|██████▋   | 1.27G/1.91G [00:14<00:07, 89.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  67%|██████▋   | 1.28G/1.91G [00:14<00:07, 86.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  67%|██████▋   | 1.29G/1.91G [00:14<00:06, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  68%|██████▊   | 1.30G/1.91G [00:14<00:06, 88.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  69%|██████▊   | 1.31G/1.91G [00:14<00:07, 79.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  69%|██████▉   | 1.32G/1.91G [00:14<00:07, 80.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  70%|██████▉   | 1.33G/1.91G [00:15<00:06, 84.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  70%|███████   | 1.34G/1.91G [00:15<00:06, 86.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  71%|███████   | 1.35G/1.91G [00:15<00:06, 87.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  71%|███████   | 1.36G/1.91G [00:15<00:06, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  72%|███████▏  | 1.37G/1.91G [00:15<00:06, 89.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  72%|███████▏  | 1.38G/1.91G [00:15<00:05, 90.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  73%|███████▎  | 1.39G/1.91G [00:15<00:05, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  73%|███████▎  | 1.41G/1.91G [00:15<00:05, 91.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  74%|███████▍  | 1.42G/1.91G [00:16<00:05, 90.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  75%|███████▍  | 1.43G/1.91G [00:16<00:07, 63.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  76%|███████▌  | 1.45G/1.91G [00:16<00:05, 90.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  77%|███████▋  | 1.47G/1.91G [00:16<00:04, 90.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  77%|███████▋  | 1.48G/1.91G [00:16<00:04, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  78%|███████▊  | 1.49G/1.91G [00:16<00:04, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  78%|███████▊  | 1.50G/1.91G [00:16<00:04, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  79%|███████▉  | 1.51G/1.91G [00:17<00:04, 91.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  79%|███████▉  | 1.52G/1.91G [00:17<00:04, 90.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  80%|████████  | 1.53G/1.91G [00:17<00:04, 90.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  81%|████████  | 1.54G/1.91G [00:17<00:04, 92.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  81%|████████  | 1.55G/1.91G [00:17<00:03, 92.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  82%|████████▏ | 1.56G/1.91G [00:17<00:03, 91.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  82%|████████▏ | 1.57G/1.91G [00:17<00:03, 92.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  83%|████████▎ | 1.58G/1.91G [00:17<00:03, 90.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  83%|████████▎ | 1.59G/1.91G [00:18<00:03, 90.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  84%|████████▍ | 1.60G/1.91G [00:18<00:03, 90.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  84%|████████▍ | 1.61G/1.91G [00:18<00:03, 82.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  85%|████████▍ | 1.63G/1.91G [00:18<00:03, 80.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  85%|████████▌ | 1.64G/1.91G [00:18<00:03, 82.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  86%|████████▌ | 1.65G/1.91G [00:18<00:03, 85.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  87%|████████▋ | 1.66G/1.91G [00:18<00:02, 88.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  87%|████████▋ | 1.67G/1.91G [00:18<00:02, 88.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  88%|████████▊ | 1.68G/1.91G [00:19<00:02, 86.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  88%|████████▊ | 1.69G/1.91G [00:19<00:02, 88.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  89%|████████▉ | 1.70G/1.91G [00:19<00:02, 90.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  89%|████████▉ | 1.71G/1.91G [00:19<00:02, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  90%|████████▉ | 1.72G/1.91G [00:19<00:02, 92.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  90%|█████████ | 1.73G/1.91G [00:19<00:01, 92.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  91%|█████████ | 1.74G/1.91G [00:19<00:01, 92.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  92%|█████████▏| 1.75G/1.91G [00:19<00:01, 87.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  92%|█████████▏| 1.76G/1.91G [00:19<00:01, 85.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  93%|█████████▎| 1.77G/1.91G [00:20<00:01, 85.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  93%|█████████▎| 1.78G/1.91G [00:20<00:01, 83.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  94%|█████████▎| 1.79G/1.91G [00:20<00:01, 87.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  94%|█████████▍| 1.80G/1.91G [00:20<00:01, 89.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  95%|█████████▍| 1.81G/1.91G [00:20<00:01, 88.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  95%|█████████▌| 1.82G/1.91G [00:20<00:00, 89.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  96%|█████████▌| 1.84G/1.91G [00:20<00:00, 89.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  96%|█████████▋| 1.85G/1.91G [00:20<00:00, 88.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  97%|█████████▋| 1.86G/1.91G [00:21<00:00, 81.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  98%|█████████▊| 1.87G/1.91G [00:21<00:00, 85.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  98%|█████████▊| 1.88G/1.91G [00:21<00:00, 86.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  99%|█████████▊| 1.89G/1.91G [00:21<00:00, 88.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  99%|█████████▉| 1.90G/1.91G [00:21<00:00, 89.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin: 100%|█████████▉| 1.91G/1.91G [00:21<00:00, 91.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin: 100%|██████████| 1.91G/1.91G [00:21<00:00, 88.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 4/8 [01:25<01:26, 21.65s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   1%|          | 10.5M/1.88G [00:00<00:25, 73.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   1%|          | 21.0M/1.88G [00:00<00:23, 79.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   2%|▏         | 31.5M/1.88G [00:00<00:21, 86.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   2%|▏         | 41.9M/1.88G [00:00<00:21, 84.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   3%|▎         | 52.4M/1.88G [00:00<00:20, 87.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   3%|▎         | 62.9M/1.88G [00:00<00:20, 87.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   4%|▍         | 73.4M/1.88G [00:00<00:20, 88.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   4%|▍         | 83.9M/1.88G [00:00<00:20, 86.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   5%|▌         | 94.4M/1.88G [00:01<00:20, 88.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   6%|▌         | 105M/1.88G [00:01<00:20, 86.1MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   6%|▌         | 115M/1.88G [00:01<00:19, 88.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   7%|▋         | 126M/1.88G [00:01<00:19, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   7%|▋         | 136M/1.88G [00:01<00:19, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   8%|▊         | 147M/1.88G [00:01<00:18, 91.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   8%|▊         | 157M/1.88G [00:01<00:18, 92.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   9%|▉         | 168M/1.88G [00:01<00:18, 92.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   9%|▉         | 178M/1.88G [00:02<00:19, 88.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  10%|█         | 189M/1.88G [00:02<00:18, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  11%|█         | 199M/1.88G [00:02<00:18, 90.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  11%|█         | 210M/1.88G [00:02<00:18, 91.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  12%|█▏        | 220M/1.88G [00:02<00:18, 90.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  12%|█▏        | 231M/1.88G [00:02<00:18, 90.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  13%|█▎        | 241M/1.88G [00:02<00:19, 86.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  13%|█▎        | 252M/1.88G [00:02<00:19, 85.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  14%|█▍        | 262M/1.88G [00:02<00:18, 87.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  15%|█▍        | 273M/1.88G [00:03<00:18, 85.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  15%|█▌        | 283M/1.88G [00:03<00:18, 86.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  16%|█▌        | 294M/1.88G [00:03<00:18, 87.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  16%|█▌        | 304M/1.88G [00:03<00:17, 87.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  17%|█▋        | 315M/1.88G [00:03<00:17, 89.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  17%|█▋        | 325M/1.88G [00:03<00:17, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  18%|█▊        | 336M/1.88G [00:03<00:16, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  18%|█▊        | 346M/1.88G [00:03<00:16, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  19%|█▉        | 357M/1.88G [00:04<00:16, 90.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  20%|█▉        | 367M/1.88G [00:04<00:17, 86.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  20%|██        | 377M/1.88G [00:04<00:16, 88.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  21%|██        | 388M/1.88G [00:04<00:16, 90.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  21%|██        | 398M/1.88G [00:04<00:16, 91.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  22%|██▏       | 409M/1.88G [00:04<00:23, 62.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  23%|██▎       | 430M/1.88G [00:04<00:16, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  23%|██▎       | 440M/1.88G [00:05<00:16, 89.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  24%|██▍       | 451M/1.88G [00:05<00:16, 87.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  25%|██▍       | 461M/1.88G [00:05<00:16, 86.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  25%|██▌       | 472M/1.88G [00:05<00:16, 84.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  26%|██▌       | 482M/1.88G [00:05<00:16, 87.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  26%|██▌       | 493M/1.88G [00:05<00:16, 84.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  27%|██▋       | 503M/1.88G [00:05<00:16, 84.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  27%|██▋       | 514M/1.88G [00:05<00:15, 87.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  28%|██▊       | 524M/1.88G [00:05<00:15, 88.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  28%|██▊       | 535M/1.88G [00:06<00:14, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  29%|██▉       | 545M/1.88G [00:06<00:15, 85.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  30%|██▉       | 556M/1.88G [00:06<00:16, 80.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  30%|███       | 566M/1.88G [00:06<00:15, 82.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  31%|███       | 577M/1.88G [00:06<00:15, 83.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  31%|███       | 587M/1.88G [00:06<00:15, 83.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  32%|███▏      | 598M/1.88G [00:06<00:15, 80.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  32%|███▏      | 608M/1.88G [00:07<00:15, 83.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  33%|███▎      | 619M/1.88G [00:07<00:14, 86.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  33%|███▎      | 629M/1.88G [00:07<00:14, 85.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  34%|███▍      | 640M/1.88G [00:07<00:14, 85.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  35%|███▍      | 650M/1.88G [00:07<00:14, 85.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  35%|███▌      | 661M/1.88G [00:07<00:14, 86.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  36%|███▌      | 671M/1.88G [00:07<00:13, 87.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  36%|███▋      | 682M/1.88G [00:07<00:13, 87.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  37%|███▋      | 692M/1.88G [00:07<00:13, 89.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  37%|███▋      | 703M/1.88G [00:08<00:13, 90.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  38%|███▊      | 713M/1.88G [00:08<00:12, 90.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  38%|███▊      | 724M/1.88G [00:08<00:12, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  39%|███▉      | 734M/1.88G [00:08<00:13, 87.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  40%|███▉      | 744M/1.88G [00:08<00:12, 88.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  40%|████      | 755M/1.88G [00:08<00:12, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  41%|████      | 765M/1.88G [00:08<00:12, 90.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  41%|████▏     | 776M/1.88G [00:08<00:12, 90.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  42%|████▏     | 786M/1.88G [00:09<00:12, 91.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  42%|████▏     | 797M/1.88G [00:09<00:11, 91.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  43%|████▎     | 807M/1.88G [00:09<00:11, 90.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  44%|████▎     | 818M/1.88G [00:09<00:12, 86.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  44%|████▍     | 828M/1.88G [00:09<00:12, 83.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  45%|████▍     | 839M/1.88G [00:09<00:12, 81.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  45%|████▌     | 849M/1.88G [00:09<00:12, 85.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  46%|████▌     | 860M/1.88G [00:09<00:12, 84.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  46%|████▋     | 870M/1.88G [00:10<00:11, 85.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  47%|████▋     | 881M/1.88G [00:10<00:11, 87.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  47%|████▋     | 891M/1.88G [00:10<00:11, 89.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  48%|████▊     | 902M/1.88G [00:10<00:10, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  49%|████▊     | 912M/1.88G [00:10<00:10, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  49%|████▉     | 923M/1.88G [00:10<00:11, 86.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  50%|████▉     | 933M/1.88G [00:10<00:10, 87.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  50%|█████     | 944M/1.88G [00:10<00:10, 86.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  51%|█████     | 954M/1.88G [00:10<00:10, 87.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  51%|█████▏    | 965M/1.88G [00:11<00:10, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  52%|█████▏    | 975M/1.88G [00:11<00:10, 90.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  52%|█████▏    | 986M/1.88G [00:11<00:09, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  53%|█████▎    | 996M/1.88G [00:11<00:09, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  54%|█████▎    | 1.01G/1.88G [00:11<00:09, 90.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  54%|█████▍    | 1.02G/1.88G [00:11<00:09, 87.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  55%|█████▍    | 1.03G/1.88G [00:11<00:09, 87.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  55%|█████▌    | 1.04G/1.88G [00:11<00:09, 87.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  56%|█████▌    | 1.05G/1.88G [00:12<00:09, 87.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  56%|█████▋    | 1.06G/1.88G [00:12<00:09, 88.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  57%|█████▋    | 1.07G/1.88G [00:12<00:09, 86.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  57%|█████▋    | 1.08G/1.88G [00:12<00:09, 85.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  58%|█████▊    | 1.09G/1.88G [00:12<00:10, 78.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  59%|█████▊    | 1.10G/1.88G [00:12<00:09, 81.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  59%|█████▉    | 1.11G/1.88G [00:12<00:09, 81.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  60%|█████▉    | 1.12G/1.88G [00:12<00:09, 83.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  60%|██████    | 1.13G/1.88G [00:13<00:08, 84.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  61%|██████    | 1.14G/1.88G [00:13<00:08, 86.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  61%|██████▏   | 1.15G/1.88G [00:13<00:08, 87.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  62%|██████▏   | 1.16G/1.88G [00:13<00:08, 86.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  62%|██████▏   | 1.17G/1.88G [00:13<00:08, 87.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  63%|██████▎   | 1.18G/1.88G [00:13<00:07, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  64%|██████▎   | 1.20G/1.88G [00:13<00:07, 91.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  64%|██████▍   | 1.21G/1.88G [00:13<00:07, 88.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  65%|██████▍   | 1.22G/1.88G [00:13<00:07, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  65%|██████▌   | 1.23G/1.88G [00:14<00:07, 91.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  66%|██████▌   | 1.24G/1.88G [00:14<00:06, 92.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  66%|██████▋   | 1.25G/1.88G [00:14<00:06, 93.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  67%|██████▋   | 1.26G/1.88G [00:14<00:06, 92.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  67%|██████▋   | 1.27G/1.88G [00:14<00:06, 93.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  68%|██████▊   | 1.28G/1.88G [00:14<00:06, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  69%|██████▊   | 1.29G/1.88G [00:14<00:06, 86.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  69%|██████▉   | 1.30G/1.88G [00:14<00:06, 86.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  70%|██████▉   | 1.31G/1.88G [00:15<00:07, 81.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  70%|███████   | 1.32G/1.88G [00:15<00:06, 84.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  71%|███████   | 1.33G/1.88G [00:15<00:06, 85.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  71%|███████▏  | 1.34G/1.88G [00:15<00:06, 86.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  72%|███████▏  | 1.35G/1.88G [00:15<00:05, 88.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  73%|███████▎  | 1.36G/1.88G [00:15<00:05, 89.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  73%|███████▎  | 1.37G/1.88G [00:15<00:05, 89.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  74%|███████▎  | 1.38G/1.88G [00:15<00:05, 91.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  74%|███████▍  | 1.39G/1.88G [00:15<00:05, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  75%|███████▍  | 1.41G/1.88G [00:16<00:05, 87.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  75%|███████▌  | 1.42G/1.88G [00:16<00:05, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  76%|███████▌  | 1.43G/1.88G [00:16<00:05, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  76%|███████▋  | 1.44G/1.88G [00:16<00:04, 88.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  77%|███████▋  | 1.45G/1.88G [00:16<00:04, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  78%|███████▊  | 1.46G/1.88G [00:16<00:04, 91.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  78%|███████▊  | 1.47G/1.88G [00:16<00:04, 92.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  79%|███████▊  | 1.48G/1.88G [00:16<00:04, 93.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  79%|███████▉  | 1.49G/1.88G [00:16<00:04, 93.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  80%|███████▉  | 1.50G/1.88G [00:17<00:04, 86.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  80%|████████  | 1.51G/1.88G [00:17<00:04, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  81%|████████  | 1.52G/1.88G [00:17<00:04, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  81%|████████▏ | 1.53G/1.88G [00:17<00:03, 89.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  82%|████████▏ | 1.54G/1.88G [00:17<00:03, 91.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  83%|████████▎ | 1.55G/1.88G [00:17<00:03, 92.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  83%|████████▎ | 1.56G/1.88G [00:17<00:03, 93.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  84%|████████▎ | 1.57G/1.88G [00:17<00:03, 93.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  84%|████████▍ | 1.58G/1.88G [00:18<00:03, 93.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  85%|████████▍ | 1.59G/1.88G [00:18<00:03, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  85%|████████▌ | 1.60G/1.88G [00:18<00:02, 92.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  86%|████████▌ | 1.61G/1.88G [00:18<00:02, 92.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  86%|████████▋ | 1.63G/1.88G [00:18<00:02, 93.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  87%|████████▋ | 1.64G/1.88G [00:18<00:02, 91.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  88%|████████▊ | 1.65G/1.88G [00:18<00:02, 91.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  88%|████████▊ | 1.66G/1.88G [00:18<00:02, 92.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  89%|████████▊ | 1.67G/1.88G [00:18<00:02, 91.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  89%|████████▉ | 1.68G/1.88G [00:19<00:02, 90.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  90%|████████▉ | 1.69G/1.88G [00:19<00:02, 92.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  90%|█████████ | 1.70G/1.88G [00:19<00:02, 89.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  91%|█████████ | 1.71G/1.88G [00:19<00:01, 91.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  91%|█████████▏| 1.72G/1.88G [00:19<00:01, 88.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  92%|█████████▏| 1.73G/1.88G [00:19<00:01, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  93%|█████████▎| 1.74G/1.88G [00:19<00:01, 87.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  93%|█████████▎| 1.75G/1.88G [00:19<00:01, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  94%|█████████▎| 1.76G/1.88G [00:20<00:01, 90.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  94%|█████████▍| 1.77G/1.88G [00:20<00:01, 83.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  95%|█████████▍| 1.78G/1.88G [00:20<00:01, 78.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  95%|█████████▌| 1.79G/1.88G [00:20<00:01, 82.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  96%|█████████▌| 1.80G/1.88G [00:20<00:00, 83.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  97%|█████████▋| 1.81G/1.88G [00:20<00:00, 85.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  97%|█████████▋| 1.82G/1.88G [00:20<00:00, 84.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  98%|█████████▊| 1.84G/1.88G [00:20<00:00, 86.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  98%|█████████▊| 1.85G/1.88G [00:21<00:00, 86.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  99%|█████████▊| 1.86G/1.88G [00:21<00:00, 83.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  99%|█████████▉| 1.87G/1.88G [00:21<00:00, 83.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin: 100%|█████████▉| 1.88G/1.88G [00:21<00:00, 86.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin: 100%|██████████| 1.88G/1.88G [00:21<00:00, 87.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  62%|██████▎   | 5/8 [01:47<01:04, 21.60s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   1%|          | 10.5M/1.88G [00:00<01:09, 26.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   1%|          | 21.0M/1.88G [00:00<01:08, 27.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   2%|▏         | 31.5M/1.88G [00:01<01:08, 27.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   2%|▏         | 41.9M/1.88G [00:01<01:07, 27.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   3%|▎         | 52.4M/1.88G [00:01<01:07, 26.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   3%|▎         | 62.9M/1.88G [00:02<01:07, 26.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   4%|▍         | 73.4M/1.88G [00:02<01:07, 26.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   4%|▍         | 83.9M/1.88G [00:03<01:06, 26.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   5%|▌         | 94.4M/1.88G [00:03<01:06, 27.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   6%|▌         | 105M/1.88G [00:03<01:05, 27.1MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   6%|▌         | 115M/1.88G [00:04<01:05, 26.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   7%|▋         | 126M/1.88G [00:04<01:06, 26.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   7%|▋         | 136M/1.88G [00:05<01:05, 26.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   8%|▊         | 147M/1.88G [00:05<01:04, 26.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   8%|▊         | 157M/1.88G [00:05<01:04, 26.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   9%|▉         | 168M/1.88G [00:06<01:04, 26.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   9%|▉         | 178M/1.88G [00:06<01:03, 26.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  10%|█         | 189M/1.88G [00:07<01:03, 26.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  11%|█         | 199M/1.88G [00:07<01:02, 26.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  11%|█         | 210M/1.88G [00:07<01:01, 27.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  12%|█▏        | 220M/1.88G [00:08<01:01, 27.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  12%|█▏        | 231M/1.88G [00:08<01:02, 26.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  13%|█▎        | 241M/1.88G [00:09<01:02, 26.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  13%|█▎        | 252M/1.88G [00:09<01:03, 25.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  14%|█▍        | 262M/1.88G [00:09<01:00, 26.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  15%|█▍        | 273M/1.88G [00:10<01:01, 26.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  15%|█▌        | 283M/1.88G [00:10<01:00, 26.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  16%|█▌        | 294M/1.88G [00:10<00:59, 26.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  16%|█▌        | 304M/1.88G [00:11<00:59, 26.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  17%|█▋        | 315M/1.88G [00:11<00:58, 26.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  17%|█▋        | 325M/1.88G [00:12<00:57, 27.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  18%|█▊        | 336M/1.88G [00:12<00:57, 26.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  18%|█▊        | 346M/1.88G [00:12<00:57, 26.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  19%|█▉        | 357M/1.88G [00:13<00:56, 26.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  20%|█▉        | 367M/1.88G [00:13<00:56, 26.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  20%|██        | 377M/1.88G [00:14<00:56, 26.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  21%|██        | 388M/1.88G [00:14<00:55, 27.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  21%|██        | 398M/1.88G [00:14<00:55, 26.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  22%|██▏       | 409M/1.88G [00:15<00:55, 26.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  22%|██▏       | 419M/1.88G [00:15<00:54, 26.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  23%|██▎       | 430M/1.88G [00:16<00:54, 26.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  23%|██▎       | 440M/1.88G [00:16<00:54, 26.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  24%|██▍       | 451M/1.88G [00:16<00:53, 26.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  25%|██▍       | 461M/1.88G [00:17<00:53, 26.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  25%|██▌       | 472M/1.88G [00:17<00:53, 26.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  26%|██▌       | 482M/1.88G [00:18<00:52, 26.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  26%|██▌       | 493M/1.88G [00:18<00:52, 26.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  27%|██▋       | 503M/1.88G [00:18<00:51, 26.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  27%|██▋       | 514M/1.88G [00:19<00:51, 26.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  28%|██▊       | 524M/1.88G [00:19<00:50, 26.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  28%|██▊       | 535M/1.88G [00:19<00:49, 27.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  29%|██▉       | 545M/1.88G [00:20<00:49, 27.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  30%|██▉       | 556M/1.88G [00:20<00:49, 26.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  30%|███       | 566M/1.88G [00:21<00:48, 27.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  31%|███       | 577M/1.88G [00:21<00:48, 26.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  31%|███       | 587M/1.88G [00:21<00:48, 26.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  32%|███▏      | 598M/1.88G [00:22<00:47, 26.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  32%|███▏      | 608M/1.88G [00:22<00:47, 26.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  33%|███▎      | 619M/1.88G [00:23<00:47, 26.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  33%|███▎      | 629M/1.88G [00:23<00:46, 26.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  34%|███▍      | 640M/1.88G [00:23<00:46, 26.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  35%|███▍      | 650M/1.88G [00:24<00:46, 26.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  35%|███▌      | 661M/1.88G [00:24<00:46, 26.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  36%|███▌      | 671M/1.88G [00:25<00:47, 25.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  36%|███▋      | 682M/1.88G [00:25<00:50, 23.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  37%|███▋      | 692M/1.88G [00:26<00:54, 21.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  37%|███▋      | 703M/1.88G [00:26<00:53, 22.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  38%|███▊      | 713M/1.88G [00:27<00:58, 20.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  38%|███▊      | 724M/1.88G [00:27<00:56, 20.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  39%|███▉      | 734M/1.88G [00:28<00:53, 21.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  40%|███▉      | 744M/1.88G [00:28<00:49, 22.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  40%|████      | 755M/1.88G [00:29<00:50, 22.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  41%|████      | 765M/1.88G [00:29<00:47, 23.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  41%|████▏     | 776M/1.88G [00:29<00:44, 24.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  42%|████▏     | 786M/1.88G [00:30<00:44, 24.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  42%|████▏     | 797M/1.88G [00:30<00:43, 25.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  43%|████▎     | 807M/1.88G [00:31<00:41, 25.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  44%|████▎     | 818M/1.88G [00:31<00:41, 25.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  44%|████▍     | 828M/1.88G [00:31<00:41, 25.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  45%|████▍     | 839M/1.88G [00:32<00:41, 25.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  45%|████▌     | 849M/1.88G [00:32<00:41, 25.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  46%|████▌     | 860M/1.88G [00:33<00:41, 24.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  46%|████▋     | 870M/1.88G [00:33<00:40, 25.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  47%|████▋     | 881M/1.88G [00:34<00:39, 25.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  47%|████▋     | 891M/1.88G [00:34<00:38, 25.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  48%|████▊     | 902M/1.88G [00:34<00:37, 25.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  49%|████▊     | 912M/1.88G [00:35<00:37, 26.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  49%|████▉     | 923M/1.88G [00:35<00:36, 26.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  50%|████▉     | 933M/1.88G [00:36<00:35, 26.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  50%|█████     | 944M/1.88G [00:36<00:35, 26.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  51%|█████     | 954M/1.88G [00:36<00:35, 26.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  51%|█████▏    | 965M/1.88G [00:37<00:34, 26.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  52%|█████▏    | 975M/1.88G [00:37<00:34, 26.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  52%|█████▏    | 986M/1.88G [00:38<00:34, 25.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  53%|█████▎    | 996M/1.88G [00:38<00:34, 26.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  54%|█████▎    | 1.01G/1.88G [00:38<00:34, 25.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  54%|█████▍    | 1.02G/1.88G [00:39<00:33, 26.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  55%|█████▍    | 1.03G/1.88G [00:39<00:32, 26.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  55%|█████▌    | 1.04G/1.88G [00:40<00:32, 26.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  56%|█████▌    | 1.05G/1.88G [00:40<00:31, 26.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  56%|█████▋    | 1.06G/1.88G [00:40<00:31, 26.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  57%|█████▋    | 1.07G/1.88G [00:41<00:31, 25.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  57%|█████▋    | 1.08G/1.88G [00:41<00:30, 26.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  58%|█████▊    | 1.09G/1.88G [00:42<00:29, 26.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  59%|█████▊    | 1.10G/1.88G [00:42<00:28, 26.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  59%|█████▉    | 1.11G/1.88G [00:42<00:28, 26.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  60%|█████▉    | 1.12G/1.88G [00:43<00:28, 27.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  60%|██████    | 1.13G/1.88G [00:43<00:27, 27.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  61%|██████    | 1.14G/1.88G [00:43<00:27, 26.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  61%|██████▏   | 1.15G/1.88G [00:44<00:27, 26.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  62%|██████▏   | 1.16G/1.88G [00:44<00:26, 26.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  62%|██████▏   | 1.17G/1.88G [00:45<00:26, 26.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  63%|██████▎   | 1.18G/1.88G [00:45<00:26, 26.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  64%|██████▎   | 1.20G/1.88G [00:45<00:26, 26.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  64%|██████▍   | 1.21G/1.88G [00:46<00:25, 26.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  65%|██████▍   | 1.22G/1.88G [00:46<00:25, 26.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  65%|██████▌   | 1.23G/1.88G [00:47<00:24, 26.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  66%|██████▌   | 1.24G/1.88G [00:47<00:24, 26.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  66%|██████▋   | 1.25G/1.88G [00:47<00:23, 26.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  67%|██████▋   | 1.26G/1.88G [00:48<00:23, 26.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  67%|██████▋   | 1.27G/1.88G [00:48<00:23, 25.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  68%|██████▊   | 1.28G/1.88G [00:49<00:23, 26.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  69%|██████▊   | 1.29G/1.88G [00:49<00:22, 26.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  69%|██████▉   | 1.30G/1.88G [00:49<00:21, 26.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  70%|██████▉   | 1.31G/1.88G [00:50<00:21, 26.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  70%|███████   | 1.32G/1.88G [00:50<00:20, 26.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  71%|███████   | 1.33G/1.88G [00:51<00:20, 26.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  71%|███████▏  | 1.34G/1.88G [00:51<00:19, 27.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  72%|███████▏  | 1.35G/1.88G [00:51<00:20, 26.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  73%|███████▎  | 1.36G/1.88G [00:52<00:20, 25.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  73%|███████▎  | 1.37G/1.88G [00:52<00:20, 24.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  74%|███████▎  | 1.38G/1.88G [00:53<00:22, 21.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  74%|███████▍  | 1.39G/1.88G [00:53<00:21, 22.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  75%|███████▍  | 1.41G/1.88G [00:54<00:20, 22.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  75%|███████▌  | 1.42G/1.88G [00:54<00:20, 23.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  76%|███████▌  | 1.43G/1.88G [00:55<00:20, 22.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  76%|███████▋  | 1.44G/1.88G [00:55<00:21, 20.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  77%|███████▋  | 1.45G/1.88G [00:56<00:18, 23.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  78%|███████▊  | 1.46G/1.88G [00:56<00:17, 24.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  78%|███████▊  | 1.47G/1.88G [00:57<00:18, 22.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  79%|███████▊  | 1.48G/1.88G [00:57<00:18, 21.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  79%|███████▉  | 1.49G/1.88G [00:58<00:18, 21.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  80%|███████▉  | 1.50G/1.88G [00:58<00:16, 23.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  80%|████████  | 1.51G/1.88G [00:58<00:15, 24.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  81%|████████  | 1.52G/1.88G [00:59<00:14, 24.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  81%|████████▏ | 1.53G/1.88G [00:59<00:14, 23.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  82%|████████▏ | 1.54G/1.88G [01:00<00:14, 23.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  83%|████████▎ | 1.55G/1.88G [01:00<00:15, 21.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  83%|████████▎ | 1.56G/1.88G [01:01<00:13, 22.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  84%|████████▎ | 1.57G/1.88G [01:01<00:13, 23.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  84%|████████▍ | 1.58G/1.88G [01:02<00:12, 23.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  85%|████████▍ | 1.59G/1.88G [01:02<00:11, 24.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  85%|████████▌ | 1.60G/1.88G [01:02<00:11, 23.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  86%|████████▌ | 1.61G/1.88G [01:03<00:11, 23.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  86%|████████▋ | 1.63G/1.88G [01:03<00:10, 23.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  87%|████████▋ | 1.64G/1.88G [01:04<00:09, 24.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  88%|████████▊ | 1.65G/1.88G [01:04<00:10, 22.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  88%|████████▊ | 1.66G/1.88G [01:05<00:09, 23.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  89%|████████▊ | 1.67G/1.88G [01:05<00:08, 24.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  89%|████████▉ | 1.68G/1.88G [01:05<00:08, 25.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  90%|████████▉ | 1.69G/1.88G [01:06<00:07, 25.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  90%|█████████ | 1.70G/1.88G [01:06<00:07, 25.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  91%|█████████ | 1.71G/1.88G [01:07<00:06, 26.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  91%|█████████▏| 1.72G/1.88G [01:07<00:06, 26.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  92%|█████████▏| 1.73G/1.88G [01:07<00:05, 26.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  93%|█████████▎| 1.74G/1.88G [01:08<00:05, 26.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  93%|█████████▎| 1.75G/1.88G [01:08<00:04, 26.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  94%|█████████▎| 1.76G/1.88G [01:09<00:04, 26.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  94%|█████████▍| 1.77G/1.88G [01:09<00:04, 26.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  95%|█████████▍| 1.78G/1.88G [01:09<00:03, 26.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  95%|█████████▌| 1.79G/1.88G [01:10<00:03, 26.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  96%|█████████▌| 1.80G/1.88G [01:10<00:02, 26.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  97%|█████████▋| 1.81G/1.88G [01:11<00:02, 26.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  97%|█████████▋| 1.82G/1.88G [01:11<00:02, 26.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  98%|█████████▊| 1.84G/1.88G [01:11<00:01, 26.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  98%|█████████▊| 1.85G/1.88G [01:12<00:01, 26.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  99%|█████████▊| 1.86G/1.88G [01:12<00:00, 25.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  99%|█████████▉| 1.87G/1.88G [01:13<00:00, 25.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin: 100%|█████████▉| 1.88G/1.88G [01:13<00:00, 26.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin: 100%|██████████| 1.88G/1.88G [01:13<00:00, 25.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 6/8 [03:00<01:18, 39.32s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   0%|          | 0.00/1.07G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   1%|          | 10.5M/1.07G [00:00<00:13, 81.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   2%|▏         | 21.0M/1.07G [00:00<00:12, 87.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   3%|▎         | 31.5M/1.07G [00:00<00:11, 91.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   4%|▍         | 41.9M/1.07G [00:00<00:11, 92.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   5%|▍         | 52.4M/1.07G [00:00<00:11, 90.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   6%|▌         | 62.9M/1.07G [00:00<00:11, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   7%|▋         | 73.4M/1.07G [00:00<00:10, 91.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   8%|▊         | 83.9M/1.07G [00:00<00:10, 90.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   9%|▉         | 94.4M/1.07G [00:01<00:10, 91.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  10%|▉         | 105M/1.07G [00:01<00:10, 88.9MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  11%|█         | 115M/1.07G [00:01<00:10, 90.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  12%|█▏        | 126M/1.07G [00:01<00:10, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  13%|█▎        | 136M/1.07G [00:01<00:10, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  14%|█▎        | 147M/1.07G [00:01<00:10, 91.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  15%|█▍        | 157M/1.07G [00:01<00:10, 91.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  16%|█▌        | 168M/1.07G [00:01<00:09, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  17%|█▋        | 178M/1.07G [00:01<00:10, 87.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  18%|█▊        | 189M/1.07G [00:02<00:10, 82.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  19%|█▊        | 199M/1.07G [00:02<00:10, 80.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  20%|█▉        | 210M/1.07G [00:02<00:10, 81.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  21%|██        | 220M/1.07G [00:02<00:10, 84.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  21%|██▏       | 231M/1.07G [00:02<00:09, 86.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  22%|██▏       | 241M/1.07G [00:02<00:09, 87.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  23%|██▎       | 252M/1.07G [00:02<00:09, 89.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  24%|██▍       | 262M/1.07G [00:02<00:09, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  25%|██▌       | 273M/1.07G [00:03<00:08, 90.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  26%|██▋       | 283M/1.07G [00:03<00:09, 85.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  27%|██▋       | 294M/1.07G [00:03<00:08, 86.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  28%|██▊       | 304M/1.07G [00:03<00:08, 88.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  29%|██▉       | 315M/1.07G [00:03<00:08, 89.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  30%|███       | 325M/1.07G [00:03<00:08, 87.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  31%|███       | 336M/1.07G [00:03<00:08, 87.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  32%|███▏      | 346M/1.07G [00:03<00:08, 89.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  33%|███▎      | 357M/1.07G [00:04<00:07, 89.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  34%|███▍      | 367M/1.07G [00:04<00:07, 89.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  35%|███▌      | 377M/1.07G [00:04<00:07, 88.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  36%|███▌      | 388M/1.07G [00:04<00:08, 82.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  37%|███▋      | 398M/1.07G [00:04<00:08, 84.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  38%|███▊      | 409M/1.07G [00:04<00:07, 86.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  39%|███▉      | 419M/1.07G [00:04<00:07, 85.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  40%|████      | 430M/1.07G [00:04<00:07, 87.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  41%|████      | 440M/1.07G [00:05<00:07, 86.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  42%|████▏     | 451M/1.07G [00:05<00:07, 86.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  43%|████▎     | 461M/1.07G [00:05<00:07, 87.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  44%|████▍     | 472M/1.07G [00:05<00:06, 87.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  45%|████▍     | 482M/1.07G [00:05<00:06, 88.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  46%|████▌     | 493M/1.07G [00:05<00:06, 89.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  47%|████▋     | 503M/1.07G [00:05<00:06, 91.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  48%|████▊     | 514M/1.07G [00:05<00:06, 91.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  49%|████▉     | 524M/1.07G [00:05<00:06, 90.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  50%|████▉     | 535M/1.07G [00:06<00:06, 88.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  51%|█████     | 545M/1.07G [00:06<00:05, 89.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  52%|█████▏    | 556M/1.07G [00:06<00:05, 88.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  53%|█████▎    | 566M/1.07G [00:06<00:05, 89.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  54%|█████▎    | 577M/1.07G [00:06<00:05, 90.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  55%|█████▍    | 587M/1.07G [00:06<00:05, 92.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  56%|█████▌    | 598M/1.07G [00:06<00:05, 93.0MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  57%|█████▋    | 608M/1.07G [00:06<00:04, 94.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  58%|█████▊    | 619M/1.07G [00:06<00:04, 92.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  59%|█████▊    | 629M/1.07G [00:07<00:04, 92.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  60%|█████▉    | 640M/1.07G [00:07<00:05, 79.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  61%|██████    | 650M/1.07G [00:07<00:05, 83.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  62%|██████▏   | 661M/1.07G [00:07<00:04, 84.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  62%|██████▏   | 671M/1.07G [00:07<00:04, 87.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  63%|██████▎   | 682M/1.07G [00:07<00:04, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  64%|██████▍   | 692M/1.07G [00:07<00:04, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  65%|██████▌   | 703M/1.07G [00:07<00:04, 88.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  66%|██████▋   | 713M/1.07G [00:08<00:04, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  67%|██████▋   | 724M/1.07G [00:08<00:03, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  68%|██████▊   | 734M/1.07G [00:08<00:03, 90.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  69%|██████▉   | 744M/1.07G [00:08<00:03, 90.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  70%|███████   | 755M/1.07G [00:08<00:03, 90.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  71%|███████▏  | 765M/1.07G [00:08<00:03, 90.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  72%|███████▏  | 776M/1.07G [00:08<00:03, 91.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  73%|███████▎  | 786M/1.07G [00:08<00:03, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  74%|███████▍  | 797M/1.07G [00:09<00:03, 88.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  75%|███████▌  | 807M/1.07G [00:09<00:02, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  76%|███████▌  | 818M/1.07G [00:09<00:02, 88.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  77%|███████▋  | 828M/1.07G [00:09<00:02, 89.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  78%|███████▊  | 839M/1.07G [00:09<00:02, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  79%|███████▉  | 849M/1.07G [00:09<00:02, 85.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  80%|████████  | 860M/1.07G [00:09<00:02, 82.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  81%|████████  | 870M/1.07G [00:09<00:02, 86.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  82%|████████▏ | 881M/1.07G [00:09<00:02, 88.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  83%|████████▎ | 891M/1.07G [00:10<00:02, 88.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  84%|████████▍ | 902M/1.07G [00:10<00:02, 85.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  85%|████████▍ | 912M/1.07G [00:10<00:01, 85.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  86%|████████▌ | 923M/1.07G [00:10<00:01, 85.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  87%|████████▋ | 933M/1.07G [00:10<00:01, 87.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  88%|████████▊ | 944M/1.07G [00:10<00:01, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  89%|████████▉ | 954M/1.07G [00:10<00:01, 92.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  90%|████████▉ | 965M/1.07G [00:10<00:01, 91.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  91%|█████████ | 975M/1.07G [00:11<00:01, 92.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  92%|█████████▏| 986M/1.07G [00:11<00:00, 91.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  93%|█████████▎| 996M/1.07G [00:11<00:00, 90.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  94%|█████████▎| 1.01G/1.07G [00:11<00:00, 90.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  95%|█████████▍| 1.02G/1.07G [00:11<00:00, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  96%|█████████▌| 1.03G/1.07G [00:11<00:00, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  97%|█████████▋| 1.04G/1.07G [00:11<00:00, 90.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  98%|█████████▊| 1.05G/1.07G [00:11<00:00, 90.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  99%|█████████▊| 1.06G/1.07G [00:11<00:00, 89.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin: 100%|█████████▉| 1.07G/1.07G [00:12<00:00, 84.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin: 100%|██████████| 1.07G/1.07G [00:12<00:00, 88.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  88%|████████▊ | 7/8 [03:13<00:30, 30.47s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:   0%|          | 0.00/1.07G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:   1%|          | 10.5M/1.07G [00:00<00:15, 69.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:   2%|▏         | 21.0M/1.07G [00:00<00:14, 74.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:   3%|▎         | 31.5M/1.07G [00:00<00:12, 83.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:   4%|▍         | 41.9M/1.07G [00:00<00:12, 83.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:   5%|▍         | 52.4M/1.07G [00:00<00:11, 86.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:   6%|▌         | 62.9M/1.07G [00:00<00:11, 89.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:   7%|▋         | 73.4M/1.07G [00:00<00:11, 89.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:   8%|▊         | 83.9M/1.07G [00:00<00:11, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:   9%|▉         | 94.4M/1.07G [00:01<00:10, 90.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  10%|▉         | 105M/1.07G [00:01<00:10, 91.9MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  11%|█         | 115M/1.07G [00:01<00:10, 92.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  12%|█▏        | 126M/1.07G [00:01<00:10, 90.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  13%|█▎        | 136M/1.07G [00:01<00:10, 88.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  14%|█▎        | 147M/1.07G [00:01<00:10, 90.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  15%|█▍        | 157M/1.07G [00:01<00:09, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  16%|█▌        | 168M/1.07G [00:01<00:09, 92.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  17%|█▋        | 178M/1.07G [00:02<00:09, 92.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  18%|█▊        | 189M/1.07G [00:02<00:09, 91.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  19%|█▊        | 199M/1.07G [00:02<00:09, 92.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  20%|█▉        | 210M/1.07G [00:02<00:09, 91.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  21%|██        | 220M/1.07G [00:02<00:09, 89.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  22%|██▏       | 231M/1.07G [00:02<00:09, 91.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  23%|██▎       | 241M/1.07G [00:02<00:09, 91.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  24%|██▎       | 252M/1.07G [00:02<00:08, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  25%|██▍       | 262M/1.07G [00:02<00:08, 91.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  25%|██▌       | 273M/1.07G [00:03<00:08, 92.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  26%|██▋       | 283M/1.07G [00:03<00:08, 93.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  27%|██▋       | 294M/1.07G [00:03<00:08, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  28%|██▊       | 304M/1.07G [00:03<00:08, 92.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  29%|██▉       | 315M/1.07G [00:03<00:08, 92.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  30%|███       | 325M/1.07G [00:03<00:08, 93.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  31%|███▏      | 336M/1.07G [00:03<00:08, 88.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  32%|███▏      | 346M/1.07G [00:03<00:08, 89.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  33%|███▎      | 357M/1.07G [00:03<00:07, 91.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  34%|███▍      | 367M/1.07G [00:04<00:08, 85.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  35%|███▌      | 377M/1.07G [00:04<00:08, 86.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  36%|███▋      | 388M/1.07G [00:04<00:08, 80.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  37%|███▋      | 398M/1.07G [00:04<00:08, 83.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  38%|███▊      | 409M/1.07G [00:04<00:07, 85.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  39%|███▉      | 419M/1.07G [00:04<00:07, 88.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  40%|████      | 430M/1.07G [00:04<00:07, 84.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  41%|████      | 440M/1.07G [00:04<00:07, 86.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  42%|████▏     | 451M/1.07G [00:05<00:06, 88.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  43%|████▎     | 461M/1.07G [00:05<00:06, 89.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  44%|████▍     | 472M/1.07G [00:05<00:06, 90.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  45%|████▌     | 482M/1.07G [00:05<00:06, 85.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  46%|████▌     | 493M/1.07G [00:05<00:06, 88.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  47%|████▋     | 503M/1.07G [00:05<00:06, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  48%|████▊     | 514M/1.07G [00:05<00:06, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  49%|████▉     | 524M/1.07G [00:05<00:05, 92.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  50%|█████     | 535M/1.07G [00:05<00:05, 93.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  51%|█████     | 545M/1.07G [00:06<00:05, 93.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  52%|█████▏    | 556M/1.07G [00:06<00:05, 94.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  53%|█████▎    | 566M/1.07G [00:06<00:05, 95.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  54%|█████▍    | 577M/1.07G [00:06<00:05, 95.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  55%|█████▍    | 587M/1.07G [00:06<00:05, 93.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  56%|█████▌    | 598M/1.07G [00:06<00:05, 93.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  57%|█████▋    | 608M/1.07G [00:06<00:04, 93.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  58%|█████▊    | 619M/1.07G [00:06<00:04, 90.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  59%|█████▉    | 629M/1.07G [00:07<00:04, 92.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  60%|█████▉    | 640M/1.07G [00:07<00:04, 93.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  61%|██████    | 650M/1.07G [00:07<00:04, 93.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  62%|██████▏   | 661M/1.07G [00:07<00:04, 93.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  63%|██████▎   | 671M/1.07G [00:07<00:04, 93.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  64%|██████▎   | 682M/1.07G [00:07<00:04, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  65%|██████▍   | 692M/1.07G [00:07<00:04, 90.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  66%|██████▌   | 703M/1.07G [00:07<00:04, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  67%|██████▋   | 713M/1.07G [00:07<00:03, 89.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  68%|██████▊   | 724M/1.07G [00:08<00:03, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  69%|██████▊   | 734M/1.07G [00:08<00:03, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  70%|██████▉   | 744M/1.07G [00:08<00:03, 91.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  71%|███████   | 755M/1.07G [00:08<00:03, 92.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  72%|███████▏  | 765M/1.07G [00:08<00:03, 93.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  73%|███████▎  | 776M/1.07G [00:08<00:03, 94.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  74%|███████▎  | 786M/1.07G [00:08<00:03, 93.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  75%|███████▍  | 797M/1.07G [00:08<00:03, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  76%|███████▌  | 807M/1.07G [00:08<00:02, 88.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  76%|███████▋  | 818M/1.07G [00:09<00:02, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  77%|███████▋  | 828M/1.07G [00:09<00:02, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  78%|███████▊  | 839M/1.07G [00:09<00:02, 91.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  79%|███████▉  | 849M/1.07G [00:09<00:02, 87.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  80%|████████  | 860M/1.07G [00:09<00:02, 88.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  81%|████████▏ | 870M/1.07G [00:09<00:02, 90.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  82%|████████▏ | 881M/1.07G [00:09<00:02, 90.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  83%|████████▎ | 891M/1.07G [00:09<00:01, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  84%|████████▍ | 902M/1.07G [00:09<00:01, 93.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  85%|████████▌ | 912M/1.07G [00:10<00:01, 93.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  86%|████████▋ | 923M/1.07G [00:10<00:01, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  87%|████████▋ | 933M/1.07G [00:10<00:01, 92.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  88%|████████▊ | 944M/1.07G [00:10<00:01, 92.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  89%|████████▉ | 954M/1.07G [00:10<00:01, 94.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  90%|█████████ | 965M/1.07G [00:10<00:01, 94.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  91%|█████████ | 975M/1.07G [00:10<00:01, 84.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  92%|█████████▏| 986M/1.07G [00:10<00:00, 86.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  93%|█████████▎| 996M/1.07G [00:11<00:00, 87.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  94%|█████████▍| 1.01G/1.07G [00:11<00:00, 85.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  95%|█████████▌| 1.02G/1.07G [00:11<00:00, 83.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  96%|█████████▌| 1.03G/1.07G [00:11<00:00, 85.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  97%|█████████▋| 1.04G/1.07G [00:11<00:00, 88.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  98%|█████████▊| 1.05G/1.07G [00:11<00:00, 85.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  99%|█████████▉| 1.06G/1.07G [00:11<00:00, 87.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin: 100%|██████████| 1.07G/1.07G [00:11<00:00, 88.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin: 100%|██████████| 1.07G/1.07G [00:11<00:00, 89.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [03:25<00:00, 24.59s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [03:25<00:00, 25.64s/it]\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:575] 2023-05-06 14:33:32,141 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"transformers_version\": \"4.27.1\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:575] 2023-05-06 14:33:32,141 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"transformers_version\": \"4.27.1\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▎        | 1/8 [00:13<01:32, 13.14s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 2/8 [00:22<01:06, 11.04s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  38%|███▊      | 3/8 [00:25<00:36,  7.37s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 4/8 [00:27<00:20,  5.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:28<00:11,  3.78s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:30<00:06,  3.01s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:31<00:02,  2.32s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:32<00:00,  1.87s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:32<00:00,  4.01s/it]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3032] 2023-05-06 14:34:04,623 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:3034] 2023-05-06 14:34:04,623 >> Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at THUDM/chatglm-6b and are newly initialized: ['transformer.prefix_encoder.embedding.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3032] 2023-05-06 14:34:04,623 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:3034] 2023-05-06 14:34:04,623 >> Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at THUDM/chatglm-6b and are newly initialized: ['transformer.prefix_encoder.embedding.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2690] 2023-05-06 14:34:04,757 >> Generation config file not found, using a generation config created from the model config.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2690] 2023-05-06 14:34:04,757 >> Generation config file not found, using a generation config created from the model config.\u001b[0m\n",
      "\u001b[34mQuantized to 4 bit\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   0%|          | 0/115 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   1%|          | 1/115 [00:00<01:34,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   2%|▏         | 2/115 [00:01<01:32,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 3/115 [00:02<01:31,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 4/115 [00:03<01:30,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   4%|▍         | 5/115 [00:04<01:29,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   5%|▌         | 6/115 [00:04<01:29,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   6%|▌         | 7/115 [00:05<01:27,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   7%|▋         | 8/115 [00:06<01:26,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   8%|▊         | 9/115 [00:07<01:26,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   9%|▊         | 10/115 [00:08<01:25,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|▉         | 11/115 [00:08<01:24,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|█         | 12/115 [00:09<01:23,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  11%|█▏        | 13/115 [00:10<01:22,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  12%|█▏        | 14/115 [00:11<01:21,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  13%|█▎        | 15/115 [00:12<01:20,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  14%|█▍        | 16/115 [00:12<01:19,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  15%|█▍        | 17/115 [00:13<01:19,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  16%|█▌        | 18/115 [00:14<01:18,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 19/115 [00:15<01:17,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 20/115 [00:16<01:17,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  18%|█▊        | 21/115 [00:17<01:16,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  19%|█▉        | 22/115 [00:17<01:16,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  20%|██        | 23/115 [00:18<01:15,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  21%|██        | 24/115 [00:19<01:13,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  22%|██▏       | 25/115 [00:20<01:12,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 26/115 [00:21<01:11,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 27/115 [00:21<01:10,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  24%|██▍       | 28/115 [00:22<01:10,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  25%|██▌       | 29/115 [00:23<01:09,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  26%|██▌       | 30/115 [00:24<01:08,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  27%|██▋       | 31/115 [00:25<01:07,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  28%|██▊       | 32/115 [00:25<01:07,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  29%|██▊       | 33/115 [00:26<01:06,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|██▉       | 34/115 [00:27<01:05,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|███       | 35/115 [00:28<01:04,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  31%|███▏      | 36/115 [00:29<01:03,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  32%|███▏      | 37/115 [00:29<01:02,  1.25ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  33%|███▎      | 38/115 [00:30<01:01,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  34%|███▍      | 39/115 [00:31<01:00,  1.25ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  35%|███▍      | 40/115 [00:32<01:00,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  36%|███▌      | 41/115 [00:33<00:59,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 42/115 [00:34<00:59,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 43/115 [00:34<00:58,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  38%|███▊      | 44/115 [00:35<00:57,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  39%|███▉      | 45/115 [00:36<00:56,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  40%|████      | 46/115 [00:37<00:55,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  41%|████      | 47/115 [00:38<00:55,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  42%|████▏     | 48/115 [00:38<00:54,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 49/115 [00:39<00:53,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 50/115 [00:40<00:53,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  44%|████▍     | 51/115 [00:41<00:52,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  45%|████▌     | 52/115 [00:42<00:51,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  46%|████▌     | 53/115 [00:43<00:51,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  47%|████▋     | 54/115 [00:43<00:49,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  48%|████▊     | 55/115 [00:44<00:48,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  49%|████▊     | 56/115 [00:45<00:47,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|████▉     | 57/115 [00:46<00:47,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|█████     | 58/115 [00:47<00:46,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  51%|█████▏    | 59/115 [00:47<00:45,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  52%|█████▏    | 60/115 [00:48<00:44,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  53%|█████▎    | 61/115 [00:49<00:44,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  54%|█████▍    | 62/115 [00:50<00:43,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  55%|█████▍    | 63/115 [00:51<00:42,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  56%|█████▌    | 64/115 [00:51<00:41,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 65/115 [00:52<00:40,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 66/115 [00:53<00:39,  1.25ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  58%|█████▊    | 67/115 [00:54<00:38,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  59%|█████▉    | 68/115 [00:55<00:38,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  60%|██████    | 69/115 [00:55<00:37,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  61%|██████    | 70/115 [00:56<00:36,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  62%|██████▏   | 71/115 [00:57<00:35,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 72/115 [00:58<00:34,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 73/115 [00:59<00:33,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  64%|██████▍   | 74/115 [01:00<00:33,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  65%|██████▌   | 75/115 [01:00<00:32,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  66%|██████▌   | 76/115 [01:01<00:31,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  67%|██████▋   | 77/115 [01:02<00:30,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  68%|██████▊   | 78/115 [01:03<00:29,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  69%|██████▊   | 79/115 [01:04<00:28,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|██████▉   | 80/115 [01:04<00:28,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|███████   | 81/115 [01:05<00:27,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  71%|███████▏  | 82/115 [01:06<00:26,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  72%|███████▏  | 83/115 [01:07<00:25,  1.25ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  73%|███████▎  | 84/115 [01:08<00:24,  1.25ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  74%|███████▍  | 85/115 [01:08<00:24,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  75%|███████▍  | 86/115 [01:09<00:23,  1.25ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  76%|███████▌  | 87/115 [01:10<00:22,  1.25ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 88/115 [01:11<00:21,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 89/115 [01:12<00:21,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  78%|███████▊  | 90/115 [01:12<00:20,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  79%|███████▉  | 91/115 [01:13<00:19,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  80%|████████  | 92/115 [01:14<00:18,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  81%|████████  | 93/115 [01:15<00:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  82%|████████▏ | 94/115 [01:16<00:17,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 95/115 [01:17<00:16,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 96/115 [01:17<00:15,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  84%|████████▍ | 97/115 [01:18<00:14,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  85%|████████▌ | 98/115 [01:19<00:13,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  86%|████████▌ | 99/115 [01:20<00:12,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  87%|████████▋ | 100/115 [01:21<00:12,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  88%|████████▊ | 101/115 [01:21<00:11,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  89%|████████▊ | 102/115 [01:22<00:10,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|████████▉ | 103/115 [01:23<00:09,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|█████████ | 104/115 [01:24<00:08,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  91%|█████████▏| 105/115 [01:25<00:08,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  92%|█████████▏| 106/115 [01:25<00:07,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  93%|█████████▎| 107/115 [01:26<00:06,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  94%|█████████▍| 108/115 [01:27<00:05,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  95%|█████████▍| 109/115 [01:28<00:04,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  96%|█████████▌| 110/115 [01:29<00:04,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 111/115 [01:29<00:03,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 112/115 [01:30<00:02,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  98%|█████████▊| 113/115 [01:31<00:01,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  99%|█████████▉| 114/115 [01:32<00:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:32<00:00,  1.39ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:32<00:00,  1.24ba/s]\u001b[0m\n",
      "\u001b[34minput_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\u001b[0m\n",
      "\u001b[34minputs 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[34mlabel_ids\u001b[0m\n",
      "\u001b[34m[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[34mlabels 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m0%|          | 0/100 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-05-06 14:37:47.600: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.27.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-06 14:37:47.636 algo-1:169 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-06 14:37:47.670 algo-1:169 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-06 14:37:47.670 algo-1:169 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-06 14:37:47.671 algo-1:169 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-06 14:37:47.671 algo-1:169 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-06 14:37:47.671 algo-1:169 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m05/06/2023 14:37:48 - WARNING - transformers_modules.THUDM.chatglm-6b.658202d88ac4bb782b99e99ac3adff58b4d0b813.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m1%|          | 1/100 [00:13<22:11, 13.45s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 2/100 [00:24<19:27, 11.91s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 3/100 [00:35<18:26, 11.41s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 4/100 [00:45<17:52, 11.18s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 5/100 [00:56<17:29, 11.05s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 6/100 [01:07<17:14, 11.00s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 7/100 [01:18<17:06, 11.04s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 8/100 [01:29<16:58, 11.07s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 9/100 [01:40<16:48, 11.08s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 10/100 [01:52<16:38, 11.09s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 5.1726, 'learning_rate': 0.018000000000000002, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m10%|█         | 10/100 [01:52<16:38, 11.09s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 11/100 [02:03<16:27, 11.10s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 12/100 [02:14<16:16, 11.10s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 13/100 [02:25<16:05, 11.10s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 14/100 [02:36<15:54, 11.10s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 15/100 [02:47<15:43, 11.09s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 16/100 [02:58<15:31, 11.09s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 17/100 [03:09<15:20, 11.10s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 18/100 [03:20<15:11, 11.11s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 19/100 [03:32<14:59, 11.11s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 20/100 [03:43<14:48, 11.11s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 4.6054, 'learning_rate': 0.016, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m20%|██        | 20/100 [03:43<14:48, 11.11s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 21/100 [03:54<14:37, 11.11s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 22/100 [04:05<14:25, 11.10s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 23/100 [04:16<14:14, 11.10s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 24/100 [04:27<14:04, 11.11s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 25/100 [04:38<13:52, 11.11s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 26/100 [04:49<13:41, 11.11s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 27/100 [05:00<13:30, 11.11s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 28/100 [05:11<13:19, 11.11s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 29/100 [05:23<13:08, 11.11s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 30/100 [05:34<12:57, 11.10s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 4.5567, 'learning_rate': 0.013999999999999999, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m30%|███       | 30/100 [05:34<12:57, 11.10s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 31/100 [05:45<12:45, 11.10s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 32/100 [05:56<12:34, 11.09s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 33/100 [06:07<12:23, 11.10s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 34/100 [06:18<12:12, 11.10s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 35/100 [06:29<12:01, 11.11s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 36/100 [06:40<11:50, 11.10s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 37/100 [06:51<11:39, 11.10s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 38/100 [07:02<11:27, 11.10s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 39/100 [07:14<11:16, 11.09s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 40/100 [07:25<11:06, 11.10s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 4.3762, 'learning_rate': 0.012, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m40%|████      | 40/100 [07:25<11:06, 11.10s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 41/100 [07:36<10:55, 11.11s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 42/100 [07:47<10:44, 11.10s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 43/100 [07:58<10:32, 11.09s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 44/100 [08:09<10:21, 11.09s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 45/100 [08:20<10:10, 11.11s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 46/100 [08:31<09:59, 11.11s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 47/100 [08:42<09:48, 11.10s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 48/100 [08:54<09:37, 11.11s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 49/100 [09:05<09:26, 11.11s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 50/100 [09:16<09:15, 11.11s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 4.3922, 'learning_rate': 0.01, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m50%|█████     | 50/100 [09:16<09:15, 11.11s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 51/100 [09:27<09:04, 11.10s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 52/100 [09:38<08:52, 11.10s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 53/100 [09:49<08:41, 11.10s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 54/100 [10:00<08:30, 11.10s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 55/100 [10:11<08:19, 11.11s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 56/100 [10:22<08:08, 11.11s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 57/100 [10:33<07:57, 11.10s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 58/100 [10:45<07:46, 11.10s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 59/100 [10:56<07:35, 11.10s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 60/100 [11:07<07:24, 11.10s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 4.2952, 'learning_rate': 0.008, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m60%|██████    | 60/100 [11:07<07:24, 11.10s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 61/100 [11:18<07:13, 11.11s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 62/100 [11:29<07:01, 11.10s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 63/100 [11:40<06:50, 11.10s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 64/100 [11:51<06:39, 11.11s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 65/100 [12:02<06:28, 11.10s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 66/100 [12:13<06:17, 11.10s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 67/100 [12:24<06:06, 11.10s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 68/100 [12:36<05:55, 11.10s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 69/100 [12:47<05:44, 11.10s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 70/100 [12:58<05:33, 11.10s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 4.3251, 'learning_rate': 0.006, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m70%|███████   | 70/100 [12:58<05:33, 11.10s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 71/100 [13:09<05:21, 11.10s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 72/100 [13:20<05:10, 11.10s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 73/100 [13:31<04:59, 11.11s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 74/100 [13:42<04:48, 11.11s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 75/100 [13:53<04:37, 11.10s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 76/100 [14:04<04:26, 11.10s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 77/100 [14:16<04:15, 11.11s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 78/100 [14:27<04:04, 11.11s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 79/100 [14:38<03:53, 11.10s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 80/100 [14:49<03:41, 11.10s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 4.2588, 'learning_rate': 0.004, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m80%|████████  | 80/100 [14:49<03:41, 11.10s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 81/100 [15:00<03:30, 11.10s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 82/100 [15:11<03:19, 11.10s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 83/100 [15:22<03:08, 11.09s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 84/100 [15:33<02:57, 11.10s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 85/100 [15:44<02:46, 11.10s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 86/100 [15:55<02:35, 11.10s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 87/100 [16:06<02:24, 11.08s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 88/100 [16:18<02:13, 11.10s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 89/100 [16:29<02:02, 11.10s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 90/100 [16:40<01:51, 11.10s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 4.2729, 'learning_rate': 0.002, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m90%|█████████ | 90/100 [16:40<01:51, 11.10s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 91/100 [16:51<01:39, 11.10s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 92/100 [17:02<01:28, 11.09s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 93/100 [17:13<01:17, 11.10s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 94/100 [17:24<01:06, 11.10s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 95/100 [17:35<00:55, 11.10s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 96/100 [17:46<00:44, 11.10s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 97/100 [17:57<00:33, 11.10s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 98/100 [18:09<00:22, 11.10s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 99/100 [18:20<00:11, 11.10s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 100/100 [18:31<00:00, 11.11s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 4.2865, 'learning_rate': 0.0, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m100%|██████████| 100/100 [18:31<00:00, 11.11s/it]\u001b[0m\n",
      "\u001b[34mSaving PrefixEncoder\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-05-06 14:56:18,772 >> Configuration saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-100/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-05-06 14:56:18,772 >> Configuration saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-100/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-05-06 14:56:18,774 >> Configuration saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-100/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-05-06 14:56:18,774 >> Configuration saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-100/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1762] 2023-05-06 14:56:18,970 >> Model weights saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-100/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1762] 2023-05-06 14:56:18,970 >> Model weights saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-100/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2163] 2023-05-06 14:56:18,971 >> tokenizer config file saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-100/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2163] 2023-05-06 14:56:18,971 >> tokenizer config file saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-100/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2170] 2023-05-06 14:56:18,972 >> Special tokens file saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-100/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2170] 2023-05-06 14:56:18,972 >> Special tokens file saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-100/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m{'train_runtime': 1111.8988, 'train_samples_per_second': 1.439, 'train_steps_per_second': 0.09, 'train_loss': 4.454150390625, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m100%|██████████| 100/100 [18:31<00:00, 11.11s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 100/100 [18:31<00:00, 11.12s/it]\u001b[0m\n",
      "\u001b[34m***** train metrics *****\u001b[0m\n",
      "\u001b[34mepoch                    =       0.01\n",
      "  train_loss               =     4.4542\n",
      "  train_runtime            = 0:18:31.89\n",
      "  train_samples            =     114599\n",
      "  train_samples_per_second =      1.439\n",
      "  train_steps_per_second   =       0.09\u001b[0m\n",
      "\u001b[34m------saving model!-----\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2163] 2023-05-06 14:56:19,373 >> tokenizer config file saved in /opt/ml/model/adgen-chatglm-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2163] 2023-05-06 14:56:19,373 >> tokenizer config file saved in /opt/ml/model/adgen-chatglm-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2170] 2023-05-06 14:56:19,373 >> Special tokens file saved in /opt/ml/model/adgen-chatglm-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2170] 2023-05-06 14:56:19,373 >> Special tokens file saved in /opt/ml/model/adgen-chatglm-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSaving PrefixEncoder\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-05-06 14:56:19,381 >> Configuration saved in /opt/ml/model/adgen-chatglm-6b-ft/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-05-06 14:56:19,381 >> Configuration saved in /opt/ml/model/adgen-chatglm-6b-ft/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-05-06 14:56:19,383 >> Configuration saved in /opt/ml/model/adgen-chatglm-6b-ft/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-05-06 14:56:19,383 >> Configuration saved in /opt/ml/model/adgen-chatglm-6b-ft/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1762] 2023-05-06 14:56:19,576 >> Model weights saved in /opt/ml/model/adgen-chatglm-6b-ft/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1762] 2023-05-06 14:56:19,576 >> Model weights saved in /opt/ml/model/adgen-chatglm-6b-ft/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2163] 2023-05-06 14:56:19,578 >> tokenizer config file saved in /opt/ml/model/adgen-chatglm-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2163] 2023-05-06 14:56:19,578 >> tokenizer config file saved in /opt/ml/model/adgen-chatglm-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2170] 2023-05-06 14:56:19,578 >> Special tokens file saved in /opt/ml/model/adgen-chatglm-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2170] 2023-05-06 14:56:19,578 >> Special tokens file saved in /opt/ml/model/adgen-chatglm-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m------model is saved!-----\u001b[0m\n",
      "\u001b[34m2023-05-06 14:56:20,639 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-05-06 14:56:20,639 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-05-06 14:56:20,639 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-05-06 14:56:28 Uploading - Uploading generated training model\n",
      "2023-05-06 14:57:29 Completed - Training job completed\n",
      "Training seconds: 1899\n",
      "Billable seconds: 1899\n"
     ]
    }
   ],
   "source": [
    "# define Training Job Name \n",
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "instance_type=\"ml.g4dn.2xlarge\"\n",
    "\n",
    "job_name = f'huggingface-chatglm-simple-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "#define the model s3 path which will store your trained model asset\n",
    "#Note: you should use your real s3 path to configure model_s3_path\n",
    "model_s3_path='s3://{}/llm/models/chatglm/simple/'.format(sagemaker_session.default_bucket())\n",
    "output_dir = '/opt/ml/model/adgen-chatglm-6b-ft'\n",
    "model_name_or_path = 'THUDM/chatglm-6b'\n",
    "\n",
    "\n",
    "instance_count = 1\n",
    "#define the enviroment variables for your scripts.\n",
    "environment = {\n",
    "              'MODEL_S3_PATH'          : model_s3_path,\n",
    "              'PYTORCH_CUDA_ALLOC_CONF': 'max_split_size_mb:32',\n",
    "              #'LD_LIBRARY_PATH'        : '${LD_LIBRARY_PATH}:/opt/conda/lib/',\n",
    "              'TRAIN_DATASET'          : '/opt/ml/input/data/AdvertiseGen/train.json',\n",
    "              'TEST_DATASET'           : '/opt/ml/input/data/AdvertiseGen/dev.json',\n",
    "              'PROMPT_COLUMN'          : 'content',\n",
    "              'RESPONSE_COLUMN'        : 'summary',\n",
    "              'MODEL_NAME_OR_PATH'     : model_name_or_path,\n",
    "              'OUTPUT_DIR'             : output_dir,\n",
    "              'MODEL_OUTPUT_S3_PATH'   : model_s3_path,\n",
    "              'TRAIN_STEPS'            : '100'\n",
    "}\n",
    "\n",
    "inputs={\n",
    "   'AdvertiseGen': f\"s3://{bucket}/llm/chatglm/datasets/\"\n",
    "}\n",
    "\n",
    "\n",
    "# create the Estimator\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "#huggingface_estimator = PyTorch(\n",
    "#                            entry_point          = 'start_simple.py',          # user endpoint script\n",
    "#                            source_dir           = 'ChatGLM-6B/ptuning',\n",
    "#                            role=role,\n",
    "#                            framework_version='1.13',\n",
    "#                            py_version='py39',\n",
    "#                            script_mode=True,\n",
    "#                            instance_count=1,  # 1 or 2 or ...\n",
    "#                            instance_type=instance_type,\n",
    "#                            environment = environment)\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'start_simple.py',          # user endpoint script\n",
    "    source_dir           = 'ChatGLM-6B/ptuning',               # directory which includes all the files needed for training\n",
    "    instance_type        = instance_type, # instances type used for the training job\n",
    "    instance_count       = instance_count,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,           # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    script_mode          = True,\n",
    "    transformers_version = '4.26',            # the transformers version used in the training job\n",
    "    pytorch_version      = '1.13',            # the pytorch_version version used in the training job\n",
    "    py_version           = 'py39',            # the python version used in the training job\n",
    "    environment = environment\n",
    ")\n",
    "\n",
    "huggingface_estimator.fit(inputs=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d799c69-96b4-4f7e-8f8a-df387f431106",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-simple-2023-05-06-1-2023-05-06-14-24-12-728/output/model.tar.gz'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8e7992f-e414-40a0-9883-4d3dd1b6ea73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-06 14:57:23  418.8 MiB huggingface-chatglm-simple-2023-05-06-1-2023-05-06-14-24-12-728/output/model.tar.gz\n",
      "\n",
      "Total Objects: 1\n",
      "   Total Size: 418.8 MiB\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-simple-2023-05-06-1-2023-05-06-14-24-12-728/output/model.tar.gz --recursive --human-readable --summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc42f0d-c317-4243-9a4f-5c4e6e9c4b81",
   "metadata": {
    "tags": []
   },
   "source": [
    "For local test only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b8a09a3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/27/2023 09:11:17 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "04/27/2023 09:11:17 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=16,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.02,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=output/adgen-chatglm-6b-pt-128-2e-2/runs/Apr27_09-11-17_ip-172-16-82-196.us-west-2.compute.internal,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=300,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=output/adgen-chatglm-6b-pt-128-2e-2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=output/adgen-chatglm-6b-pt-128-2e-2,\n",
      "save_on_each_node=False,\n",
      "save_steps=300,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "04/27/2023 09:11:17 - WARNING - datasets.builder - Found cached dataset json (/home/ec2-user/.cache/huggingface/datasets/json/default-1e7622c3b4e29de4/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 642.02it/s]\n",
      "[INFO|configuration_utils.py:668] 2023-04-27 09:11:18,027 >> loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/a8ede826cf1b62bd3c78bdfb3625c7c5d2048fbd/config.json\n",
      "[WARNING|configuration_auto.py:905] 2023-04-27 09:11:18,027 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "[INFO|configuration_utils.py:668] 2023-04-27 09:11:18,416 >> loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/a8ede826cf1b62bd3c78bdfb3625c7c5d2048fbd/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-04-27 09:11:18,417 >> Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm-6b\",\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\"\n",
      "  },\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"gmask_token_id\": 130001,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"inner_hidden_size\": 16384,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"mask_token_id\": 130000,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_encoding_2d\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 130528\n",
      "}\n",
      "\n",
      "[WARNING|tokenization_auto.py:652] 2023-04-27 09:11:18,503 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-04-27 09:11:18,755 >> loading file ice_text.model from cache at /home/ec2-user/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/a8ede826cf1b62bd3c78bdfb3625c7c5d2048fbd/ice_text.model\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-04-27 09:11:18,755 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-04-27 09:11:18,755 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-04-27 09:11:18,755 >> loading file tokenizer_config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/a8ede826cf1b62bd3c78bdfb3625c7c5d2048fbd/tokenizer_config.json\n",
      "[WARNING|auto_factory.py:456] 2023-04-27 09:11:19,003 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "[INFO|modeling_utils.py:2403] 2023-04-27 09:11:19,849 >> loading weights file pytorch_model.bin from cache at /home/ec2-user/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/a8ede826cf1b62bd3c78bdfb3625c7c5d2048fbd/pytorch_model.bin.index.json\n",
      "[INFO|configuration_utils.py:575] 2023-04-27 09:11:19,851 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"transformers_version\": \"4.27.1\"\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 8/8 [00:09<00:00,  1.15s/it]\n",
      "[INFO|modeling_utils.py:3032] 2023-04-27 09:11:29,359 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.\n",
      "\n",
      "[WARNING|modeling_utils.py:3034] 2023-04-27 09:11:29,359 >> Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at THUDM/chatglm-6b and are newly initialized: ['transformer.prefix_encoder.embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[INFO|modeling_utils.py:2690] 2023-04-27 09:11:29,491 >> Generation config file not found, using a generation config created from the model config.\n",
      "Quantized to 4 bit\n",
      "input_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "inputs 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\n",
      "label_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "labels 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|                                                   | 0/300 [00:00<?, ?it/s]04/27/2023 09:15:02 - WARNING - transformers_modules.THUDM.chatglm-6b.a8ede826cf1b62bd3c78bdfb3625c7c5d2048fbd.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "{'loss': 5.1745, 'learning_rate': 0.019333333333333334, 'epoch': 0.0}           \n",
      "{'loss': 4.5993, 'learning_rate': 0.018666666666666668, 'epoch': 0.0}           \n",
      "{'loss': 4.5411, 'learning_rate': 0.018000000000000002, 'epoch': 0.0}           \n",
      "{'loss': 4.348, 'learning_rate': 0.017333333333333336, 'epoch': 0.01}           \n",
      "{'loss': 4.3562, 'learning_rate': 0.016666666666666666, 'epoch': 0.01}          \n",
      "{'loss': 4.2723, 'learning_rate': 0.016, 'epoch': 0.01}                         \n",
      "{'loss': 4.3071, 'learning_rate': 0.015333333333333334, 'epoch': 0.01}          \n",
      "{'loss': 4.2592, 'learning_rate': 0.014666666666666666, 'epoch': 0.01}          \n",
      "{'loss': 4.2857, 'learning_rate': 0.013999999999999999, 'epoch': 0.01}          \n",
      "{'loss': 4.3017, 'learning_rate': 0.013333333333333332, 'epoch': 0.01}          \n",
      "{'loss': 4.407, 'learning_rate': 0.012666666666666666, 'epoch': 0.02}           \n",
      "{'loss': 4.3251, 'learning_rate': 0.012, 'epoch': 0.02}                         \n",
      "{'loss': 4.1959, 'learning_rate': 0.011333333333333332, 'epoch': 0.02}          \n",
      "{'loss': 4.2457, 'learning_rate': 0.010666666666666666, 'epoch': 0.02}          \n",
      "{'loss': 4.2632, 'learning_rate': 0.01, 'epoch': 0.02}                          \n",
      "{'loss': 4.2607, 'learning_rate': 0.009333333333333334, 'epoch': 0.02}          \n",
      "{'loss': 4.2509, 'learning_rate': 0.008666666666666668, 'epoch': 0.02}          \n",
      "{'loss': 4.2292, 'learning_rate': 0.008, 'epoch': 0.03}                         \n",
      "{'loss': 4.2908, 'learning_rate': 0.007333333333333333, 'epoch': 0.03}          \n",
      "{'loss': 4.196, 'learning_rate': 0.006666666666666666, 'epoch': 0.03}           \n",
      "{'loss': 4.2148, 'learning_rate': 0.006, 'epoch': 0.03}                         \n",
      "{'loss': 4.2693, 'learning_rate': 0.005333333333333333, 'epoch': 0.03}          \n",
      "{'loss': 4.1732, 'learning_rate': 0.004666666666666667, 'epoch': 0.03}          \n",
      "{'loss': 4.2286, 'learning_rate': 0.004, 'epoch': 0.03}                         \n",
      "{'loss': 4.1814, 'learning_rate': 0.003333333333333333, 'epoch': 0.03}          \n",
      "{'loss': 4.2129, 'learning_rate': 0.0026666666666666666, 'epoch': 0.04}         \n",
      "{'loss': 4.1679, 'learning_rate': 0.002, 'epoch': 0.04}                         \n",
      "{'loss': 4.3041, 'learning_rate': 0.0013333333333333333, 'epoch': 0.04}         \n",
      "{'loss': 4.2133, 'learning_rate': 0.0006666666666666666, 'epoch': 0.04}         \n",
      "{'loss': 4.1121, 'learning_rate': 0.0, 'epoch': 0.04}                           \n",
      "100%|█████████████████████████████████████████| 300/300 [56:48<00:00, 11.57s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:457] 2023-04-27 10:11:51,803 >> Configuration saved in output/adgen-chatglm-6b-pt-128-2e-2/checkpoint-300/config.json\n",
      "[INFO|configuration_utils.py:362] 2023-04-27 10:11:51,805 >> Configuration saved in output/adgen-chatglm-6b-pt-128-2e-2/checkpoint-300/generation_config.json\n",
      "[INFO|modeling_utils.py:1762] 2023-04-27 10:11:51,998 >> Model weights saved in output/adgen-chatglm-6b-pt-128-2e-2/checkpoint-300/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2163] 2023-04-27 10:11:51,999 >> tokenizer config file saved in output/adgen-chatglm-6b-pt-128-2e-2/checkpoint-300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2170] 2023-04-27 10:11:51,999 >> Special tokens file saved in output/adgen-chatglm-6b-pt-128-2e-2/checkpoint-300/special_tokens_map.json\n",
      "{'train_runtime': 3409.4339, 'train_samples_per_second': 1.408, 'train_steps_per_second': 0.088, 'train_loss': 4.306236979166667, 'epoch': 0.04}\n",
      "100%|█████████████████████████████████████████| 300/300 [56:49<00:00, 11.36s/it]\n",
      "***** train metrics *****\n",
      "  epoch                    =       0.04\n",
      "  train_loss               =     4.3062\n",
      "  train_runtime            = 0:56:49.43\n",
      "  train_samples            =     114599\n",
      "  train_samples_per_second =      1.408\n",
      "  train_steps_per_second   =      0.088\n"
     ]
    }
   ],
   "source": [
    "!cd ChatGLM-6B/ptuning/&& bash train.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e130efd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### chatglm 官方deepspeed方式（全参数的Finetune,单机多卡）\n",
    "1: 准备deepspeed lib，并修改deepspeed.json    \n",
    "2：数据集（以上一致）  \n",
    "3：entrypoint start-single-node.py,设置num-gpus\n",
    "4：触发bash ds_train_finetune_single_node.sh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "56a48210-48ff-4840-a9a0-c8d0e7085adb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processes_per_host is set to: 8\n"
     ]
    }
   ],
   "source": [
    "instance_type = 'ml.p4d.24xlarge'\n",
    "if instance_type in [\n",
    "    \"ml.p3.16xlarge\",\n",
    "    \"ml.p3dn.24xlarge\",\n",
    "    \"ml.g5.48xlarge\",\n",
    "    \"ml.p4d.24xlarge\"    \n",
    "]:\n",
    "    processes_per_host = 8\n",
    "elif instance_type == \"ml.p2.16xlarge\":\n",
    "    processes_per_host = 16\n",
    "else:\n",
    "    processes_per_host = 4\n",
    "\n",
    "print(\"processes_per_host is set to:\", processes_per_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d3f648f6-7a86-4d18-a087-37401bdd2188",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-chatglm-deepspeed-2023-05-0-2023-05-07-02-22-03-330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-07 02:22:29 Starting - Starting the training job......\n",
      "2023-05-07 02:23:11 Starting - Preparing the instances for training.........\n",
      "2023-05-07 02:25:00 Downloading - Downloading input data\n",
      "2023-05-07 02:25:00 Training - Downloading the training image..................\n",
      "2023-05-07 02:27:46 Training - Training image download completed. Training in progress.......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-05-07 02:28:44,993 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-05-07 02:28:45,060 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-07 02:28:45,069 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-05-07 02:28:45,071 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-05-07 02:28:48,909 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (3.20.2)\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.27.1\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.7/6.7 MB 24.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting cpm_kernels\u001b[0m\n",
      "\u001b[34mDownloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 416.6/416.6 kB 35.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mCollecting gradio\u001b[0m\n",
      "\u001b[34mDownloading gradio-3.28.3-py3-none-any.whl (17.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 71.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting mdtex2html\u001b[0m\n",
      "\u001b[34mDownloading mdtex2html-1.2.0-py3-none-any.whl (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (0.1.97)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (0.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (2.9.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface\u001b[0m\n",
      "\u001b[34mDownloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting jieba\u001b[0m\n",
      "\u001b[34mDownloading jieba-0.42.1.tar.gz (19.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/19.2 MB 83.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting rouge_chinese\u001b[0m\n",
      "\u001b[34mDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\u001b[0m\n",
      "\u001b[34mCollecting nltk\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 43.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting deepspeed==0.8.0\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.8.0.tar.gz (749 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 749.9/749.9 kB 81.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (2022.10.31)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (0.13.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (3.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.0->-r requirements.txt (line 14)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.0->-r requirements.txt (line 14)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.0->-r requirements.txt (line 14)) (5.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.0->-r requirements.txt (line 14)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.0->-r requirements.txt (line 14)) (1.10.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.10->-r requirements.txt (line 4)) (4.4.0)\u001b[0m\n",
      "\u001b[34mCollecting mdit-py-plugins<=0.3.3\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.5/50.5 kB 10.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (3.1.2)\u001b[0m\n",
      "\u001b[34mCollecting altair>=4.2.0\u001b[0m\n",
      "\u001b[34mDownloading altair-4.2.2-py3-none-any.whl (813 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 813.6/813.6 kB 85.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markupsafe in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (2.1.2)\u001b[0m\n",
      "\u001b[34mCollecting pydub\u001b[0m\n",
      "\u001b[34mDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.11.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 38.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (1.5.3)\u001b[0m\n",
      "\u001b[34mCollecting markdown-it-py[linkify]>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.5/84.5 kB 27.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments>=2.12.0 in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (2.14.0)\u001b[0m\n",
      "\u001b[34mCollecting websockets>=10.0\u001b[0m\n",
      "\u001b[34mDownloading websockets-11.0.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.7/129.7 kB 43.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting httpx\u001b[0m\n",
      "\u001b[34mDownloading httpx-0.24.0-py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.3/75.3 kB 24.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting gradio-client>=0.1.3\u001b[0m\n",
      "\u001b[34mDownloading gradio_client-0.2.0-py3-none-any.whl (287 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 287.9/287.9 kB 57.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiofiles\u001b[0m\n",
      "\u001b[34mDownloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (3.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (3.8.4)\u001b[0m\n",
      "\u001b[34mCollecting python-multipart\u001b[0m\n",
      "\u001b[34mDownloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.7/45.7 kB 9.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting orjson\u001b[0m\n",
      "\u001b[34mDownloading orjson-3.8.11-cp39-cp39-manylinux_2_28_x86_64.whl (135 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.6/135.6 kB 28.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting uvicorn\u001b[0m\n",
      "\u001b[34mDownloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 18.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting ffmpy\u001b[0m\n",
      "\u001b[34mDownloading ffmpy-0.3.0.tar.gz (4.8 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (9.4.0)\u001b[0m\n",
      "\u001b[34mCollecting fastapi\u001b[0m\n",
      "\u001b[34mDownloading fastapi-0.95.1-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.0/57.0 kB 17.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting semantic-version\u001b[0m\n",
      "\u001b[34mDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown in /opt/conda/lib/python3.9/site-packages (from mdtex2html->-r requirements.txt (line 6)) (3.4.1)\u001b[0m\n",
      "\u001b[34mCollecting latex2mathml\u001b[0m\n",
      "\u001b[34mDownloading latex2mathml-3.75.5-py3-none-any.whl (73 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.3/73.3 kB 23.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (11.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (2023.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from rouge_chinese->-r requirements.txt (line 12)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 13)) (8.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 13)) (1.2.0)\u001b[0m\n",
      "\u001b[34mCollecting entrypoints\u001b[0m\n",
      "\u001b[34mDownloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: toolz in /opt/conda/lib/python3.9/site-packages (from altair>=4.2.0->gradio->-r requirements.txt (line 5)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.9/site-packages (from altair>=4.2.0->gradio->-r requirements.txt (line 5)) (4.17.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (1.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (6.0.4)\u001b[0m\n",
      "\u001b[34mCollecting mdurl~=0.1\u001b[0m\n",
      "\u001b[34mDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting linkify-it-py<3,>=1\u001b[0m\n",
      "\u001b[34mDownloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->gradio->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->gradio->-r requirements.txt (line 5)) (2022.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 2)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 2)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 2)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mCollecting starlette<0.27.0,>=0.26.1\u001b[0m\n",
      "\u001b[34mDownloading starlette-0.26.1-py3-none-any.whl (66 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.9/66.9 kB 23.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting sniffio\u001b[0m\n",
      "\u001b[34mDownloading sniffio-1.3.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting httpcore<0.18.0,>=0.15.0\u001b[0m\n",
      "\u001b[34mDownloading httpcore-0.17.0-py3-none-any.whl (70 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 70.6/70.6 kB 22.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown->mdtex2html->-r requirements.txt (line 6)) (4.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 5)) (0.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 5)) (1.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 5)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 5)) (1.0.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 5)) (4.38.0)\u001b[0m\n",
      "\u001b[34mCollecting h11>=0.8\u001b[0m\n",
      "\u001b[34mDownloading h11-0.14.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 18.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting anyio<5.0,>=3.0\u001b[0m\n",
      "\u001b[34mDownloading anyio-3.6.2-py3-none-any.whl (80 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.6/80.6 kB 23.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown->mdtex2html->-r requirements.txt (line 6)) (3.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio->-r requirements.txt (line 5)) (0.19.3)\u001b[0m\n",
      "\u001b[34mCollecting uc-micro-py\u001b[0m\n",
      "\u001b[34mDownloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed, jieba, ffmpy\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.8.0-py3-none-any.whl size=752131 sha256=bfb2c8f365fa9a3251a8ac556b5160d3bb5f564f06dc75704b420012143b7bff\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/ba/27/48/edac683ddf3d54d4a619ca460375bd226bf1db4a627d681b7c\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=75446dfbf1a9a98a0fa393129feb382021f290edde747deb07bbef1ba693d0a9\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/7d/74/cf/08c94db4b784e2c1ef675a600b7b5b281fd25240dcb954ee7e\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4693 sha256=4c6a5fbecf96fdbb1720ceaafdfd4d2178c2faa2d0f4004913a8a0014906c9cf\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/91/e2/96/f676aa08bfd789328c6576cd0f1fde4a3d686703bb0c247697\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed jieba ffmpy\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pydub, jieba, huggingface, ffmpy, cpm_kernels, websockets, uc-micro-py, sniffio, semantic-version, rouge_chinese, python-multipart, orjson, nltk, mdurl, latex2mathml, h11, entrypoints, aiofiles, uvicorn, markdown-it-py, linkify-it-py, huggingface-hub, deepspeed, anyio, transformers, starlette, mdtex2html, mdit-py-plugins, httpcore, altair, httpx, fastapi, gradio-client, gradio\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.12.0\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.12.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.12.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+06f2048\u001b[0m\n",
      "\u001b[34mUninstalling deepspeed-0.6.1+06f2048:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled deepspeed-0.6.1+06f2048\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.26.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.26.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.26.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiofiles-23.1.0 altair-4.2.2 anyio-3.6.2 cpm_kernels-1.0.11 deepspeed-0.8.0 entrypoints-0.4 fastapi-0.95.1 ffmpy-0.3.0 gradio-3.28.3 gradio-client-0.2.0 h11-0.14.0 httpcore-0.17.0 httpx-0.24.0 huggingface-0.0.1 huggingface-hub-0.14.1 jieba-0.42.1 latex2mathml-3.75.5 linkify-it-py-2.0.2 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 mdtex2html-1.2.0 mdurl-0.1.2 nltk-3.8.1 orjson-3.8.11 pydub-0.25.1 python-multipart-0.0.6 rouge_chinese-1.0.3 semantic-version-2.10.0 sniffio-1.3.0 starlette-0.26.1 transformers-4.27.1 uc-micro-py-1.0.2 uvicorn-0.22.0 websockets-11.0.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.1.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-05-07 02:29:14,772 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-05-07 02:29:14,772 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-05-07 02:29:14,840 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-07 02:29:14,913 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-07 02:29:14,985 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-07 02:29:14,995 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"AdvertiseGen\": \"/opt/ml/input/data/AdvertiseGen\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"AdvertiseGen\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-chatglm-deepspeed-2023-05-0-2023-05-07-02-22-03-330\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-deepspeed-2023-05-0-2023-05-07-02-22-03-330/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"start-single-node\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"start-single-node.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=start-single-node.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"AdvertiseGen\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"AdvertiseGen\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=start-single-node\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-deepspeed-2023-05-0-2023-05-07-02-22-03-330/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"AdvertiseGen\":\"/opt/ml/input/data/AdvertiseGen\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"AdvertiseGen\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-chatglm-deepspeed-2023-05-0-2023-05-07-02-22-03-330\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-deepspeed-2023-05-0-2023-05-07-02-22-03-330/source/sourcedir.tar.gz\",\"module_name\":\"start-single-node\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"start-single-node.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_ADVERTISEGEN=/opt/ml/input/data/AdvertiseGen\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 start-single-node.py\u001b[0m\n",
      "\u001b[34m[2023-05-07 02:29:18.924: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.27.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-05-07 02:29:18,928 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-05-07 02:29:18,948 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m[2023-05-07 02:29:20,921] [WARNING] [runner.py:186:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\u001b[0m\n",
      "\u001b[34m[2023-05-07 02:29:20,977] [INFO] [runner.py:548:main] cmd = /opt/conda/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=41699 --enable_each_rank_log=None main.py --deepspeed deepspeed.json --do_train --train_file /opt/ml/input/data/AdvertiseGen/train.json --test_file /opt/ml/input/data/AdvertiseGen/dev.json --prompt_column content --response_column summary --overwrite_cache --model_name_or_path THUDM/chatglm-6b --output_dir /tmp/model/adgen-chatglm-6b-ft --model_output_s3_path s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed/ --overwrite_output_dir --max_source_length 64 --max_target_length 64 --per_device_train_batch_size 1 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --predict_with_generate --max_steps 50 --logging_steps 10 --save_steps 50 --learning_rate 1e-4 --fp16\u001b[0m\n",
      "\u001b[34m[2023-05-07 02:29:22,934] [INFO] [launch.py:135:main] 0 NCCL_VERSION=2.14.3\u001b[0m\n",
      "\u001b[34m[2023-05-07 02:29:22,935] [INFO] [launch.py:135:main] 0 NCCL_SOCKET_IFNAME=eth0\u001b[0m\n",
      "\u001b[34m[2023-05-07 02:29:22,935] [INFO] [launch.py:135:main] 0 NCCL_DEBUG=WARN\u001b[0m\n",
      "\u001b[34m[2023-05-07 02:29:22,935] [INFO] [launch.py:135:main] 0 NCCL_IB_DISABLE=1\u001b[0m\n",
      "\u001b[34m[2023-05-07 02:29:22,935] [INFO] [launch.py:142:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\u001b[0m\n",
      "\u001b[34m[2023-05-07 02:29:22,935] [INFO] [launch.py:148:main] nnodes=1, num_local_procs=8, node_rank=0\u001b[0m\n",
      "\u001b[34m[2023-05-07 02:29:22,935] [INFO] [launch.py:161:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\u001b[0m\n",
      "\u001b[34m[2023-05-07 02:29:22,935] [INFO] [launch.py:162:main] dist_world_size=8\u001b[0m\n",
      "\u001b[34m[2023-05-07 02:29:22,935] [INFO] [launch.py:164:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\u001b[0m\n",
      "\u001b[34m[2023-05-07 02:29:27,427] [INFO] [comm.py:657:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m05/07/2023 02:29:28 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/07/2023 02:29:28 - WARNING - __main__ - Process rank: 6, device: cuda:6, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/07/2023 02:29:28 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/07/2023 02:29:28 - WARNING - __main__ - Process rank: 7, device: cuda:7, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/07/2023 02:29:28 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/07/2023 02:29:28 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/07/2023 02:29:28 - WARNING - __main__ - Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/07/2023 02:29:28 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/07/2023 02:29:28 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=False,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=deepspeed.json,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdo_eval=False,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=no,\u001b[0m\n",
      "\u001b[34mfp16=True,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgeneration_max_length=None,\u001b[0m\n",
      "\u001b[34mgeneration_num_beams=None,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=1,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=False,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=0.0001,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=0,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/tmp/model/adgen-chatglm-6b-ft/runs/May07_02-29-27_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=10,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=linear,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=50,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=3.0,\u001b[0m\n",
      "\u001b[34moptim=adamw_hf,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moutput_dir=/tmp/model/adgen-chatglm-6b-ft,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=True,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=4,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=1,\u001b[0m\n",
      "\u001b[34mpredict_with_generate=True,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/tmp/model/adgen-chatglm-6b-ft,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_steps=50,\u001b[0m\n",
      "\u001b[34msave_strategy=steps,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=42,\u001b[0m\n",
      "\u001b[34msharded_ddp=[],\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34msortish_sampler=False,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,\u001b[0m\n",
      "\u001b[34mxpu_backend=None,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m05/07/2023 02:29:28 - WARNING - datasets.builder - Using custom data configuration default-b9d04254ee42c6ff\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-b9d04254ee42c6ff/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 2/2 [00:00<00:00, 12104.77it/s]\u001b[0m\n",
      "\u001b[34m05/07/2023 02:29:28 - WARNING - datasets.builder - Using custom data configuration default-b9d04254ee42c6ff\u001b[0m\n",
      "\u001b[34m05/07/2023 02:29:28 - WARNING - datasets.builder - Using custom data configuration default-b9d04254ee42c6ff\u001b[0m\n",
      "\u001b[34m05/07/2023 02:29:28 - WARNING - datasets.builder - Using custom data configuration default-b9d04254ee42c6ff\u001b[0m\n",
      "\u001b[34m05/07/2023 02:29:28 - WARNING - datasets.builder - Using custom data configuration default-b9d04254ee42c6ff\u001b[0m\n",
      "\u001b[34m05/07/2023 02:29:28 - WARNING - datasets.builder - Using custom data configuration default-b9d04254ee42c6ff\u001b[0m\n",
      "\u001b[34m05/07/2023 02:29:28 - WARNING - datasets.builder - Using custom data configuration default-b9d04254ee42c6ff\u001b[0m\n",
      "\u001b[34m05/07/2023 02:29:28 - WARNING - datasets.builder - Using custom data configuration default-b9d04254ee42c6ff\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 2/2 [00:00<00:00, 1798.97it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 111750 examples [00:00, 898681.20 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating test split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-b9d04254ee42c6ff/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 541.38it/s]\u001b[0m\n",
      "\u001b[34m05/07/2023 02:29:29 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b9d04254ee42c6ff/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m05/07/2023 02:29:29 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b9d04254ee42c6ff/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 512.38it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 547.70it/s]\u001b[0m\n",
      "\u001b[34m05/07/2023 02:29:29 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b9d04254ee42c6ff/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m05/07/2023 02:29:29 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b9d04254ee42c6ff/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 506.37it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 531.93it/s]\u001b[0m\n",
      "\u001b[34m05/07/2023 02:29:29 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b9d04254ee42c6ff/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m05/07/2023 02:29:29 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b9d04254ee42c6ff/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 559.88it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 466.11it/s]\u001b[0m\n",
      "\u001b[34m05/07/2023 02:29:29 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b9d04254ee42c6ff/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 499.98it/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/773 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 773/773 [00:00<00:00, 132kB/s]\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:905] 2023-05-07 02:29:29,414 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:905] 2023-05-07 02:29:29,414 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:668] 2023-05-07 02:29:29,415 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/658202d88ac4bb782b99e99ac3adff58b4d0b813/config.json\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:905] 2023-05-07 02:29:29,416 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:668] 2023-05-07 02:29:29,415 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/658202d88ac4bb782b99e99ac3adff58b4d0b813/config.json\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:905] 2023-05-07 02:29:29,416 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:905] 2023-05-07 02:29:29,416 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:905] 2023-05-07 02:29:29,416 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:905] 2023-05-07 02:29:29,426 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:905] 2023-05-07 02:29:29,426 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:905] 2023-05-07 02:29:29,452 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:905] 2023-05-07 02:29:29,452 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:905] 2023-05-07 02:29:29,453 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:905] 2023-05-07 02:29:29,453 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:905] 2023-05-07 02:29:29,455 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:905] 2023-05-07 02:29:29,455 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:905] 2023-05-07 02:29:29,457 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:905] 2023-05-07 02:29:29,457 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mDownloading (…)iguration_chatglm.py:   0%|          | 0.00/4.28k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)iguration_chatglm.py: 100%|██████████| 4.28k/4.28k [00:00<00:00, 706kB/s]\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\u001b[0m\n",
      "\u001b[34mFileNotFoundError: [Errno 2] No such file or directory: '/root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b/658202d88ac4bb782b99e99ac3adff58b4d0b813/configuration_chatglm.py'\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:668] 2023-05-07 02:29:29,868 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/658202d88ac4bb782b99e99ac3adff58b4d0b813/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:668] 2023-05-07 02:29:29,868 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/658202d88ac4bb782b99e99ac3adff58b4d0b813/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:720] 2023-05-07 02:29:29,869 >> Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm-6b\",\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\"\n",
      "  },\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"gmask_token_id\": 130001,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"inner_hidden_size\": 16384,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"mask_token_id\": 130000,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_encoding_2d\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 130528\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:720] 2023-05-07 02:29:29,869 >> Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm-6b\",\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\"\n",
      "  },\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"gmask_token_id\": 130001,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"inner_hidden_size\": 16384,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"mask_token_id\": 130000,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_encoding_2d\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 130528\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json:   0%|          | 0.00/441 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json: 100%|██████████| 441/441 [00:00<00:00, 78.6kB/s]\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:652] 2023-05-07 02:29:30,066 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:652] 2023-05-07 02:29:30,066 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:652] 2023-05-07 02:29:30,093 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:652] 2023-05-07 02:29:30,093 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:652] 2023-05-07 02:29:30,108 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:652] 2023-05-07 02:29:30,108 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:652] 2023-05-07 02:29:30,110 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:652] 2023-05-07 02:29:30,110 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:652] 2023-05-07 02:29:30,110 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:652] 2023-05-07 02:29:30,110 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:652] 2023-05-07 02:29:30,112 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:652] 2023-05-07 02:29:30,112 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:652] 2023-05-07 02:29:30,114 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:652] 2023-05-07 02:29:30,114 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:652] 2023-05-07 02:29:30,116 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:652] 2023-05-07 02:29:30,116 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mDownloading (…)enization_chatglm.py:   0%|          | 0.00/16.7k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)enization_chatglm.py: 100%|██████████| 16.7k/16.7k [00:00<00:00, 3.00MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ice_text.model:   0%|          | 0.00/2.71M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ice_text.model: 100%|██████████| 2.71M/2.71M [00:00<00:00, 59.1MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-05-07 02:29:30,877 >> loading file ice_text.model from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/658202d88ac4bb782b99e99ac3adff58b4d0b813/ice_text.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-05-07 02:29:30,877 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-05-07 02:29:30,877 >> loading file special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-05-07 02:29:30,877 >> loading file ice_text.model from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/658202d88ac4bb782b99e99ac3adff58b4d0b813/ice_text.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-05-07 02:29:30,877 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-05-07 02:29:30,877 >> loading file special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-05-07 02:29:30,877 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/658202d88ac4bb782b99e99ac3adff58b4d0b813/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-05-07 02:29:30,877 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/658202d88ac4bb782b99e99ac3adff58b4d0b813/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-07 02:29:31,054 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-07 02:29:31,054 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-07 02:29:31,056 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-07 02:29:31,056 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-07 02:29:31,075 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-07 02:29:31,075 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-07 02:29:31,108 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-07 02:29:31,108 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-07 02:29:31,112 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-07 02:29:31,112 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-07 02:29:31,115 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-07 02:29:31,115 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-07 02:29:31,123 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-07 02:29:31,123 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-07 02:29:31,136 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-07 02:29:31,136 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mDownloading (…)/modeling_chatglm.py:   0%|          | 0.00/57.6k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/modeling_chatglm.py: 100%|██████████| 57.6k/57.6k [00:00<00:00, 9.67MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)main/quantization.py:   0%|          | 0.00/15.1k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)main/quantization.py: 100%|██████████| 15.1k/15.1k [00:00<00:00, 2.01MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)model.bin.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m╭───────────────────── Traceback (most recent call last) ──────────────────────╮\u001b[0m\n",
      "\u001b[34m│ /opt/ml/code/main.py:446 in <module>                                         │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   443                                                                        │\u001b[0m\n",
      "\u001b[34m│   444                                                                        │\u001b[0m\n",
      "\u001b[34m│   445 if __name__ == \"__main__\":                                             │\u001b[0m\n",
      "\u001b[34m│ ❱ 446 │   main()                                                             │\u001b[0m\n",
      "\u001b[34m│   447                                                                        │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/ml/code/main.py:126 in main                                             │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   123 │   │   │   │   new_prefix_state_dict[k[len(\"transformer.prefix_encode │\u001b[0m\n",
      "\u001b[34m│   124 │   │   model.transformer.prefix_encoder.load_state_dict(new_prefix_st │\u001b[0m\n",
      "\u001b[34m│   125 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m│ ❱ 126 │   │   model = AutoModel.from_pretrained(model_args.model_name_or_pat │\u001b[0m\n",
      "\u001b[34m│   127 │                                                                      │\u001b[0m\n",
      "\u001b[34m│   128 │   if model_args.quantization_bit is not None:                        │\u001b[0m\n",
      "\u001b[34m│   129 │   │   print(f\"Quantized to {model_args.quantization_bit} bit\")       │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.9/site-packages/transformers/models/auto/auto_factory │\u001b[0m\n",
      "\u001b[34m│ .py:462 in from_pretrained                                                   │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   459 │   │   │   │   )                                                      │\u001b[0m\n",
      "\u001b[34m│   460 │   │   │   class_ref = config.auto_map[cls.__name__]                  │\u001b[0m\n",
      "\u001b[34m│   461 │   │   │   module_file, class_name = class_ref.split(\".\")             │\u001b[0m\n",
      "\u001b[34m│ ❱ 462 │   │   │   model_class = get_class_from_dynamic_module(               │\u001b[0m\n",
      "\u001b[34m│   463 │   │   │   │   pretrained_model_name_or_path, module_file + \".py\", cl │\u001b[0m\n",
      "\u001b[34m│   464 │   │   │   )                                                          │\u001b[0m\n",
      "\u001b[34m│   465 │   │   │   model_class.register_for_auto_class(cls.__name__)          │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.9/site-packages/transformers/dynamic_module_utils.py: │\u001b[0m\n",
      "\u001b[34m│ 399 in get_class_from_dynamic_module                                         │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   396 │   │   revision=revision,                                             │\u001b[0m\n",
      "\u001b[34m│   397 │   │   local_files_only=local_files_only,                             │\u001b[0m\n",
      "\u001b[34m│   398 │   )                                                                  │\u001b[0m\n",
      "\u001b[34m│ ❱ 399 │   return get_class_in_module(class_name, final_module.replace(\".py\", │\u001b[0m\n",
      "\u001b[34m│   400                                                                        │\u001b[0m\n",
      "\u001b[34m│   401                                                                        │\u001b[0m\n",
      "\u001b[34m│   402 def custom_object_save(obj, folder, config=None):                      │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.9/site-packages/transformers/dynamic_module_utils.py: │\u001b[0m\n",
      "\u001b[34m│ 157 in get_class_in_module                                                   │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   154 │   │                                                                  │\u001b[0m\n",
      "\u001b[34m│   155 │   │   # Copy to a temporary directory. We need to do this in another │\u001b[0m\n",
      "\u001b[34m│   156 │   │   # `ModuleNotFoundError: No module named 'transformers_modules. │\u001b[0m\n",
      "\u001b[34m│ ❱ 157 │   │   shutil.copy(f\"{module_dir}/{module_file_name}\", tmp_dir)       │\u001b[0m\n",
      "\u001b[34m│   158 │   │   # On Windows, we need this character `r` before the path argum │\u001b[0m\n",
      "\u001b[34m│   159 │   │   cmd = f'import os; os.remove(r\"{module_dir}{os.path.sep}{modul │\u001b[0m\n",
      "\u001b[34m│   160 │   │   # We don't know which python binary file exists in an environm │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.9/shutil.py:427 in copy                               │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│    424 │   \"\"\"                                                               │\u001b[0m\n",
      "\u001b[34m│    425 │   if os.path.isdir(dst):                                            │\u001b[0m\n",
      "\u001b[34m│    426 │   │   dst = os.path.join(dst, os.path.basename(src))                │\u001b[0m\n",
      "\u001b[34m│ ❱  427 │   copyfile(src, dst, follow_symlinks=follow_symlinks)               │\u001b[0m\n",
      "\u001b[34m│    428 │   copymode(src, dst, follow_symlinks=follow_symlinks)               │\u001b[0m\n",
      "\u001b[34m│    429 │   return dst                                                        │\u001b[0m\n",
      "\u001b[34m│    430                                                                       │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.9/shutil.py:264 in copyfile                           │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│    261 │   if not follow_symlinks and _islink(src):                          │\u001b[0m\n",
      "\u001b[34m│    262 │   │   os.symlink(os.readlink(src), dst)                             │\u001b[0m\n",
      "\u001b[34m│    263 │   else:                                                             │\u001b[0m\n",
      "\u001b[34m│ ❱  264 │   │   with open(src, 'rb') as fsrc:                                 │\u001b[0m\n",
      "\u001b[34m│    265 │   │   │   try:                                                      │\u001b[0m\n",
      "\u001b[34m│    266 │   │   │   │   with open(dst, 'wb') as fdst:                         │\u001b[0m\n",
      "\u001b[34m│    267 │   │   │   │   │   # macOS                                           │\u001b[0m\n",
      "\u001b[34m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\u001b[34mFileNotFoundError: [Errno 2] No such file or directory: \u001b[0m\n",
      "\u001b[34m'/root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b/658202d8\u001b[0m\n",
      "\u001b[34m8ac4bb782b99e99ac3adff58b4d0b813/modeling_chatglm.py'\u001b[0m\n",
      "\u001b[34mDownloading (…)model.bin.index.json: 100%|██████████| 33.4k/33.4k [00:00<00:00, 458kB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2403] 2023-05-07 02:29:32,310 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/658202d88ac4bb782b99e99ac3adff58b4d0b813/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2403] 2023-05-07 02:29:32,310 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/658202d88ac4bb782b99e99ac3adff58b4d0b813/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m╭───────────────────── Traceback (most recent call last) ──────────────────────╮\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.9/site-packages/torch/_sources.py:23 in               │\u001b[0m\n",
      "\u001b[34m│ get_source_lines_and_file                                                    │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│    20 │   filename = None  # in case getsourcefile throws                    │\u001b[0m\n",
      "\u001b[34m│    21 │   try:                                                               │\u001b[0m\n",
      "\u001b[34m│    22 │   │   filename = inspect.getsourcefile(obj)                          │\u001b[0m\n",
      "\u001b[34m│ ❱  23 │   │   sourcelines, file_lineno = inspect.getsourcelines(obj)         │\u001b[0m\n",
      "\u001b[34m│    24 │   except OSError as e:                                               │\u001b[0m\n",
      "\u001b[34m│    25 │   │   msg = (                                                        │\u001b[0m\n",
      "\u001b[34m│    26 │   │   │   f\"Can't get source for {obj}. TorchScript requires source  │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.9/inspect.py:1006 in getsourcelines                   │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   1003 │   original source file the first line of code was found.  An OSErro │\u001b[0m\n",
      "\u001b[34m│   1004 │   raised if the source code cannot be retrieved.\"\"\"                 │\u001b[0m\n",
      "\u001b[34m│   1005 │   object = unwrap(object)                                           │\u001b[0m\n",
      "\u001b[34m│ ❱ 1006 │   lines, lnum = findsource(object)                                  │\u001b[0m\n",
      "\u001b[34m│   1007 │                                                                     │\u001b[0m\n",
      "\u001b[34m│   1008 │   if istraceback(object):                                           │\u001b[0m\n",
      "\u001b[34m│   1009 │   │   object = object.tb_frame                                      │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.9/inspect.py:835 in findsource                        │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│    832 │   else:                                                             │\u001b[0m\n",
      "\u001b[34m│    833 │   │   lines = linecache.getlines(file)                              │\u001b[0m\n",
      "\u001b[34m│    834 │   if not lines:                                                     │\u001b[0m\n",
      "\u001b[34m│ ❱  835 │   │   raise OSError('could not get source code')                    │\u001b[0m\n",
      "\u001b[34m│    836 │                                                                     │\u001b[0m\n",
      "\u001b[34m│    837 │   if ismodule(object):                                              │\u001b[0m\n",
      "\u001b[34m│    838 │   │   return lines, 0                                               │\u001b[0m\n",
      "\u001b[34m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\u001b[34mOSError: could not get source code\u001b[0m\n",
      "\u001b[34mThe above exception was the direct cause of the following exception:\u001b[0m\n",
      "\u001b[34m╭───────────────────── Traceback (most recent call last) ──────────────────────╮\u001b[0m\n",
      "\u001b[34m│ /opt/ml/code/main.py:446 in <module>                                         │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   443                                                                        │\u001b[0m\n",
      "\u001b[34m│   444                                                                        │\u001b[0m\n",
      "\u001b[34m│   445 if __name__ == \"__main__\":                                             │\u001b[0m\n",
      "\u001b[34m│ ❱ 446 │   main()                                                             │\u001b[0m\n",
      "\u001b[34m│   447                                                                        │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/ml/code/main.py:126 in main                                             │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   123 │   │   │   │   new_prefix_state_dict[k[len(\"transformer.prefix_encode │\u001b[0m\n",
      "\u001b[34m│   124 │   │   model.transformer.prefix_encoder.load_state_dict(new_prefix_st │\u001b[0m\n",
      "\u001b[34m│   125 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m│ ❱ 126 │   │   model = AutoModel.from_pretrained(model_args.model_name_or_pat │\u001b[0m\n",
      "\u001b[34m│   127 │                                                                      │\u001b[0m\n",
      "\u001b[34m│   128 │   if model_args.quantization_bit is not None:                        │\u001b[0m\n",
      "\u001b[34m│   129 │   │   print(f\"Quantized to {model_args.quantization_bit} bit\")       │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.9/site-packages/transformers/models/auto/auto_factory │\u001b[0m\n",
      "\u001b[34m│ .py:462 in from_pretrained                                                   │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   459 │   │   │   │   )                                                      │\u001b[0m\n",
      "\u001b[34m│   460 │   │   │   class_ref = config.auto_map[cls.__name__]                  │\u001b[0m\n",
      "\u001b[34m│   461 │   │   │   module_file, class_name = class_ref.split(\".\")             │\u001b[0m\n",
      "\u001b[34m│ ❱ 462 │   │   │   model_class = get_class_from_dynamic_module(               │\u001b[0m\n",
      "\u001b[34m│   463 │   │   │   │   pretrained_model_name_or_path, module_file + \".py\", cl │\u001b[0m\n",
      "\u001b[34m│   464 │   │   │   )                                                          │\u001b[0m\n",
      "\u001b[34m│   465 │   │   │   model_class.register_for_auto_class(cls.__name__)          │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.9/site-packages/transformers/dynamic_module_utils.py: │\u001b[0m\n",
      "\u001b[34m│ 399 in get_class_from_dynamic_module                                         │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   396 │   │   revision=revision,                                             │\u001b[0m\n",
      "\u001b[34m│   397 │   │   local_files_only=local_files_only,                             │\u001b[0m\n",
      "\u001b[34m│   398 │   )                                                                  │\u001b[0m\n",
      "\u001b[34m│ ❱ 399 │   return get_class_in_module(class_name, final_module.replace(\".py\", │\u001b[0m\n",
      "\u001b[34m│   400                                                                        │\u001b[0m\n",
      "\u001b[34m│   401                                                                        │\u001b[0m\n",
      "\u001b[34m│   402 def custom_object_save(obj, folder, config=None):                      │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.9/site-packages/transformers/dynamic_module_utils.py: │\u001b[0m\n",
      "\u001b[34m│ 177 in get_class_in_module                                                   │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   174 │   │                                                                  │\u001b[0m\n",
      "\u001b[34m│   175 │   │   # import the module                                            │\u001b[0m\n",
      "\u001b[34m│   176 │   │   module_path = module_path.replace(os.path.sep, \".\")            │\u001b[0m\n",
      "\u001b[34m│ ❱ 177 │   │   module = importlib.import_module(module_path)                  │\u001b[0m\n",
      "\u001b[34m│   178 │   │                                                                  │\u001b[0m\n",
      "\u001b[34m│   179 │   │   return getattr(module, class_name)                             │\u001b[0m\n",
      "\u001b[34m│   180                                                                        │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.9/importlib/__init__.py:127 in import_module          │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   124 │   │   │   if character != '.':                                       │\u001b[0m\n",
      "\u001b[34m│   125 │   │   │   │   break                                                  │\u001b[0m\n",
      "\u001b[34m│   126 │   │   │   level += 1                                                 │\u001b[0m\n",
      "\u001b[34m│ ❱ 127 │   return _bootstrap._gcd_import(name[level:], package, level)        │\u001b[0m\n",
      "\u001b[34m│   128                                                                        │\u001b[0m\n",
      "\u001b[34m│   129                                                                        │\u001b[0m\n",
      "\u001b[34m│   130 _RELOADING = {}                                                        │\u001b[0m\n",
      "\u001b[34m│ <frozen importlib._bootstrap>:1030 in _gcd_import                            │\u001b[0m\n",
      "\u001b[34m│ <frozen importlib._bootstrap>:1007 in _find_and_load                         │\u001b[0m\n",
      "\u001b[34m│ <frozen importlib._bootstrap>:986 in _find_and_load_unlocked                 │\u001b[0m\n",
      "\u001b[34m│ <frozen importlib._bootstrap>:680 in _load_unlocked                          │\u001b[0m\n",
      "\u001b[34m│ <frozen importlib._bootstrap_external>:850 in exec_module                    │\u001b[0m\n",
      "\u001b[34m│ <frozen importlib._bootstrap>:228 in _call_with_frames_removed               │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b/65820 │\u001b[0m\n",
      "\u001b[34m│ 2d88ac4bb782b99e99ac3adff58b4d0b813/modeling_chatglm.py:234 in <module>      │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│    231                                                                       │\u001b[0m\n",
      "\u001b[34m│    232                                                                       │\u001b[0m\n",
      "\u001b[34m│    233 @torch.jit.script                                                     │\u001b[0m\n",
      "\u001b[34m│ ❱  234 def apply_rotary_pos_emb_index(q, k, cos, sin, position_id):          │\u001b[0m\n",
      "\u001b[34m│    235 │   # position_id: [sq, b], q, k: [sq, b, np, hn], cos: [sq, 1, hn] - │\u001b[0m\n",
      "\u001b[34m│    236 │   cos, sin = F.embedding(position_id, cos.squeeze(1)).unsqueeze(2), │\u001b[0m\n",
      "\u001b[34m│    237 │   │   F.embedding(position_id, sin.squeeze(1)).unsqueeze(2)         │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.9/site-packages/torch/jit/_script.py:1340 in script   │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   1337 │   │   maybe_already_compiled_fn = _try_get_jit_cached_function(obj) │\u001b[0m\n",
      "\u001b[34m│   1338 │   │   if maybe_already_compiled_fn:                                 │\u001b[0m\n",
      "\u001b[34m│   1339 │   │   │   return maybe_already_compiled_fn                          │\u001b[0m\n",
      "\u001b[34m│ ❱ 1340 │   │   ast = get_jit_def(obj, obj.__name__)                          │\u001b[0m\n",
      "\u001b[34m│   1341 │   │   if _rcb is None:                                              │\u001b[0m\n",
      "\u001b[34m│   1342 │   │   │   _rcb = _jit_internal.createResolutionCallbackFromClosure( │\u001b[0m\n",
      "\u001b[34m│   1343 │   │   fn = torch._C._jit_script_compile(                            │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.9/site-packages/torch/jit/frontend.py:262 in          │\u001b[0m\n",
      "\u001b[34m│ get_jit_def                                                                  │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│    259 │   │   │   but we want the result AST to have the name \"forward\".    │\u001b[0m\n",
      "\u001b[34m│    260 │   │   self_name: If this function is a method, what the type name o │\u001b[0m\n",
      "\u001b[34m│    261 │   \"\"\"                                                               │\u001b[0m\n",
      "\u001b[34m│ ❱  262 │   parsed_def = parse_def(fn) if not isinstance(fn, _ParsedDef) else │\u001b[0m\n",
      "\u001b[34m│    263 │   type_line = torch.jit.annotations.get_type_line(parsed_def.source │\u001b[0m\n",
      "\u001b[34m│    264 │   fn_def = parsed_def.ast.body[0]                                   │\u001b[0m\n",
      "\u001b[34m│    265                                                                       │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.9/site-packages/torch/_sources.py:122 in parse_def    │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   119                                                                        │\u001b[0m\n",
      "\u001b[34m│   120                                                                        │\u001b[0m\n",
      "\u001b[34m│   121 def parse_def(fn):                                                     │\u001b[0m\n",
      "\u001b[34m│ ❱ 122 │   sourcelines, file_lineno, filename = get_source_lines_and_file(    │\u001b[0m\n",
      "\u001b[34m│   123 │   │   fn, ErrorReport.call_stack()                                   │\u001b[0m\n",
      "\u001b[34m│   124 │   )                                                                  │\u001b[0m\n",
      "\u001b[34m│   125 │   sourcelines = normalize_source_lines(sourcelines)                  │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.9/site-packages/torch/_sources.py:32 in               │\u001b[0m\n",
      "\u001b[34m│ get_source_lines_and_file                                                    │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│    29 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m│    30 │   │   if error_msg:                                                  │\u001b[0m\n",
      "\u001b[34m│    31 │   │   │   msg += \"\\n\" + error_msg                                    │\u001b[0m\n",
      "\u001b[34m│ ❱  32 │   │   raise OSError(msg) from e                                      │\u001b[0m\n",
      "\u001b[34m│    33 │                                                                      │\u001b[0m\n",
      "\u001b[34m│    34 │   return sourcelines, file_lineno, filename                          │\u001b[0m\n",
      "\u001b[34m│    35                                                                        │\u001b[0m\n",
      "\u001b[34m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\u001b[34mOSError: Can't get source for <function apply_rotary_pos_emb_index at \u001b[0m\n",
      "\u001b[34m0x7f3ac8b4c1f0>. TorchScript requires source access in order to carry out \u001b[0m\n",
      "\u001b[34mcompilation, make sure original .py files are available.\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   0%|          | 0.00/1.74G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m╭───────────────────── Traceback (most recent call last) ──────────────────────╮\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.9/site-packages/torch/_sources.py:23 in               │\u001b[0m\n",
      "\u001b[34m│ get_source_lines_and_file                                                    │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│    20 │   filename = None  # in case getsourcefile throws                    │\u001b[0m\n",
      "\u001b[34m│    21 │   try:                                                               │\u001b[0m\n",
      "\u001b[34m│    22 │   │   filename = inspect.getsourcefile(obj)                          │\u001b[0m\n",
      "\u001b[34m│ ❱  23 │   │   sourcelines, file_lineno = inspect.getsourcelines(obj)         │\u001b[0m\n",
      "\u001b[34m│    24 │   except OSError as e:                                               │\u001b[0m\n",
      "\u001b[34m│    25 │   │   msg = (                                                        │\u001b[0m\n",
      "\u001b[34m│    26 │   │   │   f\"Can't get source for {obj}. TorchScript requires source  │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.9/inspect.py:1006 in getsourcelines                   │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   1003 │   original source file the first line of code was found.  An OSErro │\u001b[0m\n",
      "\u001b[34m│   1004 │   raised if the source code cannot be retrieved.\"\"\"                 │\u001b[0m\n",
      "\u001b[34m│   1005 │   object = unwrap(object)                                           │\u001b[0m\n",
      "\u001b[34m│ ❱ 1006 │   lines, lnum = findsource(object)                                  │\u001b[0m\n",
      "\u001b[34m│   1007 │                                                                     │\u001b[0m\n",
      "\u001b[34m│   1008 │   if istraceback(object):                                           │\u001b[0m\n",
      "\u001b[34m│   1009 │   │   object = object.tb_frame                                      │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.9/inspect.py:835 in findsource                        │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│    832 │   else:                                                             │\u001b[0m\n",
      "\u001b[34m│    833 │   │   lines = linecache.getlines(file)                              │\u001b[0m\n",
      "\u001b[34m│    834 │   if not lines:                                                     │\u001b[0m\n",
      "\u001b[34m│ ❱  835 │   │   raise OSError('could not get source code')                    │\u001b[0m\n",
      "\u001b[34m│    836 │                                                                     │\u001b[0m\n",
      "\u001b[34m│    837 │   if ismodule(object):                                              │\u001b[0m\n",
      "\u001b[34m│    838 │   │   return lines, 0                                               │\u001b[0m\n",
      "\u001b[34m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\u001b[34mOSError: could not get source code\u001b[0m\n",
      "\u001b[34mThe above exception was the direct cause of the following exception:\u001b[0m\n",
      "\u001b[34m╭───────────────────── Traceback (most recent call last) ──────────────────────╮\u001b[0m\n",
      "\u001b[34m│ /opt/ml/code/main.py:446 in <module>                                         │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   443                                                                        │\u001b[0m\n",
      "\u001b[34m│   444                                                                        │\u001b[0m\n",
      "\u001b[34m│   445 if __name__ == \"__main__\":                                             │\u001b[0m\n",
      "\u001b[34m│ ❱ 446 │   main()                                                             │\u001b[0m\n",
      "\u001b[34m│   447                                                                        │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/ml/code/main.py:126 in main                                             │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   123 │   │   │   │   new_prefix_state_dict[k[len(\"transformer.prefix_encode │\u001b[0m\n",
      "\u001b[34m│   124 │   │   model.transformer.prefix_encoder.load_state_dict(new_prefix_st │\u001b[0m\n",
      "\u001b[34m│   125 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m│ ❱ 126 │   │   model = AutoModel.from_pretrained(model_args.model_name_or_pat │\u001b[0m\n",
      "\u001b[34m│   127 │                                                                      │\u001b[0m\n",
      "\u001b[34m│   128 │   if model_args.quantization_bit is not None:                        │\u001b[0m\n",
      "\u001b[34m│   129 │   │   print(f\"Quantized to {model_args.quantization_bit} bit\")       │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.9/site-packages/transformers/models/auto/auto_factory │\u001b[0m\n",
      "\u001b[34m│ .py:462 in from_pretrained                                                   │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   459 │   │   │   │   )                                                      │\u001b[0m\n",
      "\u001b[34m│   460 │   │   │   class_ref = config.auto_map[cls.__name__]                  │\u001b[0m\n",
      "\u001b[34m│   461 │   │   │   module_file, class_name = class_ref.split(\".\")             │\u001b[0m\n",
      "\u001b[34m│ ❱ 462 │   │   │   model_class = get_class_from_dynamic_module(               │\u001b[0m\n",
      "\u001b[34m│   463 │   │   │   │   pretrained_model_name_or_path, module_file + \".py\", cl │\u001b[0m\n",
      "\u001b[34m│   464 │   │   │   )                                                          │\u001b[0m\n",
      "\u001b[34m│   465 │   │   │   model_class.register_for_auto_class(cls.__name__)          │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.9/site-packages/transformers/dynamic_module_utils.py: │\u001b[0m\n",
      "\u001b[34m│ 399 in get_class_from_dynamic_module                                         │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   396 │   │   revision=revision,                                             │\u001b[0m\n",
      "\u001b[34m│   397 │   │   local_files_only=local_files_only,                             │\u001b[0m\n",
      "\u001b[34m│   398 │   )                                                                  │\u001b[0m\n",
      "\u001b[34m│ ❱ 399 │   return get_class_in_module(class_name, final_module.replace(\".py\", │\u001b[0m\n",
      "\u001b[34m│   400                                                                        │\u001b[0m\n",
      "\u001b[34m│   401                                                                        │\u001b[0m\n",
      "\u001b[34m│   402 def custom_object_save(obj, folder, config=None):                      │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.9/site-packages/transformers/dynamic_module_utils.py: │\u001b[0m\n",
      "\u001b[34m│ 177 in get_class_in_module                                                   │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   174 │   │                                                                  │\u001b[0m\n",
      "\u001b[34m│   175 │   │   # import the module                                            │\u001b[0m\n",
      "\u001b[34m│   176 │   │   module_path = module_path.replace(os.path.sep, \".\")            │\u001b[0m\n",
      "\u001b[34m│ ❱ 177 │   │   module = importlib.import_module(module_path)                  │\u001b[0m\n",
      "\u001b[34m│   178 │   │                                                                  │\u001b[0m\n",
      "\u001b[34m│   179 │   │   return getattr(module, class_name)                             │\u001b[0m\n",
      "\u001b[34m│   180                                                                        │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.9/importlib/__init__.py:127 in import_module          │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   124 │   │   │   if character != '.':                                       │\u001b[0m\n",
      "\u001b[34m│   125 │   │   │   │   break                                                  │\u001b[0m\n",
      "\u001b[34m│   126 │   │   │   level += 1                                                 │\u001b[0m\n",
      "\u001b[34m│ ❱ 127 │   return _bootstrap._gcd_import(name[level:], package, level)        │\u001b[0m\n",
      "\u001b[34m│   128                                                                        │\u001b[0m\n",
      "\u001b[34m│   129                                                                        │\u001b[0m\n",
      "\u001b[34m│   130 _RELOADING = {}                                                        │\u001b[0m\n",
      "\u001b[34m│ <frozen importlib._bootstrap>:1030 in _gcd_import                            │\u001b[0m\n",
      "\u001b[34m│ <frozen importlib._bootstrap>:1007 in _find_and_load                         │\u001b[0m\n",
      "\u001b[34m│ <frozen importlib._bootstrap>:986 in _find_and_load_unlocked                 │\u001b[0m\n",
      "\u001b[34m│ <frozen importlib._bootstrap>:680 in _load_unlocked                          │\u001b[0m\n",
      "\u001b[34m│ <frozen importlib._bootstrap_external>:850 in exec_module                    │\u001b[0m\n",
      "\u001b[34m│ <frozen importlib._bootstrap>:228 in _call_with_frames_removed               │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b/65820 │\u001b[0m\n",
      "\u001b[34m│ 2d88ac4bb782b99e99ac3adff58b4d0b813/modeling_chatglm.py:234 in <module>      │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│    231                                                                       │\u001b[0m\n",
      "\u001b[34m│    232                                                                       │\u001b[0m\n",
      "\u001b[34m│    233 @torch.jit.script                                                     │\u001b[0m\n",
      "\u001b[34m│ ❱  234 def apply_rotary_pos_emb_index(q, k, cos, sin, position_id):          │\u001b[0m\n",
      "\u001b[34m│    235 │   # position_id: [sq, b], q, k: [sq, b, np, hn], cos: [sq, 1, hn] - │\u001b[0m\n",
      "\u001b[34m│    236 │   cos, sin = F.embedding(position_id, cos.squeeze(1)).unsqueeze(2), │\u001b[0m\n",
      "\u001b[34m│    237 │   │   F.embedding(position_id, sin.squeeze(1)).unsqueeze(2)         │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.9/site-packages/torch/jit/_script.py:1340 in script   │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   1337 │   │   maybe_already_compiled_fn = _try_get_jit_cached_function(obj) │\u001b[0m\n",
      "\u001b[34m│   1338 │   │   if maybe_already_compiled_fn:                                 │\u001b[0m\n",
      "\u001b[34m│   1339 │   │   │   return maybe_already_compiled_fn                          │\u001b[0m\n",
      "\u001b[34m│ ❱ 1340 │   │   ast = get_jit_def(obj, obj.__name__)                          │\u001b[0m\n",
      "\u001b[34m│   1341 │   │   if _rcb is None:                                              │\u001b[0m\n",
      "\u001b[34m│   1342 │   │   │   _rcb = _jit_internal.createResolutionCallbackFromClosure( │\u001b[0m\n",
      "\u001b[34m│   1343 │   │   fn = torch._C._jit_script_compile(                            │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.9/site-packages/torch/jit/frontend.py:262 in          │\u001b[0m\n",
      "\u001b[34m│ get_jit_def                                                                  │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│    259 │   │   │   but we want the result AST to have the name \"forward\".    │\u001b[0m\n",
      "\u001b[34m│    260 │   │   self_name: If this function is a method, what the type name o │\u001b[0m\n",
      "\u001b[34m│    261 │   \"\"\"                                                               │\u001b[0m\n",
      "\u001b[34m│ ❱  262 │   parsed_def = parse_def(fn) if not isinstance(fn, _ParsedDef) else │\u001b[0m\n",
      "\u001b[34m│    263 │   type_line = torch.jit.annotations.get_type_line(parsed_def.source │\u001b[0m\n",
      "\u001b[34m│    264 │   fn_def = parsed_def.ast.body[0]                                   │\u001b[0m\n",
      "\u001b[34m│    265                                                                       │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.9/site-packages/torch/_sources.py:122 in parse_def    │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   119                                                                        │\u001b[0m\n",
      "\u001b[34m│   120                                                                        │\u001b[0m\n",
      "\u001b[34m│   121 def parse_def(fn):                                                     │\u001b[0m\n",
      "\u001b[34m│ ❱ 122 │   sourcelines, file_lineno, filename = get_source_lines_and_file(    │\u001b[0m\n",
      "\u001b[34m│   123 │   │   fn, ErrorReport.call_stack()                                   │\u001b[0m\n",
      "\u001b[34m│   124 │   )                                                                  │\u001b[0m\n",
      "\u001b[34m│   125 │   sourcelines = normalize_source_lines(sourcelines)                  │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.9/site-packages/torch/_sources.py:32 in               │\u001b[0m\n",
      "\u001b[34m│ get_source_lines_and_file                                                    │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│    29 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m│    30 │   │   if error_msg:                                                  │\u001b[0m\n",
      "\u001b[34m│    31 │   │   │   msg += \"\\n\" + error_msg                                    │\u001b[0m\n",
      "\u001b[34m│ ❱  32 │   │   raise OSError(msg) from e                                      │\u001b[0m\n",
      "\u001b[34m│    33 │                                                                      │\u001b[0m\n",
      "\u001b[34m│    34 │   return sourcelines, file_lineno, filename                          │\u001b[0m\n",
      "\u001b[34m│    35                                                                        │\u001b[0m\n",
      "\u001b[34m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\u001b[34mOSError: Can't get source for <function apply_rotary_pos_emb_index at \u001b[0m\n",
      "\u001b[34m0x7f63301331f0>. TorchScript requires source access in order to carry out \u001b[0m\n",
      "\u001b[34mcompilation, make sure original .py files are available.\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   1%|          | 21.0M/1.74G [00:00<00:08, 209MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   4%|▎         | 62.9M/1.74G [00:00<00:05, 299MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   6%|▌         | 105M/1.74G [00:00<00:05, 322MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   8%|▊         | 147M/1.74G [00:00<00:04, 334MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  11%|█         | 189M/1.74G [00:00<00:04, 339MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  13%|█▎        | 231M/1.74G [00:00<00:04, 345MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  16%|█▌        | 273M/1.74G [00:00<00:04, 351MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  18%|█▊        | 315M/1.74G [00:00<00:04, 354MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  20%|██        | 357M/1.74G [00:01<00:03, 359MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  23%|██▎       | 398M/1.74G [00:01<00:03, 361MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  25%|██▌       | 440M/1.74G [00:01<00:03, 364MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  28%|██▊       | 482M/1.74G [00:01<00:03, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  30%|███       | 524M/1.74G [00:01<00:03, 369MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[2023-05-07 02:29:33,976] [INFO] [launch.py:318:sigkill_handler] Killing subprocess 480\u001b[0m\n",
      "\u001b[34m[2023-05-07 02:29:34,030] [INFO] [launch.py:318:sigkill_handler] Killing subprocess 481\u001b[0m\n",
      "\u001b[34m[2023-05-07 02:29:34,030] [INFO] [launch.py:318:sigkill_handler] Killing subprocess 482\u001b[0m\n",
      "\u001b[34m[2023-05-07 02:29:34,030] [INFO] [launch.py:318:sigkill_handler] Killing subprocess 483\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  33%|███▎      | 566M/1.74G [00:01<00:03, 364MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[2023-05-07 02:29:34,057] [INFO] [launch.py:318:sigkill_handler] Killing subprocess 484\u001b[0m\n",
      "\u001b[34m[2023-05-07 02:29:34,084] [INFO] [launch.py:318:sigkill_handler] Killing subprocess 485\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   0%|          | 0.00/1.74G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[2023-05-07 02:29:34,111] [INFO] [launch.py:318:sigkill_handler] Killing subprocess 486\u001b[0m\n",
      "\u001b[34m[2023-05-07 02:29:34,284] [INFO] [launch.py:318:sigkill_handler] Killing subprocess 487\u001b[0m\n",
      "\u001b[34m[2023-05-07 02:29:34,285] [ERROR] [launch.py:324:sigkill_handler] ['/opt/conda/bin/python3.9', '-u', 'main.py', '--local_rank=7', '--deepspeed', 'deepspeed.json', '--do_train', '--train_file', '/opt/ml/input/data/AdvertiseGen/train.json', '--test_file', '/opt/ml/input/data/AdvertiseGen/dev.json', '--prompt_column', 'content', '--response_column', 'summary', '--overwrite_cache', '--model_name_or_path', 'THUDM/chatglm-6b', '--output_dir', '/tmp/model/adgen-chatglm-6b-ft', '--model_output_s3_path', 's3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed/', '--overwrite_output_dir', '--max_source_length', '64', '--max_target_length', '64', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--predict_with_generate', '--max_steps', '50', '--logging_steps', '10', '--save_steps', '50', '--learning_rate', '1e-4', '--fp16'] exits with return code = 1\u001b[0m\n",
      "\u001b[34m2023-05-07 02:29:34,720 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-05-07 02:29:34,720 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-05-07 02:29:34,721 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-05-07 02:29:48 Uploading - Uploading generated training model\n",
      "2023-05-07 02:29:48 Completed - Training job completed\n",
      "Training seconds: 304\n",
      "Billable seconds: 304\n"
     ]
    }
   ],
   "source": [
    "# define Training Job Name \n",
    "job_name = f'huggingface-chatglm-deepspeed-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "#define the model s3 path which will store your trained model asset\n",
    "#Note: you should use your real s3 path to configure model_s3_path\n",
    "model_s3_path='s3://{}/llm/models/chatglm/deepspeed/'.format(sagemaker_session.default_bucket())\n",
    "output_dir = '/tmp/model/adgen-chatglm-6b-ft'\n",
    "model_name_or_path = 'THUDM/chatglm-6b'\n",
    "\n",
    "\n",
    "instance_count = 1\n",
    "#define the enviroment variables for your scripts.\n",
    "environment = {\n",
    "              'MODEL_S3_PATH'          : model_s3_path,\n",
    "              'PYTORCH_CUDA_ALLOC_CONF': 'max_split_size_mb:32',\n",
    "              #'LD_LIBRARY_PATH'        : '${LD_LIBRARY_PATH}:/opt/conda/lib/',\n",
    "              'NUM_GPUS'               : str(processes_per_host),\n",
    "              'TRAIN_DATASET'          : '/opt/ml/input/data/AdvertiseGen/train.json',\n",
    "              'TEST_DATASET'           : '/opt/ml/input/data/AdvertiseGen/dev.json',\n",
    "              'PROMPT_COLUMN'          : 'content',\n",
    "              'RESPONSE_COLUMN'        : 'summary',\n",
    "              'MODEL_NAME_OR_PATH'     : model_name_or_path,\n",
    "              'OUTPUT_DIR'             : output_dir,\n",
    "              'MODEL_OUTPUT_S3_PATH'   : model_s3_path,\n",
    "              'TRAIN_STEPS'            :'50'\n",
    "}\n",
    "\n",
    "inputs={\n",
    "   'AdvertiseGen': f\"s3://{bucket}/llm/chatglm/datasets/\"\n",
    "}\n",
    "\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'start-single-node.py',          # user endpoint script\n",
    "    source_dir           = 'ChatGLM-6B/ptuning',               # directory which includes all the files needed for training\n",
    "    instance_type        = instance_type, # instances type used for the training job\n",
    "    instance_count       = instance_count,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    script_mode          = True,\n",
    "    transformers_version = '4.26',            # the transformers version used in the training job\n",
    "    pytorch_version      = '1.13',            # the pytorch_version version used in the training job\n",
    "    py_version           = 'py39',            # the python version used in the training job\n",
    "    environment = environment,\n",
    ")\n",
    "huggingface_estimator.fit(inputs=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b4ed43d3-3727-421d-8a5f-d289dcdd2f22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-deepspeed-2023-05-0-2023-05-07-02-22-03-330/output/model.tar.gz'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf58dcd-d9f2-43b8-b9b7-f477985fb823",
   "metadata": {
    "tags": []
   },
   "source": [
    "### chatglm deepspeed 多机多卡改造\n",
    "1: 准备deepspeed lib，并修改deepspeed.json    \n",
    "2：数据集（以上一致）  \n",
    "3：entrypoint start.py,设置deepspeed hosts configure\n",
    "4：触发bash ds_train_finetune.sh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea32b09-7afd-42f1-b144-721db4660e53",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-chatglm-deepspeed-2023-05-0-2023-05-07-02-32-53-308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-07 02:33:19 Starting - Starting the training job......\n",
      "2023-05-07 02:34:06 Starting - Preparing the instances for training.........\n",
      "2023-05-07 02:35:38 Downloading - Downloading input data...\n",
      "2023-05-07 02:35:53 Training - Downloading the training image...............\n",
      "2023-05-07 02:38:44 Training - Training image download completed. Training in progress.......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-05-07 02:39:41,180 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-05-07 02:39:41,241 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-07 02:39:41,250 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-05-07 02:39:41,252 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-05-07 02:39:45,020 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (3.20.2)\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.27.1\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.7/6.7 MB 52.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting cpm_kernels\u001b[0m\n",
      "\u001b[34mDownloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 416.6/416.6 kB 72.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mCollecting gradio\u001b[0m\n",
      "\u001b[34mDownloading gradio-3.28.3-py3-none-any.whl (17.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 91.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting mdtex2html\u001b[0m\n",
      "\u001b[34mDownloading mdtex2html-1.2.0-py3-none-any.whl (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (0.1.97)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (0.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (2.9.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface\u001b[0m\n",
      "\u001b[34mDownloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting jieba\u001b[0m\n",
      "\u001b[34mDownloading jieba-0.42.1.tar.gz (19.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/19.2 MB 80.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting rouge_chinese\u001b[0m\n",
      "\u001b[34mDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\u001b[0m\n",
      "\u001b[34mCollecting nltk\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 102.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting deepspeed==0.8.0\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.8.0.tar.gz (749 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 749.9/749.9 kB 87.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2023-05-07 02:39:43,564 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2023-05-07 02:39:43,625 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-05-07 02:39:43,634 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2023-05-07 02:39:43,636 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2023-05-07 02:39:47,241 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (3.20.2)\u001b[0m\n",
      "\u001b[35mCollecting transformers==4.27.1\u001b[0m\n",
      "\u001b[35mDownloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.7/6.7 MB 62.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting cpm_kernels\u001b[0m\n",
      "\u001b[35mDownloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 416.6/416.6 kB 64.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch>=1.10 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[35mCollecting gradio\u001b[0m\n",
      "\u001b[35mDownloading gradio-3.28.3-py3-none-any.whl (17.3 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 75.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting mdtex2html\u001b[0m\n",
      "\u001b[35mDownloading mdtex2html-1.2.0-py3-none-any.whl (13 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (0.1.97)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (0.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: datasets in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (2.9.0)\u001b[0m\n",
      "\u001b[35mCollecting huggingface\u001b[0m\n",
      "\u001b[35mDownloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\u001b[0m\n",
      "\u001b[35mCollecting jieba\u001b[0m\n",
      "\u001b[35mDownloading jieba-0.42.1.tar.gz (19.2 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/19.2 MB 73.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting rouge_chinese\u001b[0m\n",
      "\u001b[35mDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\u001b[0m\n",
      "\u001b[35mCollecting nltk\u001b[0m\n",
      "\u001b[35mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 109.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting deepspeed==0.8.0\u001b[0m\n",
      "\u001b[35mDownloading deepspeed-0.8.0.tar.gz (749 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 749.9/749.9 kB 79.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (0.13.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (2022.10.31)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (3.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.0->-r requirements.txt (line 14)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.0->-r requirements.txt (line 14)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.0->-r requirements.txt (line 14)) (5.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.0->-r requirements.txt (line 14)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.0->-r requirements.txt (line 14)) (1.10.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.10->-r requirements.txt (line 4)) (4.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (3.1.2)\u001b[0m\n",
      "\u001b[34mCollecting ffmpy\u001b[0m\n",
      "\u001b[34mDownloading ffmpy-0.3.0.tar.gz (4.8 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting markdown-it-py[linkify]>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.5/84.5 kB 24.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting fastapi\u001b[0m\n",
      "\u001b[34mDownloading fastapi-0.95.1-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.0/57.0 kB 16.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting python-multipart\u001b[0m\n",
      "\u001b[34mDownloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.7/45.7 kB 12.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (1.5.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (3.6.3)\u001b[0m\n",
      "\u001b[34mCollecting uvicorn\u001b[0m\n",
      "\u001b[34mDownloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 16.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiofiles\u001b[0m\n",
      "\u001b[34mDownloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mCollecting altair>=4.2.0\u001b[0m\n",
      "\u001b[34mDownloading altair-4.2.2-py3-none-any.whl (813 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 813.6/813.6 kB 84.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pydub\u001b[0m\n",
      "\u001b[34mDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments>=2.12.0 in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (2.14.0)\u001b[0m\n",
      "\u001b[34mCollecting httpx\u001b[0m\n",
      "\u001b[34mDownloading httpx-0.24.0-py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.3/75.3 kB 22.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting orjson\u001b[0m\n",
      "\u001b[34mDownloading orjson-3.8.11-cp39-cp39-manylinux_2_28_x86_64.whl (135 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.6/135.6 kB 33.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (3.8.4)\u001b[0m\n",
      "\u001b[34mCollecting mdit-py-plugins<=0.3.3\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.5/50.5 kB 15.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.11.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 42.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting gradio-client>=0.1.3\u001b[0m\n",
      "\u001b[34mDownloading gradio_client-0.2.0-py3-none-any.whl (287 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 287.9/287.9 kB 50.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markupsafe in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (2.1.2)\u001b[0m\n",
      "\u001b[34mCollecting semantic-version\u001b[0m\n",
      "\u001b[34mDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (9.4.0)\u001b[0m\n",
      "\u001b[34mCollecting websockets>=10.0\u001b[0m\n",
      "\u001b[34mDownloading websockets-11.0.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.7/129.7 kB 37.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown in /opt/conda/lib/python3.9/site-packages (from mdtex2html->-r requirements.txt (line 6)) (3.4.1)\u001b[0m\n",
      "\u001b[34mCollecting latex2mathml\u001b[0m\n",
      "\u001b[34mDownloading latex2mathml-3.75.5-py3-none-any.whl (73 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.3/73.3 kB 14.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (11.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (2023.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from rouge_chinese->-r requirements.txt (line 12)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 13)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 13)) (8.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.9/site-packages (from altair>=4.2.0->gradio->-r requirements.txt (line 5)) (4.17.3)\u001b[0m\n",
      "\u001b[34mCollecting entrypoints\u001b[0m\n",
      "\u001b[34mDownloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: toolz in /opt/conda/lib/python3.9/site-packages (from altair>=4.2.0->gradio->-r requirements.txt (line 5)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (1.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (6.0.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (23.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (4.64.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (2022.10.31)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (0.13.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (1.23.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (2.28.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (0.12.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (3.9.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (5.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: hjson in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.0->-r requirements.txt (line 14)) (3.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.0->-r requirements.txt (line 14)) (1.11.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.0->-r requirements.txt (line 14)) (5.9.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.0->-r requirements.txt (line 14)) (9.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pydantic in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.0->-r requirements.txt (line 14)) (1.10.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.10->-r requirements.txt (line 4)) (4.4.0)\u001b[0m\n",
      "\u001b[35mCollecting aiofiles\u001b[0m\n",
      "\u001b[35mDownloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[35mCollecting python-multipart\u001b[0m\n",
      "\u001b[35mDownloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.7/45.7 kB 11.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting websockets>=10.0\u001b[0m\n",
      "\u001b[35mDownloading websockets-11.0.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.7/129.7 kB 31.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting mdit-py-plugins<=0.3.3\u001b[0m\n",
      "\u001b[35mDownloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.5/50.5 kB 12.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pillow in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (9.4.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: markupsafe in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (2.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (1.5.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (3.8.4)\u001b[0m\n",
      "\u001b[35mCollecting semantic-version\u001b[0m\n",
      "\u001b[35mDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[35mCollecting ffmpy\u001b[0m\n",
      "\u001b[35mDownloading ffmpy-0.3.0.tar.gz (4.8 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pygments>=2.12.0 in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (2.14.0)\u001b[0m\n",
      "\u001b[34mCollecting mdurl~=0.1\u001b[0m\n",
      "\u001b[34mDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting linkify-it-py<3,>=1\u001b[0m\n",
      "\u001b[34mDownloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->gradio->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->gradio->-r requirements.txt (line 5)) (2022.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 2)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 2)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 2)) (3.4)\u001b[0m\n",
      "\u001b[34mCollecting starlette<0.27.0,>=0.26.1\u001b[0m\n",
      "\u001b[34mDownloading starlette-0.26.1-py3-none-any.whl (66 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.9/66.9 kB 18.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting sniffio\u001b[0m\n",
      "\u001b[34mDownloading sniffio-1.3.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting httpcore<0.18.0,>=0.15.0\u001b[0m\n",
      "\u001b[34mDownloading httpcore-0.17.0-py3-none-any.whl (70 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 70.6/70.6 kB 20.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown->mdtex2html->-r requirements.txt (line 6)) (4.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 5)) (1.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 5)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 5)) (1.0.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 5)) (0.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 5)) (4.38.0)\u001b[0m\n",
      "\u001b[34mCollecting h11>=0.8\u001b[0m\n",
      "\u001b[34mDownloading h11-0.14.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 15.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting anyio<5.0,>=3.0\u001b[0m\n",
      "\u001b[34mDownloading anyio-3.6.2-py3-none-any.whl (80 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.6/80.6 kB 26.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown->mdtex2html->-r requirements.txt (line 6)) (3.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio->-r requirements.txt (line 5)) (0.19.3)\u001b[0m\n",
      "\u001b[34mCollecting uc-micro-py\u001b[0m\n",
      "\u001b[34mDownloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed, jieba, ffmpy\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[35mCollecting orjson\u001b[0m\n",
      "\u001b[35mDownloading orjson-3.8.11-cp39-cp39-manylinux_2_28_x86_64.whl (135 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.6/135.6 kB 37.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting altair>=4.2.0\u001b[0m\n",
      "\u001b[35mDownloading altair-4.2.2-py3-none-any.whl (813 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 813.6/813.6 kB 77.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting pydub\u001b[0m\n",
      "\u001b[35mDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\u001b[0m\n",
      "\u001b[35mCollecting markdown-it-py[linkify]>=2.0.0\u001b[0m\n",
      "\u001b[35mDownloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.5/84.5 kB 25.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting fastapi\u001b[0m\n",
      "\u001b[35mDownloading fastapi-0.95.1-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.0/57.0 kB 12.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting huggingface-hub<1.0,>=0.11.0\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 51.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (3.6.3)\u001b[0m\n",
      "\u001b[35mCollecting gradio-client>=0.1.3\u001b[0m\n",
      "\u001b[35mDownloading gradio_client-0.2.0-py3-none-any.whl (287 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 287.9/287.9 kB 47.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting httpx\u001b[0m\n",
      "\u001b[35mDownloading httpx-0.24.0-py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.3/75.3 kB 18.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting uvicorn\u001b[0m\n",
      "\u001b[35mDownloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 16.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 5)) (3.1.2)\u001b[0m\n",
      "\u001b[35mCollecting latex2mathml\u001b[0m\n",
      "\u001b[35mDownloading latex2mathml-3.75.5-py3-none-any.whl (73 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.3/73.3 kB 21.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: markdown in /opt/conda/lib/python3.9/site-packages (from mdtex2html->-r requirements.txt (line 6)) (3.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (0.3.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (2023.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (3.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (0.18.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (11.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (0.70.14)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from rouge_chinese->-r requirements.txt (line 12)) (1.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 13)) (8.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 13)) (1.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.9/site-packages (from altair>=4.2.0->gradio->-r requirements.txt (line 5)) (4.17.3)\u001b[0m\n",
      "\u001b[35mCollecting entrypoints\u001b[0m\n",
      "\u001b[35mDownloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: toolz in /opt/conda/lib/python3.9/site-packages (from altair>=4.2.0->gradio->-r requirements.txt (line 5)) (0.12.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (1.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (6.0.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (4.0.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (1.3.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (22.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (2.1.1)\u001b[0m\n",
      "\u001b[35mCollecting mdurl~=0.1\u001b[0m\n",
      "\u001b[35mDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\u001b[0m\n",
      "\u001b[35mCollecting linkify-it-py<3,>=1\u001b[0m\n",
      "\u001b[35mDownloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->gradio->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->gradio->-r requirements.txt (line 5)) (2022.7.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 2)) (1.26.14)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 2)) (2022.12.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 2)) (3.4)\u001b[0m\n",
      "\u001b[35mCollecting starlette<0.27.0,>=0.26.1\u001b[0m\n",
      "\u001b[35mDownloading starlette-0.26.1-py3-none-any.whl (66 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.9/66.9 kB 19.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting sniffio\u001b[0m\n",
      "\u001b[35mDownloading sniffio-1.3.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[35mCollecting httpcore<0.18.0,>=0.15.0\u001b[0m\n",
      "\u001b[35mDownloading httpcore-0.17.0-py3-none-any.whl (70 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 70.6/70.6 kB 22.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown->mdtex2html->-r requirements.txt (line 6)) (4.13.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 5)) (1.4.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 5)) (0.11.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 5)) (3.0.9)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 5)) (4.38.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 5)) (1.0.7)\u001b[0m\n",
      "\u001b[35mCollecting h11>=0.8\u001b[0m\n",
      "\u001b[35mDownloading h11-0.14.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 13.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting anyio<5.0,>=3.0\u001b[0m\n",
      "\u001b[35mDownloading anyio-3.6.2-py3-none-any.whl (80 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.6/80.6 kB 24.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown->mdtex2html->-r requirements.txt (line 6)) (3.13.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio->-r requirements.txt (line 5)) (0.19.3)\u001b[0m\n",
      "\u001b[35mCollecting uc-micro-py\u001b[0m\n",
      "\u001b[35mDownloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: deepspeed, jieba, ffmpy\u001b[0m\n",
      "\u001b[35mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.8.0-py3-none-any.whl size=752137 sha256=7bc368b44aeeb20132911d4bd790c9ebc07b68cd4864de8a3de5182a2859977b\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/ba/27/48/edac683ddf3d54d4a619ca460375bd226bf1db4a627d681b7c\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for deepspeed: filename=deepspeed-0.8.0-py3-none-any.whl size=752137 sha256=991557a6e91750c6b36103a38c8d795d308e1cf03fae51cd67a5dbdb1e967a63\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/ba/27/48/edac683ddf3d54d4a619ca460375bd226bf1db4a627d681b7c\u001b[0m\n",
      "\u001b[35mBuilding wheel for jieba (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=aeda6e928f5e3d6cbc2dc9a8640358e3d8c4efaed588d890e584c1ab18fda0f1\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/7d/74/cf/08c94db4b784e2c1ef675a600b7b5b281fd25240dcb954ee7e\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4693 sha256=88639e5b23502afedc0ef702cac1555cd37552dd9870be3df985709aabcfc1fd\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/91/e2/96/f676aa08bfd789328c6576cd0f1fde4a3d686703bb0c247697\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed jieba ffmpy\u001b[0m\n",
      "\u001b[35mBuilding wheel for jieba (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=6f77cc45ac27780e11674ea0f3bad0018582fa85211d55aec894c52811b301b7\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/7d/74/cf/08c94db4b784e2c1ef675a600b7b5b281fd25240dcb954ee7e\u001b[0m\n",
      "\u001b[35mBuilding wheel for ffmpy (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for ffmpy (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4693 sha256=a5014cd6ae09e91f39ddfe04630726dbba97fb9880ad1af5ecfaef22b8b003ab\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/91/e2/96/f676aa08bfd789328c6576cd0f1fde4a3d686703bb0c247697\u001b[0m\n",
      "\u001b[35mSuccessfully built deepspeed jieba ffmpy\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pydub, jieba, huggingface, ffmpy, cpm_kernels, websockets, uc-micro-py, sniffio, semantic-version, rouge_chinese, python-multipart, orjson, nltk, mdurl, latex2mathml, h11, entrypoints, aiofiles, uvicorn, markdown-it-py, linkify-it-py, huggingface-hub, deepspeed, anyio, transformers, starlette, mdtex2html, mdit-py-plugins, httpcore, altair, httpx, fastapi, gradio-client, gradio\u001b[0m\n",
      "\u001b[35mInstalling collected packages: pydub, jieba, huggingface, ffmpy, cpm_kernels, websockets, uc-micro-py, sniffio, semantic-version, rouge_chinese, python-multipart, orjson, nltk, mdurl, latex2mathml, h11, entrypoints, aiofiles, uvicorn, markdown-it-py, linkify-it-py, huggingface-hub, deepspeed, anyio, transformers, starlette, mdtex2html, mdit-py-plugins, httpcore, altair, httpx, fastapi, gradio-client, gradio\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.12.0\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.12.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.12.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+06f2048\u001b[0m\n",
      "\u001b[34mUninstalling deepspeed-0.6.1+06f2048:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled deepspeed-0.6.1+06f2048\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.26.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.26.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.26.0\u001b[0m\n",
      "\u001b[35mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[35mFound existing installation: huggingface-hub 0.12.0\u001b[0m\n",
      "\u001b[35mUninstalling huggingface-hub-0.12.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled huggingface-hub-0.12.0\u001b[0m\n",
      "\u001b[35mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[35mFound existing installation: deepspeed 0.6.1+06f2048\u001b[0m\n",
      "\u001b[35mUninstalling deepspeed-0.6.1+06f2048:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled deepspeed-0.6.1+06f2048\u001b[0m\n",
      "\u001b[35mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[35mFound existing installation: transformers 4.26.0\u001b[0m\n",
      "\u001b[35mUninstalling transformers-4.26.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled transformers-4.26.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiofiles-23.1.0 altair-4.2.2 anyio-3.6.2 cpm_kernels-1.0.11 deepspeed-0.8.0 entrypoints-0.4 fastapi-0.95.1 ffmpy-0.3.0 gradio-3.28.3 gradio-client-0.2.0 h11-0.14.0 httpcore-0.17.0 httpx-0.24.0 huggingface-0.0.1 huggingface-hub-0.14.1 jieba-0.42.1 latex2mathml-3.75.5 linkify-it-py-2.0.2 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 mdtex2html-1.2.0 mdurl-0.1.2 nltk-3.8.1 orjson-3.8.11 pydub-0.25.1 python-multipart-0.0.6 rouge_chinese-1.0.3 semantic-version-2.10.0 sniffio-1.3.0 starlette-0.26.1 transformers-4.27.1 uc-micro-py-1.0.2 uvicorn-0.22.0 websockets-11.0.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.1.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-05-07 02:40:10,533 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-05-07 02:40:10,533 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-05-07 02:40:10,600 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-07 02:40:10,675 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-07 02:40:10,747 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-07 02:40:10,757 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"AdvertiseGen\": \"/opt/ml/input/data/AdvertiseGen\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"AdvertiseGen\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-chatglm-deepspeed-2023-05-0-2023-05-07-02-32-53-308\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-deepspeed-2023-05-0-2023-05-07-02-32-53-308/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"start\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"start.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=start.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"AdvertiseGen\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"AdvertiseGen\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=start\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-deepspeed-2023-05-0-2023-05-07-02-32-53-308/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"AdvertiseGen\":\"/opt/ml/input/data/AdvertiseGen\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"AdvertiseGen\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-chatglm-deepspeed-2023-05-0-2023-05-07-02-32-53-308\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-deepspeed-2023-05-0-2023-05-07-02-32-53-308/source/sourcedir.tar.gz\",\"module_name\":\"start\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"start.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_ADVERTISEGEN=/opt/ml/input/data/AdvertiseGen\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 start.py\u001b[0m\n",
      "\u001b[35mSuccessfully installed aiofiles-23.1.0 altair-4.2.2 anyio-3.6.2 cpm_kernels-1.0.11 deepspeed-0.8.0 entrypoints-0.4 fastapi-0.95.1 ffmpy-0.3.0 gradio-3.28.3 gradio-client-0.2.0 h11-0.14.0 httpcore-0.17.0 httpx-0.24.0 huggingface-0.0.1 huggingface-hub-0.14.1 jieba-0.42.1 latex2mathml-3.75.5 linkify-it-py-2.0.2 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 mdtex2html-1.2.0 mdurl-0.1.2 nltk-3.8.1 orjson-3.8.11 pydub-0.25.1 python-multipart-0.0.6 rouge_chinese-1.0.3 semantic-version-2.10.0 sniffio-1.3.0 starlette-0.26.1 transformers-4.27.1 uc-micro-py-1.0.2 uvicorn-0.22.0 websockets-11.0.2\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m[notice] A new release of pip is available: 23.0 -> 23.1.2\u001b[0m\n",
      "\u001b[35m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[35m2023-05-07 02:40:12,655 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2023-05-07 02:40:12,656 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2023-05-07 02:40:12,721 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-05-07 02:40:12,794 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-05-07 02:40:12,866 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-05-07 02:40:12,876 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"AdvertiseGen\": \"/opt/ml/input/data/AdvertiseGen\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"AdvertiseGen\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-chatglm-deepspeed-2023-05-0-2023-05-07-02-32-53-308\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-deepspeed-2023-05-0-2023-05-07-02-32-53-308/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"start\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"start.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=start.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"AdvertiseGen\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"AdvertiseGen\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=start\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-deepspeed-2023-05-0-2023-05-07-02-32-53-308/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"AdvertiseGen\":\"/opt/ml/input/data/AdvertiseGen\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"AdvertiseGen\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-chatglm-deepspeed-2023-05-0-2023-05-07-02-32-53-308\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-deepspeed-2023-05-0-2023-05-07-02-32-53-308/source/sourcedir.tar.gz\",\"module_name\":\"start\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"start.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_ADVERTISEGEN=/opt/ml/input/data/AdvertiseGen\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.9 start.py\u001b[0m\n",
      "\u001b[34m[2023-05-07 02:40:14.665: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.27.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-05-07 02:40:14,670 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-05-07 02:40:14,690 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m10.0.232.182 slots=8\u001b[0m\n",
      "\u001b[34m10.0.208.196 slots=8\u001b[0m\n",
      "\u001b[34mpython -m torch.distributed.launch --nproc_per_node 8 --nnodes 2 --node_rank 0 --master_addr algo-1 --master_port 51380 /opt/ml/code/ChatGLM-6B/ptuning//main.py --deepspeed --deepspeed_config deepspeed.json --do_train --train_file /opt/ml/input/data/AdvertiseGen/train.json --test_file /opt/ml/input/data/AdvertiseGen/dev.json --prompt_column content --response_column summary --overwrite_cache --model_name_or_path THUDM/chatglm-6b --output_dir /tmp/model/adgen-chatglm-6b-ft --model_output_s3_path s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed/ --overwrite_output_dir --max_source_length 64 --max_target_length 64 --per_device_train_batch_size 1 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --predict_with_generate --max_steps 50 --logging_steps 10 --save_steps 50 --learning_rate 1e-4 --fp16\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated\u001b[0m\n",
      "\u001b[34mand will be removed in future. Use torchrun.\u001b[0m\n",
      "\u001b[34mNote that --use_env is set by default in torchrun.\u001b[0m\n",
      "\u001b[34mIf your script expects `--local_rank` argument to be set, please\u001b[0m\n",
      "\u001b[34mchange it to read from `os.environ['LOCAL_RANK']` instead. See \u001b[0m\n",
      "\u001b[34mhttps://pytorch.org/docs/stable/distributed.html#launch-utility for \u001b[0m\n",
      "\u001b[34mfurther instructions\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[35m[2023-05-07 02:40:16.880: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.27.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m2023-05-07 02:40:16,884 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[35m2023-05-07 02:40:16,905 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[35m10.0.232.182 slots=8\u001b[0m\n",
      "\u001b[35m10.0.208.196 slots=8\u001b[0m\n",
      "\u001b[35m2023-05-07 02:40:16,954 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2023-05-07 02:40:16,954 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2023-05-07 02:40:16,954 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# define Training Job Name \n",
    "job_name = f'huggingface-chatglm-deepspeed-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "#define the model s3 path which will store your trained model asset\n",
    "#Note: you should use your real s3 path to configure model_s3_path\n",
    "model_s3_path='s3://{}/llm/models/chatglm/deepspeed/'.format(sagemaker_session.default_bucket())\n",
    "output_dir = '/tmp/model/adgen-chatglm-6b-ft'\n",
    "model_name_or_path = 'THUDM/chatglm-6b'\n",
    "\n",
    "\n",
    "instance_count = 2\n",
    "#define the enviroment variables for your scripts.\n",
    "environment = {\n",
    "              'NODE_NUMBER'            : str(instance_count),\n",
    "              'MODEL_S3_PATH'          : model_s3_path,\n",
    "              'PYTORCH_CUDA_ALLOC_CONF': 'max_split_size_mb:32',\n",
    "              #'LD_LIBRARY_PATH'        : '${LD_LIBRARY_PATH}:/opt/conda/lib/',\n",
    "              'NUM_GPUS'               : str(processes_per_host),\n",
    "              'TRAIN_DATASET'          : '/opt/ml/input/data/AdvertiseGen/train.json',\n",
    "              'TEST_DATASET'           : '/opt/ml/input/data/AdvertiseGen/dev.json',\n",
    "              'PROMPT_COLUMN'          : 'content',\n",
    "              'RESPONSE_COLUMN'        : 'summary',\n",
    "              'MODEL_NAME_OR_PATH'     : model_name_or_path,\n",
    "              'OUTPUT_DIR'             : output_dir,\n",
    "              'MODEL_OUTPUT_S3_PATH'   : model_s3_path,\n",
    "              'TRAIN_STEPS'            :'50'\n",
    "}\n",
    "\n",
    "inputs={\n",
    "   'AdvertiseGen': f\"s3://{bucket}/llm/chatglm/datasets/\"\n",
    "}\n",
    "\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'start.py',          # user endpoint script\n",
    "    source_dir           = 'ChatGLM-6B/ptuning',               # directory which includes all the files needed for training\n",
    "    instance_type        = instance_type, # instances type used for the training job\n",
    "    instance_count       = instance_count,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    script_mode          = True,\n",
    "    transformers_version = '4.26',            # the transformers version used in the training job\n",
    "    pytorch_version      = '1.13',            # the pytorch_version version used in the training job\n",
    "    py_version           = 'py39',            # the python version used in the training job\n",
    "    environment = environment,\n",
    ")\n",
    "huggingface_estimator.fit(inputs=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37933c32-ad7c-4564-a8d1-dcfd79531338",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
