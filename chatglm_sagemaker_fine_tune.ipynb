{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "252de0de",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SageMaker fine tune ChatGLM\n",
    "\n",
    "#### 准备\n",
    "1. 升级boto3, sagemaker python sdk  \n",
    "2. 准备requirements.txt\n",
    "3. 准备s5cmd utility\n",
    "4. 下载HF model，并上传到S3(optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8f2c403",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.153.0-py2.py3-none-any.whl size=1008105 sha256=b5c59266f9d3f8b737aad3656323127b1d96ebc365d2213038e25c3985ac00cf\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/6a/0b/81/d7a0f0e734bc9f5ea74716acef6b397a6d473cb8e6b9b3f121\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: cloudpickle, sagemaker\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 2.2.0\n",
      "    Uninstalling cloudpickle-2.2.0:\n",
      "      Successfully uninstalled cloudpickle-2.2.0\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.146.0\n",
      "    Uninstalling sagemaker-2.146.0:\n",
      "      Successfully uninstalled sagemaker-2.146.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "distributed 2022.11.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cloudpickle-2.2.1 sagemaker-2.153.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade boto3\n",
    "!pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9436071c-6e98-4b38-aff8-507193445382",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 4176k  100 4176k    0     0  5928k      0 --:--:-- --:--:-- --:--:-- 11.9M\n"
     ]
    }
   ],
   "source": [
    "#print('s3://{}/llm/models/'.format(sagemaker_session.default_bucket()))\n",
    "#!aws s3 ls s3://sagemaker-us-west-2-687912291502/llm/models/\n",
    "!curl -L https://github.com/peak/s5cmd/releases/download/v2.0.0/s5cmd_2.0.0_Linux-64bit.tar.gz | tar -xz && mv s5cmd ./ptuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4a30f3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::687912291502:role/service-role/AmazonSageMaker-ExecutionRole-20211013T113123\n",
      "sagemaker-us-west-2-687912291502\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region_name = boto3.session.Session().region_name\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(role)\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ffb9b61-51de-49ab-a21f-e462918b6920",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface_hub) (3.6.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface_hub) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface_hub) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface_hub) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface_hub) (4.4.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface_hub) (4.63.2)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface_hub) (2022.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from packaging>=20.9->huggingface_hub) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->huggingface_hub) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->huggingface_hub) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->huggingface_hub) (2022.12.7)\n",
      "Installing collected packages: huggingface_hub\n",
      "Successfully installed huggingface_hub-0.14.1\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30102f03-a5b5-42e4-9f87-76131cd3de1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013855934143066406,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Fetching 21 files",
       "rate": null,
       "total": 21,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb00b6542394ebe8193648242d5f8e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 21 files:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005120038986206055,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading ice_text.model",
       "rate": null,
       "total": 2706249,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8adbf746d3d94dbda7e02c898d238c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ice_text.model:   0%|          | 0.00/2.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010117769241333008,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)fa7e74/MODEL_LICENSE",
       "rate": null,
       "total": 2347,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6c78aa919b4a4d9aea65473ad7646d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fa7e74/MODEL_LICENSE:   0%|          | 0.00/2.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015799760818481445,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)a7e74/.gitattributes",
       "rate": null,
       "total": 1477,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7889120a96145d59c3a0081e1b810ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)a7e74/.gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03249549865722656,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)dcfa7e74/config.json",
       "rate": null,
       "total": 773,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b249f042aa18473395ee67874162f2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)dcfa7e74/config.json:   0%|          | 0.00/773 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011421680450439453,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)/modeling_chatglm.py",
       "rate": null,
       "total": 57620,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77858351a95848f1898fc77803821ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/modeling_chatglm.py:   0%|          | 0.00/57.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03154349327087402,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)bd66dcfa7e74/LICENSE",
       "rate": null,
       "total": 11336,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3465747a1c8d4a15bf2f5dc719be35eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)bd66dcfa7e74/LICENSE:   0%|          | 0.00/11.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0328061580657959,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)66dcfa7e74/README.md",
       "rate": null,
       "total": 5945,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc33e1cdce18443aa221f0740b01a5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)66dcfa7e74/README.md:   0%|          | 0.00/5.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.031374454498291016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)iguration_chatglm.py",
       "rate": null,
       "total": 4276,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9dbb7efcbd401482956d8a88011f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)iguration_chatglm.py:   0%|          | 0.00/4.28k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004847049713134766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00001-of-00008.bin",
       "rate": null,
       "total": 1740651802,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a7ced40b2141f09659660810483d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00008.bin:   0%|          | 0.00/1.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011675596237182617,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00008-of-00008.bin",
       "rate": null,
       "total": 1069286123,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6fe321ad40c4baa86ad6554c2e45b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00008-of-00008.bin:   0%|          | 0.00/1.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03728818893432617,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00005-of-00008.bin",
       "rate": null,
       "total": 1879722289,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a9e6fe2a294f55b9e70df58bbaac20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00005-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016272306442260742,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00002-of-00008.bin",
       "rate": null,
       "total": 1879731432,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c817a78f3884c7ebc6c92a482a27402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021346092224121094,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00006-of-00008.bin",
       "rate": null,
       "total": 1879731496,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf44dcbcff24b9dbae01d121266db90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00006-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.022115707397460938,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00003-of-00008.bin",
       "rate": null,
       "total": 1980385902,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed17f050fb34b81a72d9e58685dba98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00003-of-00008.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01946878433227539,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00004-of-00008.bin",
       "rate": null,
       "total": 1913294120,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db027469d4246609b4af5756ed0db79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00004-of-00008.bin:   0%|          | 0.00/1.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02467489242553711,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00007-of-00008.bin",
       "rate": null,
       "total": 1074103621,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d83b70251a41329af3bcee524baa2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00007-of-00008.bin:   0%|          | 0.00/1.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0070955753326416016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)model.bin.index.json",
       "rate": null,
       "total": 33416,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80bb8e49f3f4476289e6e2e47d5820e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007597684860229492,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)7e74/quantization.py",
       "rate": null,
       "total": 15054,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9893b7215c5a45a6a48d9034860a04c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e74/quantization.py:   0%|          | 0.00/15.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007790803909301758,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)_modeling_chatglm.py",
       "rate": null,
       "total": 13822,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad6443843b1417f905754b354549191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_modeling_chatglm.py:   0%|          | 0.00/13.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010191917419433594,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)enization_chatglm.py",
       "rate": null,
       "total": 17047,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dbdd9f2962d413fb5790078ef704571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)enization_chatglm.py:   0%|          | 0.00/17.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009071111679077148,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)okenizer_config.json",
       "rate": null,
       "total": 441,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a95adbbed94957b67f5da830f2feab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "local_cache_path = Path(\"./model\")\n",
    "local_cache_path.mkdir(exist_ok=True)\n",
    "\n",
    "model_name = \"THUDM/chatglm-6b\"#\n",
    "\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.model\"]\n",
    "\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_cache_path,\n",
    "    #allow_patterns=allow_patterns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "821416b3-e6d2-43a7-8553-c71b20df5aab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74\n",
      "s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/\n"
     ]
    }
   ],
   "source": [
    "print(model_download_path)\n",
    "model_uri=\"s3://\"+bucket+\"/llm/model/chatglm/orignal/\"\n",
    "print(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2e7189-f340-49eb-983f-0d238fe7a99e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#using s5cmd util to fast upload model to s3\n",
    "! ./ptuning/s5cmd sync ./model/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/ s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/\n",
    "#!rm -rf model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cbcac2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### chatglm 官方P-tuning v2方式（单机单卡）\n",
    "1:安装依赖lib   \n",
    "2:准备数据集(本例以ADGEN 文本生成数据集为例，将解压后的 AdvertiseGen 目录放到本目录  \n",
    "3:修改并bash运行 train.sh  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ca76f3d-e961-4c4c-84cc-6ea297711a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-19 01:20:02--  https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1\n",
      "Resolving cloud.tsinghua.edu.cn (cloud.tsinghua.edu.cn)... 166.111.6.101, 2402:f000:1:406:166:111:6:101\n",
      "Connecting to cloud.tsinghua.edu.cn (cloud.tsinghua.edu.cn)|166.111.6.101|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cloud.tsinghua.edu.cn/seafhttp/files/12408e4c-4a4a-4a3e-9c7d-4b03f57ae207/AdvertiseGen.tar.gz [following]\n",
      "--2023-05-19 01:20:03--  https://cloud.tsinghua.edu.cn/seafhttp/files/12408e4c-4a4a-4a3e-9c7d-4b03f57ae207/AdvertiseGen.tar.gz\n",
      "Reusing existing connection to cloud.tsinghua.edu.cn:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 17069994 (16M) [application/octet-stream]\n",
      "Saving to: ‘index.html?dl=1’\n",
      "\n",
      "100%[======================================>] 17,069,994  5.53MB/s   in 2.9s   \n",
      "\n",
      "2023-05-19 01:20:07 (5.53 MB/s) - ‘index.html?dl=1’ saved [17069994/17069994]\n",
      "\n",
      "AdvertiseGen/\n",
      "AdvertiseGen/train.json\n",
      "AdvertiseGen/dev.json\n",
      "Name:\n",
      "  sync - sync objects\n",
      "\n",
      "Usage:\n",
      "  sync [options] source destination\n",
      "\n",
      "Options:\n",
      "ERROR \"sync ChatGLM-6B/ptuning/AdvertiseGen/ s3://sagemaker-us-west-2-687912291502/llm/chatglm/datasets/\": given object not found\n",
      "  --delete                       delete objects in destination but not in source (default: false)\n",
      "  --size-only                    make size of object only criteria to decide whether an object should be synced (default: false)\n",
      "  --no-follow-symlinks           do not follow symbolic links (default: false)\n",
      "  --storage-class value          set storage class for target ('STANDARD','REDUCED_REDUNDANCY','GLACIER','STANDARD_IA','ONEZONE_IA','INTELLIGENT_TIERING','DEEP_ARCHIVE')\n",
      "  --concurrency value, -c value  number of concurrent parts transferred between host and remote server (default: 5)\n",
      "  --part-size value, -p value    size of each part transferred between host and remote server, in MiB (default: 50)\n",
      "  --sse value                    perform server side encryption of the data at its destination, e.g. aws:kms\n",
      "  --sse-kms-key-id value         customer master key (CMK) id for SSE-KMS encryption; leave it out if server-side generated key is desired\n",
      "  --acl value                    set acl for target: defines granted accesses and their types on different accounts/groups, e.g. cp --acl 'public-read'\n",
      "  --cache-control value          set cache control for target: defines cache control header for object, e.g. cp --cache-control 'public, max-age=345600'\n",
      "  --expires value                set expires for target (uses RFC3339 format): defines expires header for object, e.g. cp  --expires '2024-10-01T20:30:00Z'\n",
      "  --force-glacier-transfer       force transfer of glacier objects whether they are restored or not (default: false)\n",
      "  --ignore-glacier-warnings      turns off glacier warnings: ignore errors encountered during copying, downloading and moving glacier objects (default: false)\n",
      "  --source-region value          set the region of source bucket; the region of the source bucket will be automatically discovered if --source-region is not specified\n",
      "  --destination-region value     set the region of destination bucket: the region of the destination bucket will be automatically discovered if --destination-region is not specified\n",
      "  --exclude value                exclude objects with given pattern\n",
      "  --raw                          disable the wildcard operations, useful with filenames that contains glob characters (default: false)\n",
      "  --help, -h                     show help (default: false)\n",
      "  \n",
      "Examples:\n",
      "  01. Sync local folder to s3 bucket\n",
      "     > s5cmd sync folder/ s3://bucket/\n",
      "\n",
      "  02. Sync S3 bucket to local folder\n",
      "     > s5cmd sync s3://bucket/* folder/\n",
      "\n",
      "  03. Sync S3 bucket objects under prefix to S3 bucket.\n",
      "     > s5cmd sync s3://sourcebucket/prefix/* s3://destbucket/\n",
      "\n",
      "  04. Sync local folder to S3 but delete the files that S3 bucket has but local does not have.\n",
      "     > s5cmd sync --delete folder/ s3://bucket/\n",
      "\n",
      "  05. Sync S3 bucket to local folder but use size as only comparison criteria.\n",
      "     > s5cmd sync --size-only s3://bucket/* folder/\n",
      "\n",
      "  06. Sync a file to S3 bucket\n",
      "     > s5cmd sync myfile.gz s3://bucket/\n",
      "\n",
      "  07. Sync matching S3 objects to another bucket\n",
      "     > s5cmd sync s3://bucket/*.gz s3://target-bucket/prefix/\n",
      "\n",
      "  08. Perform KMS Server Side Encryption of the object(s) at the destination\n",
      "     > s5cmd sync --sse aws:kms s3://bucket/object s3://target-bucket/prefix/object\n",
      "\n",
      "  09. Perform KMS-SSE of the object(s) at the destination using customer managed Customer Master Key (CMK) key id\n",
      "     > s5cmd sync --sse aws:kms --sse-kms-key-id <your-kms-key-id> s3://bucket/object s3://target-bucket/prefix/object\n",
      "\n",
      "  10. Sync all files to S3 bucket but exclude the ones with txt and gz extension\n",
      "     > s5cmd sync --exclude \"*.txt\" --exclude \"*.gz\" dir/ s3://bucket\n"
     ]
    }
   ],
   "source": [
    "!cd ptuning/ && wget \"https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1\"\n",
    "!cd ptuning/ && mv \"index.html?dl=1\" dataset.tar.gz\n",
    "!cd ptuning/ && tar -xvf dataset.tar.gz\n",
    "!./ptuning/s5cmd sync ./ptuning/AdvertiseGen/ s3://{bucket}/llm/chatglm/datasets/ \n",
    "!rm -rf cd ./ptuning/dataset.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0d9e0f9-e025-4529-aadf-2f9d798b018b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: huggingface-chatglm-simple-2023-05-19-0-2023-05-19-00-55-21-706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-19 00:55:24 Starting - Starting the training job...\n",
      "2023-05-19 00:55:39 Starting - Preparing the instances for training......\n",
      "2023-05-19 00:56:54 Downloading - Downloading input data\n",
      "2023-05-19 00:56:54 Training - Downloading the training image.....................\n",
      "2023-05-19 01:00:06 Training - Training image download completed. Training in progress...\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-05-19 01:00:29,809 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-05-19 01:00:29,827 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-19 01:00:29,838 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-05-19 01:00:29,840 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-05-19 01:00:30,378 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (3.20.2)\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.28.0\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 19.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting cpm_kernels\u001b[0m\n",
      "\u001b[34mDownloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 416.6/416.6 kB 33.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mCollecting gradio\u001b[0m\n",
      "\u001b[34mDownloading gradio-3.31.0-py3-none-any.whl (17.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.4/17.4 MB 56.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting mdtex2html\u001b[0m\n",
      "\u001b[34mDownloading mdtex2html-1.2.0-py3-none-any.whl (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (0.1.97)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (0.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (2.9.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface\u001b[0m\n",
      "\u001b[34mDownloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting jieba\u001b[0m\n",
      "\u001b[34mDownloading jieba-0.42.1.tar.gz (19.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/19.2 MB 66.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting rouge_chinese\u001b[0m\n",
      "\u001b[34mDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\u001b[0m\n",
      "\u001b[34mCollecting nltk\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 87.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting deepspeed==0.8.3\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.8.3.tar.gz (765 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 765.4/765.4 kB 84.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (2022.10.31)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (3.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (0.13.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 15)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 15)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 15)) (5.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 15)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 15)) (1.10.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.10->-r requirements.txt (line 5)) (4.4.0)\u001b[0m\n",
      "\u001b[34mCollecting markdown-it-py[linkify]>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.5/84.5 kB 26.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting python-multipart\u001b[0m\n",
      "\u001b[34mDownloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.7/45.7 kB 14.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markupsafe in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (2.1.2)\u001b[0m\n",
      "\u001b[34mCollecting mdit-py-plugins<=0.3.3\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.5/50.5 kB 17.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments>=2.12.0 in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (2.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (3.1.2)\u001b[0m\n",
      "\u001b[34mCollecting websockets>=10.0\u001b[0m\n",
      "\u001b[34mDownloading websockets-11.0.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.7/129.7 kB 35.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting httpx\u001b[0m\n",
      "\u001b[34mDownloading httpx-0.24.1-py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.4/75.4 kB 17.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pydub\u001b[0m\n",
      "\u001b[34mDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (1.5.3)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.11.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 51.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (3.6.3)\u001b[0m\n",
      "\u001b[34mCollecting altair>=4.2.0\u001b[0m\n",
      "\u001b[34mDownloading altair-5.0.0-py3-none-any.whl (477 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 477.4/477.4 kB 72.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting gradio-client>=0.2.4\u001b[0m\n",
      "\u001b[34mDownloading gradio_client-0.2.5-py3-none-any.whl (288 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 288.1/288.1 kB 54.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting orjson\u001b[0m\n",
      "\u001b[34mDownloading orjson-3.8.12-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 137.2/137.2 kB 38.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiofiles\u001b[0m\n",
      "\u001b[34mDownloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mCollecting uvicorn>=0.14.0\u001b[0m\n",
      "\u001b[34mDownloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 21.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting ffmpy\u001b[0m\n",
      "\u001b[34mDownloading ffmpy-0.3.0.tar.gz (4.8 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting fastapi\u001b[0m\n",
      "\u001b[34mDownloading fastapi-0.95.2-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.0/57.0 kB 19.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting semantic-version\u001b[0m\n",
      "\u001b[34mDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (9.4.0)\u001b[0m\n",
      "\u001b[34mCollecting latex2mathml\u001b[0m\n",
      "\u001b[34mDownloading latex2mathml-3.75.5-py3-none-any.whl (73 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.3/73.3 kB 19.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown in /opt/conda/lib/python3.9/site-packages (from mdtex2html->-r requirements.txt (line 7)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 10)) (2023.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 10)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 10)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 10)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 10)) (11.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 10)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from rouge_chinese->-r requirements.txt (line 13)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 14)) (8.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 14)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.9/site-packages (from altair>=4.2.0->gradio->-r requirements.txt (line 6)) (4.17.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: toolz in /opt/conda/lib/python3.9/site-packages (from altair>=4.2.0->gradio->-r requirements.txt (line 6)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (1.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (6.0.4)\u001b[0m\n",
      "\u001b[34mCollecting mdurl~=0.1\u001b[0m\n",
      "\u001b[34mDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting linkify-it-py<3,>=1\u001b[0m\n",
      "\u001b[34mDownloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->gradio->-r requirements.txt (line 6)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->gradio->-r requirements.txt (line 6)) (2022.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.0->-r requirements.txt (line 3)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.0->-r requirements.txt (line 3)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.0->-r requirements.txt (line 3)) (1.26.14)\u001b[0m\n",
      "\u001b[34mCollecting h11>=0.8\u001b[0m\n",
      "\u001b[34mDownloading h11-0.14.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 18.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting starlette<0.28.0,>=0.27.0\u001b[0m\n",
      "\u001b[34mDownloading starlette-0.27.0-py3-none-any.whl (66 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.0/67.0 kB 20.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting sniffio\u001b[0m\n",
      "\u001b[34mDownloading sniffio-1.3.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting httpcore<0.18.0,>=0.15.0\u001b[0m\n",
      "\u001b[34mDownloading httpcore-0.17.1-py3-none-any.whl (70 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 70.9/70.9 kB 21.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown->mdtex2html->-r requirements.txt (line 7)) (4.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 6)) (1.0.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 6)) (0.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 6)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 6)) (4.38.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 6)) (1.4.4)\u001b[0m\n",
      "\u001b[34mCollecting anyio<5.0,>=3.0\u001b[0m\n",
      "\u001b[34mDownloading anyio-3.6.2-py3-none-any.whl (80 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.6/80.6 kB 24.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown->mdtex2html->-r requirements.txt (line 7)) (3.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio->-r requirements.txt (line 6)) (0.19.3)\u001b[0m\n",
      "\u001b[34mCollecting uc-micro-py\u001b[0m\n",
      "\u001b[34mDownloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed, jieba, ffmpy\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.8.3-py3-none-any.whl size=776410 sha256=13f455e27a1964a5b8da3f1426448dfb20aa74ed30bc870a3a34866db613baf8\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/f8/ea/8f/0768328ba436ed66f602d8d3b809624448c9eb627434176d04\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=63eb1fc8372cf509e2dfeb60d6389fbe84b5827fc996a2d1252ad68e5bad6a38\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/7d/74/cf/08c94db4b784e2c1ef675a600b7b5b281fd25240dcb954ee7e\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4693 sha256=6dc2d1664d93d09a64fe30cf2019a4310abc41db9ca5704059428d74fdd381ae\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/91/e2/96/f676aa08bfd789328c6576cd0f1fde4a3d686703bb0c247697\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed jieba ffmpy\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pydub, jieba, huggingface, ffmpy, cpm_kernels, websockets, uc-micro-py, sniffio, semantic-version, rouge_chinese, python-multipart, orjson, nltk, mdurl, latex2mathml, h11, aiofiles, uvicorn, markdown-it-py, linkify-it-py, huggingface-hub, deepspeed, anyio, transformers, starlette, mdtex2html, mdit-py-plugins, httpcore, altair, httpx, fastapi, gradio-client, gradio\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.12.0\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.12.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.12.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+06f2048\u001b[0m\n",
      "\u001b[34mUninstalling deepspeed-0.6.1+06f2048:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled deepspeed-0.6.1+06f2048\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.26.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.26.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.26.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiofiles-23.1.0 altair-5.0.0 anyio-3.6.2 cpm_kernels-1.0.11 deepspeed-0.8.3 fastapi-0.95.2 ffmpy-0.3.0 gradio-3.31.0 gradio-client-0.2.5 h11-0.14.0 httpcore-0.17.1 httpx-0.24.1 huggingface-0.0.1 huggingface-hub-0.14.1 jieba-0.42.1 latex2mathml-3.75.5 linkify-it-py-2.0.2 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 mdtex2html-1.2.0 mdurl-0.1.2 nltk-3.8.1 orjson-3.8.12 pydub-0.25.1 python-multipart-0.0.6 rouge_chinese-1.0.3 semantic-version-2.10.0 sniffio-1.3.0 starlette-0.27.0 transformers-4.28.0 uc-micro-py-1.0.2 uvicorn-0.22.0 websockets-11.0.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.1.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-05-19 01:00:58,345 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-05-19 01:00:58,345 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-05-19 01:00:58,366 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-19 01:00:58,397 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-19 01:00:58,427 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-19 01:00:58,438 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"AdvertiseGen\": \"/opt/ml/input/data/AdvertiseGen\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"AdvertiseGen\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-chatglm-simple-2023-05-19-0-2023-05-19-00-55-21-706\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-simple-2023-05-19-0-2023-05-19-00-55-21-706/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"start_simple\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"start_simple.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=start_simple.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"AdvertiseGen\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"AdvertiseGen\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=start_simple\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-simple-2023-05-19-0-2023-05-19-00-55-21-706/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"AdvertiseGen\":\"/opt/ml/input/data/AdvertiseGen\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"AdvertiseGen\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-chatglm-simple-2023-05-19-0-2023-05-19-00-55-21-706\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-simple-2023-05-19-0-2023-05-19-00-55-21-706/source/sourcedir.tar.gz\",\"module_name\":\"start_simple\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"start_simple.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_ADVERTISEGEN=/opt/ml/input/data/AdvertiseGen\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 start_simple.py\u001b[0m\n",
      "\u001b[34m[2023-05-19 01:01:03.026: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-05-19 01:01:03,031 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-05-19 01:01:03,055 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m05/19/2023 01:01:07 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m05/19/2023 01:01:07 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=False,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdo_eval=False,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=no,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgeneration_config=None,\u001b[0m\n",
      "\u001b[34mgeneration_max_length=None,\u001b[0m\n",
      "\u001b[34mgeneration_num_beams=None,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=4,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=False,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=1e-05,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=-1,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/opt/ml/model/adgen-chatglm-6b-ft/runs/May19_01-01-07_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=10,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=linear,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=50,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=3.0,\u001b[0m\n",
      "\u001b[34moptim=adamw_hf,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moutput_dir=/opt/ml/model/adgen-chatglm-6b-ft,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=True,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=1,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=1,\u001b[0m\n",
      "\u001b[34mpredict_with_generate=True,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/opt/ml/model/adgen-chatglm-6b-ft,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_safetensors=False,\u001b[0m\n",
      "\u001b[34msave_steps=50,\u001b[0m\n",
      "\u001b[34msave_strategy=steps,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=42,\u001b[0m\n",
      "\u001b[34msharded_ddp=[],\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34msortish_sampler=False,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,\u001b[0m\n",
      "\u001b[34mxpu_backend=None,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m05/19/2023 01:01:08 - WARNING - datasets.builder - Using custom data configuration default-fd409d4a8656eec4\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-fd409d4a8656eec4/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 2/2 [00:00<00:00, 12069.94it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 2/2 [00:00<00:00, 1816.90it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 111750 examples [00:00, 911030.74 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating validation split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-fd409d4a8656eec4/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 545.03it/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/773 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 773/773 [00:00<00:00, 136kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:668] 2023-05-19 01:01:08,577 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/a10da4c68b5d616030d3531fc37a13bb44ea814d/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:668] 2023-05-19 01:01:08,577 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/a10da4c68b5d616030d3531fc37a13bb44ea814d/config.json\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-19 01:01:08,577 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-19 01:01:08,577 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mDownloading (…)iguration_chatglm.py:   0%|          | 0.00/4.28k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)iguration_chatglm.py: 100%|██████████| 4.28k/4.28k [00:00<00:00, 2.63MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:668] 2023-05-19 01:01:08,844 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/a10da4c68b5d616030d3531fc37a13bb44ea814d/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:668] 2023-05-19 01:01:08,844 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/a10da4c68b5d616030d3531fc37a13bb44ea814d/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:720] 2023-05-19 01:01:08,845 >> Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm-6b\",\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\"\n",
      "  },\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"gmask_token_id\": 130001,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"inner_hidden_size\": 16384,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"mask_token_id\": 130000,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_encoding_2d\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.28.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 130528\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:720] 2023-05-19 01:01:08,845 >> Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm-6b\",\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\"\n",
      "  },\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"gmask_token_id\": 130001,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"inner_hidden_size\": 16384,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"mask_token_id\": 130000,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_encoding_2d\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.28.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 130528\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json:   0%|          | 0.00/441 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json: 100%|██████████| 441/441 [00:00<00:00, 279kB/s]\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-19 01:01:09,020 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-19 01:01:09,020 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mDownloading (…)enization_chatglm.py:   0%|          | 0.00/17.0k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)enization_chatglm.py: 100%|██████████| 17.0k/17.0k [00:00<00:00, 10.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ice_text.model:   0%|          | 0.00/2.71M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ice_text.model: 100%|██████████| 2.71M/2.71M [00:00<00:00, 66.5MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1809] 2023-05-19 01:01:09,580 >> loading file ice_text.model from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/a10da4c68b5d616030d3531fc37a13bb44ea814d/ice_text.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1809] 2023-05-19 01:01:09,580 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1809] 2023-05-19 01:01:09,580 >> loading file ice_text.model from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/a10da4c68b5d616030d3531fc37a13bb44ea814d/ice_text.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1809] 2023-05-19 01:01:09,580 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1809] 2023-05-19 01:01:09,580 >> loading file special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1809] 2023-05-19 01:01:09,580 >> loading file special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1809] 2023-05-19 01:01:09,580 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/a10da4c68b5d616030d3531fc37a13bb44ea814d/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1809] 2023-05-19 01:01:09,580 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/a10da4c68b5d616030d3531fc37a13bb44ea814d/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-19 01:01:09,804 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-19 01:01:09,804 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mDownloading (…)/modeling_chatglm.py:   0%|          | 0.00/57.6k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/modeling_chatglm.py: 100%|██████████| 57.6k/57.6k [00:00<00:00, 852kB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)main/quantization.py:   0%|          | 0.00/15.1k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)main/quantization.py: 100%|██████████| 15.1k/15.1k [00:00<00:00, 9.15MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)model.bin.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)model.bin.index.json: 100%|██████████| 33.4k/33.4k [00:00<00:00, 468kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2534] 2023-05-19 01:01:10,728 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/a10da4c68b5d616030d3531fc37a13bb44ea814d/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2534] 2023-05-19 01:01:10,728 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/a10da4c68b5d616030d3531fc37a13bb44ea814d/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   0%|          | 0.00/1.74G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   2%|▏         | 31.5M/1.74G [00:00<00:05, 307MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   4%|▍         | 73.4M/1.74G [00:00<00:05, 332MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   7%|▋         | 115M/1.74G [00:00<00:04, 333MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   9%|▉         | 157M/1.74G [00:00<00:04, 341MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  11%|█▏        | 199M/1.74G [00:00<00:04, 347MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  14%|█▍        | 241M/1.74G [00:00<00:04, 348MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  16%|█▋        | 283M/1.74G [00:00<00:04, 357MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  19%|█▊        | 325M/1.74G [00:00<00:03, 358MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  21%|██        | 367M/1.74G [00:01<00:03, 345MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  23%|██▎       | 409M/1.74G [00:01<00:03, 351MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  26%|██▌       | 451M/1.74G [00:01<00:03, 356MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  28%|██▊       | 493M/1.74G [00:01<00:03, 354MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  31%|███       | 535M/1.74G [00:01<00:03, 351MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  33%|███▎      | 577M/1.74G [00:01<00:03, 349MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  36%|███▌      | 619M/1.74G [00:01<00:03, 344MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  38%|███▊      | 661M/1.74G [00:01<00:03, 344MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  40%|████      | 703M/1.74G [00:02<00:03, 345MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  43%|████▎     | 744M/1.74G [00:02<00:02, 350MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  45%|████▌     | 786M/1.74G [00:02<00:02, 350MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  48%|████▊     | 828M/1.74G [00:02<00:02, 349MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  50%|████▉     | 870M/1.74G [00:02<00:02, 346MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  52%|█████▏    | 912M/1.74G [00:02<00:02, 344MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  55%|█████▍    | 954M/1.74G [00:02<00:02, 344MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  57%|█████▋    | 996M/1.74G [00:02<00:02, 346MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  60%|█████▉    | 1.04G/1.74G [00:02<00:02, 351MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  62%|██████▏   | 1.08G/1.74G [00:03<00:01, 353MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  64%|██████▍   | 1.12G/1.74G [00:03<00:01, 356MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  67%|██████▋   | 1.16G/1.74G [00:03<00:01, 355MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  69%|██████▉   | 1.21G/1.74G [00:03<00:01, 357MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  72%|███████▏  | 1.25G/1.74G [00:03<00:01, 355MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  74%|███████▍  | 1.29G/1.74G [00:03<00:01, 351MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  77%|███████▋  | 1.33G/1.74G [00:03<00:01, 355MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  79%|███████▉  | 1.37G/1.74G [00:03<00:01, 355MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  81%|████████▏ | 1.42G/1.74G [00:04<00:00, 360MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  84%|████████▎ | 1.46G/1.74G [00:04<00:00, 359MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  86%|████████▌ | 1.50G/1.74G [00:04<00:00, 356MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  89%|████████▊ | 1.54G/1.74G [00:04<00:00, 356MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  91%|█████████ | 1.58G/1.74G [00:04<00:00, 355MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  93%|█████████▎| 1.63G/1.74G [00:04<00:00, 358MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  96%|█████████▌| 1.67G/1.74G [00:04<00:00, 363MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  98%|█████████▊| 1.71G/1.74G [00:04<00:00, 361MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin: 100%|██████████| 1.74G/1.74G [00:04<00:00, 351MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  12%|█▎        | 1/8 [00:05<00:35,  5.05s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   1%|          | 21.0M/1.88G [00:00<00:12, 155MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   2%|▏         | 41.9M/1.88G [00:00<00:10, 183MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   4%|▍         | 83.9M/1.88G [00:00<00:06, 264MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   7%|▋         | 126M/1.88G [00:00<00:05, 299MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   9%|▉         | 168M/1.88G [00:00<00:05, 317MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  11%|█         | 210M/1.88G [00:00<00:05, 326MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  13%|█▎        | 252M/1.88G [00:00<00:04, 331MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  16%|█▌        | 294M/1.88G [00:00<00:04, 333MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  18%|█▊        | 336M/1.88G [00:01<00:04, 340MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  20%|██        | 377M/1.88G [00:01<00:04, 344MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  22%|██▏       | 419M/1.88G [00:01<00:04, 352MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  25%|██▍       | 461M/1.88G [00:01<00:04, 354MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  27%|██▋       | 503M/1.88G [00:01<00:03, 357MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  29%|██▉       | 545M/1.88G [00:01<00:03, 361MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  31%|███       | 587M/1.88G [00:01<00:03, 364MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  33%|███▎      | 629M/1.88G [00:01<00:03, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  36%|███▌      | 671M/1.88G [00:01<00:03, 370MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  38%|███▊      | 713M/1.88G [00:02<00:03, 370MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  40%|████      | 755M/1.88G [00:02<00:03, 370MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  42%|████▏     | 797M/1.88G [00:02<00:02, 372MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  45%|████▍     | 839M/1.88G [00:02<00:02, 369MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  47%|████▋     | 881M/1.88G [00:02<00:02, 368MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  49%|████▉     | 923M/1.88G [00:02<00:02, 364MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  51%|█████▏    | 965M/1.88G [00:02<00:02, 360MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  54%|█████▎    | 1.01G/1.88G [00:02<00:02, 359MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  56%|█████▌    | 1.05G/1.88G [00:03<00:02, 360MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  58%|█████▊    | 1.09G/1.88G [00:03<00:02, 363MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  60%|██████    | 1.13G/1.88G [00:03<00:02, 362MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  62%|██████▏   | 1.17G/1.88G [00:03<00:01, 365MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  65%|██████▍   | 1.22G/1.88G [00:03<00:01, 364MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  67%|██████▋   | 1.26G/1.88G [00:03<00:01, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  69%|██████▉   | 1.30G/1.88G [00:03<00:01, 367MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  71%|███████▏  | 1.34G/1.88G [00:03<00:01, 369MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  74%|███████▎  | 1.38G/1.88G [00:03<00:01, 368MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  76%|███████▌  | 1.43G/1.88G [00:04<00:01, 367MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  78%|███████▊  | 1.47G/1.88G [00:04<00:01, 367MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  80%|████████  | 1.51G/1.88G [00:04<00:01, 365MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  83%|████████▎ | 1.55G/1.88G [00:04<00:00, 365MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  85%|████████▍ | 1.59G/1.88G [00:04<00:00, 365MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  87%|████████▋ | 1.64G/1.88G [00:04<00:00, 361MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  89%|████████▉ | 1.68G/1.88G [00:04<00:00, 346MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  91%|█████████▏| 1.72G/1.88G [00:04<00:00, 348MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  94%|█████████▎| 1.76G/1.88G [00:05<00:00, 352MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  96%|█████████▌| 1.80G/1.88G [00:05<00:00, 351MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  98%|█████████▊| 1.85G/1.88G [00:05<00:00, 346MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin: 100%|██████████| 1.88G/1.88G [00:05<00:00, 351MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 2/8 [00:10<00:31,  5.29s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   1%|          | 21.0M/1.98G [00:00<00:12, 158MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   3%|▎         | 52.4M/1.98G [00:00<00:09, 207MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   5%|▍         | 94.4M/1.98G [00:00<00:07, 263MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   7%|▋         | 136M/1.98G [00:00<00:06, 291MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   9%|▉         | 178M/1.98G [00:00<00:05, 305MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  11%|█         | 220M/1.98G [00:00<00:05, 316MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  13%|█▎        | 262M/1.98G [00:00<00:05, 327MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  15%|█▌        | 304M/1.98G [00:01<00:04, 338MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  17%|█▋        | 346M/1.98G [00:01<00:04, 340MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  20%|█▉        | 388M/1.98G [00:01<00:04, 344MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  22%|██▏       | 430M/1.98G [00:01<00:06, 227MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  24%|██▍       | 472M/1.98G [00:01<00:05, 255MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  26%|██▌       | 514M/1.98G [00:01<00:05, 279MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  28%|██▊       | 556M/1.98G [00:01<00:04, 299MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  30%|███       | 598M/1.98G [00:02<00:04, 316MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  32%|███▏      | 640M/1.98G [00:02<00:04, 327MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  34%|███▍      | 682M/1.98G [00:02<00:03, 337MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  37%|███▋      | 724M/1.98G [00:02<00:03, 346MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  39%|███▊      | 765M/1.98G [00:02<00:03, 352MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  41%|████      | 807M/1.98G [00:02<00:03, 353MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  43%|████▎     | 849M/1.98G [00:02<00:03, 356MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  45%|████▌     | 891M/1.98G [00:02<00:03, 360MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  47%|████▋     | 933M/1.98G [00:02<00:02, 358MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  49%|████▉     | 975M/1.98G [00:03<00:02, 359MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  51%|█████▏    | 1.02G/1.98G [00:03<00:02, 361MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  53%|█████▎    | 1.06G/1.98G [00:03<00:02, 358MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  56%|█████▌    | 1.10G/1.98G [00:03<00:02, 358MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  58%|█████▊    | 1.14G/1.98G [00:03<00:02, 362MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  60%|█████▉    | 1.18G/1.98G [00:03<00:02, 365MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  62%|██████▏   | 1.23G/1.98G [00:03<00:02, 367MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  64%|██████▍   | 1.27G/1.98G [00:03<00:01, 364MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  66%|██████▌   | 1.31G/1.98G [00:04<00:01, 364MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  68%|██████▊   | 1.35G/1.98G [00:04<00:01, 358MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  70%|███████   | 1.39G/1.98G [00:04<00:01, 353MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  73%|███████▎  | 1.44G/1.98G [00:04<00:01, 349MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  75%|███████▍  | 1.48G/1.98G [00:04<00:01, 343MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  77%|███████▋  | 1.52G/1.98G [00:04<00:01, 342MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  79%|███████▉  | 1.56G/1.98G [00:04<00:01, 342MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  81%|████████  | 1.60G/1.98G [00:04<00:01, 335MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  83%|████████▎ | 1.65G/1.98G [00:04<00:00, 344MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  85%|████████▌ | 1.69G/1.98G [00:05<00:00, 348MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  87%|████████▋ | 1.73G/1.98G [00:05<00:00, 355MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  89%|████████▉ | 1.77G/1.98G [00:05<00:00, 360MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  92%|█████████▏| 1.81G/1.98G [00:05<00:00, 358MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  94%|█████████▎| 1.86G/1.98G [00:05<00:00, 361MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  96%|█████████▌| 1.90G/1.98G [00:05<00:00, 351MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  98%|█████████▊| 1.94G/1.98G [00:05<00:00, 355MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin: 100%|██████████| 1.98G/1.98G [00:05<00:00, 360MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin: 100%|██████████| 1.98G/1.98G [00:05<00:00, 335MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  38%|███▊      | 3/8 [00:16<00:28,  5.63s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   0%|          | 0.00/1.91G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   2%|▏         | 41.9M/1.91G [00:00<00:05, 333MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   4%|▍         | 83.9M/1.91G [00:00<00:05, 355MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   7%|▋         | 126M/1.91G [00:00<00:05, 357MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   9%|▉         | 168M/1.91G [00:00<00:04, 359MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  11%|█         | 210M/1.91G [00:00<00:04, 363MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  13%|█▎        | 252M/1.91G [00:00<00:04, 365MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  15%|█▌        | 294M/1.91G [00:00<00:04, 365MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  18%|█▊        | 336M/1.91G [00:00<00:04, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  20%|█▉        | 377M/1.91G [00:01<00:04, 370MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  22%|██▏       | 419M/1.91G [00:01<00:04, 319MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  24%|██▍       | 461M/1.91G [00:01<00:05, 273MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  26%|██▌       | 493M/1.91G [00:01<00:05, 251MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  27%|██▋       | 524M/1.91G [00:01<00:05, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  29%|██▉       | 556M/1.91G [00:01<00:05, 227MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  31%|███       | 587M/1.91G [00:02<00:06, 218MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  32%|███▏      | 619M/1.91G [00:02<00:05, 216MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  34%|███▍      | 650M/1.91G [00:02<00:05, 212MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  36%|███▌      | 682M/1.91G [00:02<00:05, 212MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  37%|███▋      | 713M/1.91G [00:02<00:05, 210MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  39%|███▉      | 744M/1.91G [00:02<00:05, 207MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  40%|████      | 765M/1.91G [00:02<00:05, 205MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  42%|████▏     | 797M/1.91G [00:03<00:05, 209MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  43%|████▎     | 818M/1.91G [00:03<00:05, 205MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  44%|████▍     | 849M/1.91G [00:03<00:05, 208MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  45%|████▌     | 870M/1.91G [00:03<00:05, 204MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  47%|████▋     | 891M/1.91G [00:03<00:04, 205MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  48%|████▊     | 912M/1.91G [00:03<00:04, 204MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  49%|████▉     | 933M/1.91G [00:03<00:04, 201MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  50%|█████     | 965M/1.91G [00:03<00:04, 207MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  52%|█████▏    | 986M/1.91G [00:03<00:04, 195MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  53%|█████▎    | 1.02G/1.91G [00:04<00:04, 209MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  54%|█████▍    | 1.04G/1.91G [00:04<00:04, 204MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  55%|█████▌    | 1.06G/1.91G [00:04<00:04, 203MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  56%|█████▋    | 1.08G/1.91G [00:04<00:04, 205MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  58%|█████▊    | 1.10G/1.91G [00:04<00:03, 205MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  59%|█████▉    | 1.13G/1.91G [00:04<00:03, 204MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  60%|██████    | 1.15G/1.91G [00:04<00:03, 196MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  62%|██████▏   | 1.18G/1.91G [00:04<00:03, 197MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  63%|██████▎   | 1.21G/1.91G [00:05<00:03, 179MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  64%|██████▍   | 1.23G/1.91G [00:05<00:03, 177MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  66%|██████▌   | 1.26G/1.91G [00:05<00:03, 196MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  67%|██████▋   | 1.29G/1.91G [00:05<00:03, 207MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  69%|██████▊   | 1.31G/1.91G [00:05<00:02, 206MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  70%|███████   | 1.34G/1.91G [00:05<00:02, 226MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  72%|███████▏  | 1.37G/1.91G [00:05<00:02, 224MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  73%|███████▎  | 1.41G/1.91G [00:06<00:02, 217MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  75%|███████▌  | 1.44G/1.91G [00:06<00:02, 202MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  77%|███████▋  | 1.47G/1.91G [00:06<00:02, 202MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  78%|███████▊  | 1.50G/1.91G [00:06<00:02, 202MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  79%|███████▉  | 1.52G/1.91G [00:06<00:02, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  81%|████████  | 1.54G/1.91G [00:06<00:02, 177MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  82%|████████▏ | 1.57G/1.91G [00:06<00:01, 191MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  84%|████████▍ | 1.60G/1.91G [00:07<00:01, 183MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  85%|████████▍ | 1.63G/1.91G [00:07<00:01, 185MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  87%|████████▋ | 1.66G/1.91G [00:07<00:01, 192MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  88%|████████▊ | 1.68G/1.91G [00:07<00:01, 188MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  89%|████████▉ | 1.71G/1.91G [00:07<00:01, 195MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  91%|█████████ | 1.74G/1.91G [00:07<00:00, 206MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  93%|█████████▎| 1.77G/1.91G [00:07<00:00, 200MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  94%|█████████▍| 1.80G/1.91G [00:08<00:00, 209MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  96%|█████████▌| 1.84G/1.91G [00:08<00:00, 179MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  97%|█████████▋| 1.86G/1.91G [00:08<00:00, 177MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  98%|█████████▊| 1.88G/1.91G [00:08<00:00, 181MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  99%|█████████▉| 1.90G/1.91G [00:08<00:00, 153MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin: 100%|██████████| 1.91G/1.91G [00:08<00:00, 215MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 4/8 [00:25<00:27,  6.96s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   1%|          | 21.0M/1.88G [00:00<00:14, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   2%|▏         | 41.9M/1.88G [00:00<00:11, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   4%|▍         | 73.4M/1.88G [00:00<00:08, 212MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   6%|▌         | 105M/1.88G [00:00<00:09, 196MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   7%|▋         | 126M/1.88G [00:00<00:10, 159MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   8%|▊         | 147M/1.88G [00:00<00:11, 151MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   9%|▉         | 168M/1.88G [00:01<00:10, 158MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  10%|█         | 189M/1.88G [00:01<00:12, 138MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  11%|█         | 210M/1.88G [00:01<00:12, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  12%|█▏        | 231M/1.88G [00:01<00:11, 150MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  13%|█▎        | 252M/1.88G [00:01<00:12, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  15%|█▍        | 273M/1.88G [00:01<00:12, 125MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  16%|█▌        | 294M/1.88G [00:01<00:11, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  17%|█▋        | 315M/1.88G [00:02<00:11, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  18%|█▊        | 336M/1.88G [00:02<00:12, 125MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  19%|█▉        | 357M/1.88G [00:02<00:10, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  20%|██        | 377M/1.88G [00:02<00:10, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  21%|██        | 398M/1.88G [00:02<00:11, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  22%|██▏       | 419M/1.88G [00:02<00:10, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  23%|██▎       | 440M/1.88G [00:03<00:10, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  25%|██▍       | 461M/1.88G [00:03<00:11, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  26%|██▌       | 482M/1.88G [00:03<00:10, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  27%|██▋       | 503M/1.88G [00:03<00:09, 146MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  28%|██▊       | 524M/1.88G [00:03<00:10, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  29%|██▉       | 545M/1.88G [00:03<00:10, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  30%|███       | 566M/1.88G [00:03<00:09, 146MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  31%|███       | 587M/1.88G [00:04<00:09, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  32%|███▏      | 608M/1.88G [00:04<00:10, 124MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  33%|███▎      | 629M/1.88G [00:04<00:09, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  35%|███▍      | 650M/1.88G [00:04<00:09, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  36%|███▌      | 671M/1.88G [00:04<00:09, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  37%|███▋      | 692M/1.88G [00:04<00:08, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  38%|███▊      | 713M/1.88G [00:05<00:08, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  39%|███▉      | 734M/1.88G [00:05<00:12, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  40%|████      | 755M/1.88G [00:05<00:11, 101MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  41%|████▏     | 776M/1.88G [00:05<00:09, 111MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  42%|████▏     | 797M/1.88G [00:05<00:08, 125MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  44%|████▎     | 818M/1.88G [00:06<00:09, 116MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  45%|████▍     | 839M/1.88G [00:06<00:08, 121MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  46%|████▌     | 860M/1.88G [00:06<00:08, 122MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  47%|████▋     | 881M/1.88G [00:06<00:07, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  48%|████▊     | 902M/1.88G [00:06<00:07, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  49%|████▉     | 923M/1.88G [00:06<00:06, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  50%|█████     | 944M/1.88G [00:07<00:06, 151MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  51%|█████▏    | 965M/1.88G [00:07<00:05, 159MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  52%|█████▏    | 986M/1.88G [00:07<00:05, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  54%|█████▍    | 1.02G/1.88G [00:07<00:04, 176MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  55%|█████▌    | 1.04G/1.88G [00:07<00:04, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  56%|█████▋    | 1.06G/1.88G [00:07<00:05, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  57%|█████▋    | 1.08G/1.88G [00:07<00:04, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  59%|█████▉    | 1.11G/1.88G [00:07<00:04, 185MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  61%|██████    | 1.14G/1.88G [00:08<00:03, 191MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  62%|██████▏   | 1.16G/1.88G [00:08<00:04, 153MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  63%|██████▎   | 1.18G/1.88G [00:08<00:05, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  64%|██████▍   | 1.21G/1.88G [00:08<00:05, 126MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  65%|██████▌   | 1.23G/1.88G [00:08<00:05, 123MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  66%|██████▋   | 1.25G/1.88G [00:09<00:04, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  67%|██████▋   | 1.27G/1.88G [00:09<00:04, 123MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  69%|██████▊   | 1.29G/1.88G [00:09<00:04, 127MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  70%|██████▉   | 1.31G/1.88G [00:09<00:03, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  71%|███████   | 1.33G/1.88G [00:09<00:04, 125MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  72%|███████▏  | 1.35G/1.88G [00:09<00:04, 119MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  73%|███████▎  | 1.37G/1.88G [00:10<00:04, 123MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  74%|███████▍  | 1.39G/1.88G [00:10<00:03, 126MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  75%|███████▌  | 1.42G/1.88G [00:10<00:03, 122MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  76%|███████▋  | 1.44G/1.88G [00:10<00:03, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  78%|███████▊  | 1.46G/1.88G [00:10<00:03, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  79%|███████▊  | 1.48G/1.88G [00:10<00:03, 122MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  80%|███████▉  | 1.50G/1.88G [00:10<00:02, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  81%|████████  | 1.52G/1.88G [00:11<00:02, 139MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  82%|████████▏ | 1.54G/1.88G [00:11<00:02, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  83%|████████▎ | 1.56G/1.88G [00:11<00:02, 138MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  84%|████████▍ | 1.58G/1.88G [00:11<00:02, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  85%|████████▌ | 1.60G/1.88G [00:11<00:02, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  86%|████████▋ | 1.63G/1.88G [00:11<00:01, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  88%|████████▊ | 1.65G/1.88G [00:12<00:01, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  89%|████████▊ | 1.67G/1.88G [00:12<00:01, 127MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  90%|████████▉ | 1.69G/1.88G [00:12<00:01, 132MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  91%|█████████ | 1.71G/1.88G [00:12<00:01, 146MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  92%|█████████▏| 1.73G/1.88G [00:12<00:01, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  93%|█████████▎| 1.75G/1.88G [00:12<00:00, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  94%|█████████▍| 1.77G/1.88G [00:12<00:00, 145MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  95%|█████████▌| 1.79G/1.88G [00:13<00:00, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  97%|█████████▋| 1.81G/1.88G [00:13<00:00, 124MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  98%|█████████▊| 1.85G/1.88G [00:13<00:00, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  99%|█████████▉| 1.87G/1.88G [00:13<00:00, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin: 100%|██████████| 1.88G/1.88G [00:13<00:00, 135MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  62%|██████▎   | 5/8 [00:39<00:28,  9.49s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   2%|▏         | 31.5M/1.88G [00:00<00:06, 285MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   3%|▎         | 62.9M/1.88G [00:00<00:10, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   4%|▍         | 83.9M/1.88G [00:00<00:10, 177MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   6%|▌         | 105M/1.88G [00:00<00:10, 169MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   7%|▋         | 126M/1.88G [00:00<00:12, 139MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   8%|▊         | 147M/1.88G [00:00<00:12, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   9%|▉         | 168M/1.88G [00:01<00:11, 150MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  10%|█         | 189M/1.88G [00:01<00:12, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  11%|█         | 210M/1.88G [00:01<00:12, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  12%|█▏        | 231M/1.88G [00:01<00:11, 148MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  13%|█▎        | 252M/1.88G [00:01<00:14, 110MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  16%|█▌        | 294M/1.88G [00:01<00:09, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  17%|█▋        | 315M/1.88G [00:02<00:10, 147MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  18%|█▊        | 336M/1.88G [00:02<00:11, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  19%|█▉        | 357M/1.88G [00:02<00:10, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  20%|██        | 377M/1.88G [00:02<00:10, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  21%|██        | 398M/1.88G [00:02<00:11, 124MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  23%|██▎       | 430M/1.88G [00:02<00:09, 147MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  24%|██▍       | 451M/1.88G [00:03<00:10, 132MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  25%|██▌       | 472M/1.88G [00:03<00:11, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  26%|██▌       | 493M/1.88G [00:03<00:09, 139MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  27%|██▋       | 514M/1.88G [00:03<00:10, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  28%|██▊       | 535M/1.88G [00:03<00:10, 123MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  30%|██▉       | 556M/1.88G [00:03<00:09, 138MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  31%|███       | 577M/1.88G [00:04<00:09, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  32%|███▏      | 598M/1.88G [00:04<00:10, 124MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  33%|███▎      | 619M/1.88G [00:04<00:09, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  34%|███▍      | 640M/1.88G [00:04<00:08, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  35%|███▌      | 661M/1.88G [00:04<00:09, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  36%|███▋      | 682M/1.88G [00:04<00:08, 145MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  37%|███▋      | 703M/1.88G [00:04<00:07, 152MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  38%|███▊      | 724M/1.88G [00:05<00:08, 132MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  40%|███▉      | 744M/1.88G [00:05<00:08, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  41%|████      | 765M/1.88G [00:05<00:07, 141MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  42%|████▏     | 786M/1.88G [00:05<00:08, 132MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  43%|████▎     | 807M/1.88G [00:05<00:08, 123MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  44%|████▍     | 828M/1.88G [00:05<00:07, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  45%|████▌     | 849M/1.88G [00:06<00:07, 132MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  46%|████▋     | 870M/1.88G [00:06<00:08, 120MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  48%|████▊     | 902M/1.88G [00:06<00:06, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  49%|████▉     | 923M/1.88G [00:06<00:07, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  50%|█████     | 944M/1.88G [00:06<00:07, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  51%|█████▏    | 965M/1.88G [00:06<00:06, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  52%|█████▏    | 986M/1.88G [00:07<00:06, 132MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  54%|█████▎    | 1.01G/1.88G [00:07<00:07, 125MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  55%|█████▌    | 1.04G/1.88G [00:07<00:05, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  56%|█████▋    | 1.06G/1.88G [00:07<00:06, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  57%|█████▋    | 1.08G/1.88G [00:07<00:06, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  59%|█████▉    | 1.11G/1.88G [00:08<00:05, 141MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  60%|██████    | 1.13G/1.88G [00:08<00:05, 126MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  61%|██████▏   | 1.15G/1.88G [00:08<00:05, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  62%|██████▏   | 1.17G/1.88G [00:08<00:04, 141MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  64%|██████▎   | 1.20G/1.88G [00:08<00:05, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  65%|██████▍   | 1.22G/1.88G [00:08<00:05, 132MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  66%|██████▌   | 1.24G/1.88G [00:09<00:04, 145MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  67%|██████▋   | 1.26G/1.88G [00:09<00:04, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  68%|██████▊   | 1.28G/1.88G [00:09<00:04, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  69%|██████▉   | 1.30G/1.88G [00:09<00:04, 141MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  70%|███████   | 1.32G/1.88G [00:09<00:04, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  71%|███████▏  | 1.34G/1.88G [00:09<00:04, 124MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  73%|███████▎  | 1.37G/1.88G [00:10<00:03, 139MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  74%|███████▍  | 1.39G/1.88G [00:10<00:03, 127MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  75%|███████▌  | 1.42G/1.88G [00:10<00:03, 132MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  76%|███████▋  | 1.44G/1.88G [00:10<00:03, 139MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  78%|███████▊  | 1.46G/1.88G [00:10<00:03, 126MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  79%|███████▊  | 1.48G/1.88G [00:10<00:03, 132MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  80%|███████▉  | 1.50G/1.88G [00:10<00:02, 145MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  81%|████████  | 1.52G/1.88G [00:11<00:02, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  82%|████████▏ | 1.54G/1.88G [00:11<00:02, 126MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  84%|████████▎ | 1.57G/1.88G [00:11<00:02, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  85%|████████▍ | 1.59G/1.88G [00:11<00:02, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  86%|████████▌ | 1.61G/1.88G [00:11<00:02, 118MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  87%|████████▋ | 1.64G/1.88G [00:12<00:02, 114MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  88%|████████▊ | 1.66G/1.88G [00:12<00:02, 111MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  89%|████████▉ | 1.68G/1.88G [00:12<00:01, 109MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  90%|█████████ | 1.70G/1.88G [00:12<00:01, 107MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  91%|█████████▏| 1.72G/1.88G [00:12<00:01, 105MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  93%|█████████▎| 1.74G/1.88G [00:13<00:01, 104MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  93%|█████████▎| 1.75G/1.88G [00:13<00:01, 104MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  94%|█████████▍| 1.77G/1.88G [00:13<00:01, 104MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  95%|█████████▍| 1.78G/1.88G [00:13<00:00, 103MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  95%|█████████▌| 1.79G/1.88G [00:13<00:00, 103MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  96%|█████████▌| 1.80G/1.88G [00:13<00:00, 102MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  97%|█████████▋| 1.81G/1.88G [00:13<00:00, 101MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  98%|█████████▊| 1.84G/1.88G [00:14<00:00, 101MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  99%|█████████▊| 1.86G/1.88G [00:14<00:00, 103MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin: 100%|█████████▉| 1.88G/1.88G [00:14<00:00, 106MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin: 100%|██████████| 1.88G/1.88G [00:14<00:00, 129MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 6/8 [00:54<00:22, 11.27s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   0%|          | 0.00/1.07G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   3%|▎         | 31.5M/1.07G [00:00<00:04, 255MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   6%|▌         | 62.9M/1.07G [00:00<00:04, 233MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   9%|▉         | 94.4M/1.07G [00:00<00:06, 148MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  11%|█         | 115M/1.07G [00:00<00:06, 152MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  14%|█▎        | 147M/1.07G [00:00<00:05, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  16%|█▌        | 168M/1.07G [00:01<00:06, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  18%|█▊        | 189M/1.07G [00:01<00:06, 147MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  20%|█▉        | 210M/1.07G [00:01<00:05, 159MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  21%|██▏       | 231M/1.07G [00:01<00:05, 154MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  23%|██▎       | 252M/1.07G [00:01<00:05, 140MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  25%|██▌       | 273M/1.07G [00:01<00:05, 152MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  28%|██▊       | 304M/1.07G [00:01<00:04, 159MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  30%|███       | 325M/1.07G [00:02<00:05, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  32%|███▏      | 346M/1.07G [00:02<00:04, 152MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  35%|███▌      | 377M/1.07G [00:02<00:04, 154MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  37%|███▋      | 398M/1.07G [00:02<00:04, 147MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  40%|████      | 430M/1.07G [00:02<00:03, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  42%|████▏     | 451M/1.07G [00:02<00:03, 157MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  44%|████▍     | 472M/1.07G [00:03<00:04, 139MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  46%|████▌     | 493M/1.07G [00:03<00:03, 149MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  49%|████▉     | 524M/1.07G [00:03<00:03, 157MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  51%|█████     | 545M/1.07G [00:03<00:03, 139MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  53%|█████▎    | 566M/1.07G [00:03<00:03, 151MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  55%|█████▍    | 587M/1.07G [00:03<00:04, 117MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  59%|█████▊    | 629M/1.07G [00:04<00:02, 157MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  61%|██████    | 650M/1.07G [00:04<00:02, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  62%|██████▏   | 671M/1.07G [00:04<00:02, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  64%|██████▍   | 692M/1.07G [00:04<00:02, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  66%|██████▋   | 713M/1.07G [00:04<00:02, 146MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  69%|██████▉   | 744M/1.07G [00:04<00:02, 151MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  71%|███████▏  | 765M/1.07G [00:05<00:02, 132MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  73%|███████▎  | 786M/1.07G [00:05<00:01, 146MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  75%|███████▌  | 807M/1.07G [00:05<00:01, 154MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  77%|███████▋  | 828M/1.07G [00:05<00:01, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  79%|███████▉  | 849M/1.07G [00:05<00:01, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  82%|████████▏ | 881M/1.07G [00:05<00:01, 158MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  84%|████████▍ | 902M/1.07G [00:06<00:01, 138MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  86%|████████▌ | 923M/1.07G [00:06<00:01, 139MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  89%|████████▉ | 954M/1.07G [00:06<00:00, 155MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  91%|█████████ | 975M/1.07G [00:06<00:00, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  93%|█████████▎| 996M/1.07G [00:06<00:00, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  95%|█████████▍| 1.02G/1.07G [00:06<00:00, 155MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  97%|█████████▋| 1.04G/1.07G [00:06<00:00, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  99%|█████████▊| 1.06G/1.07G [00:07<00:00, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin: 100%|██████████| 1.07G/1.07G [00:07<00:00, 148MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  88%|████████▊ | 7/8 [01:01<00:09,  9.99s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:   0%|          | 0.00/1.07G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:   3%|▎         | 31.5M/1.07G [00:00<00:03, 306MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:   6%|▌         | 62.9M/1.07G [00:00<00:05, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:   8%|▊         | 83.9M/1.07G [00:00<00:05, 185MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  10%|▉         | 105M/1.07G [00:00<00:06, 149MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  12%|█▏        | 126M/1.07G [00:00<00:06, 144MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  14%|█▎        | 147M/1.07G [00:00<00:05, 157MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  16%|█▌        | 168M/1.07G [00:01<00:06, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  18%|█▊        | 189M/1.07G [00:01<00:06, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  20%|█▉        | 210M/1.07G [00:01<00:06, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  22%|██▏       | 231M/1.07G [00:01<00:05, 147MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  24%|██▎       | 252M/1.07G [00:01<00:06, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  25%|██▌       | 273M/1.07G [00:01<00:05, 139MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  27%|██▋       | 294M/1.07G [00:01<00:05, 148MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  29%|██▉       | 315M/1.07G [00:02<00:05, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  31%|███▏      | 336M/1.07G [00:02<00:05, 132MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  33%|███▎      | 357M/1.07G [00:02<00:04, 148MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  35%|███▌      | 377M/1.07G [00:02<00:05, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  37%|███▋      | 398M/1.07G [00:02<00:05, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  39%|███▉      | 419M/1.07G [00:02<00:04, 145MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  41%|████      | 440M/1.07G [00:03<00:04, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  43%|████▎     | 461M/1.07G [00:03<00:04, 141MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  46%|████▌     | 493M/1.07G [00:03<00:03, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  48%|████▊     | 514M/1.07G [00:03<00:03, 139MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  50%|█████     | 535M/1.07G [00:03<00:04, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  52%|█████▏    | 556M/1.07G [00:03<00:03, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  54%|█████▍    | 577M/1.07G [00:04<00:03, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  56%|█████▌    | 598M/1.07G [00:04<00:03, 123MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  58%|█████▊    | 619M/1.07G [00:04<00:03, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  60%|█████▉    | 640M/1.07G [00:04<00:03, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  62%|██████▏   | 661M/1.07G [00:04<00:03, 125MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  65%|██████▍   | 692M/1.07G [00:04<00:02, 148MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  67%|██████▋   | 713M/1.07G [00:05<00:02, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  69%|██████▊   | 734M/1.07G [00:05<00:02, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  71%|███████   | 755M/1.07G [00:05<00:02, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  73%|███████▎  | 776M/1.07G [00:05<00:02, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  75%|███████▍  | 797M/1.07G [00:05<00:02, 124MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  76%|███████▋  | 818M/1.07G [00:05<00:01, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  78%|███████▊  | 839M/1.07G [00:05<00:01, 141MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  80%|████████  | 860M/1.07G [00:06<00:01, 127MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  82%|████████▏ | 881M/1.07G [00:06<00:01, 139MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  84%|████████▍ | 902M/1.07G [00:06<00:01, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  86%|████████▋ | 923M/1.07G [00:06<00:01, 125MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  88%|████████▊ | 944M/1.07G [00:06<00:00, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  90%|█████████ | 965M/1.07G [00:06<00:00, 145MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  92%|█████████▏| 986M/1.07G [00:07<00:00, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  94%|█████████▍| 1.01G/1.07G [00:07<00:00, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  96%|█████████▌| 1.03G/1.07G [00:07<00:00, 148MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  98%|█████████▊| 1.05G/1.07G [00:07<00:00, 141MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin: 100%|██████████| 1.07G/1.07G [00:07<00:00, 126MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin: 100%|██████████| 1.07G/1.07G [00:07<00:00, 138MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [01:09<00:00,  9.32s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [01:09<00:00,  8.68s/it]\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:575] 2023-05-19 01:02:20,200 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"transformers_version\": \"4.28.0\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:575] 2023-05-19 01:02:20,200 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"transformers_version\": \"4.28.0\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▎        | 1/8 [00:17<02:02, 17.46s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 2/8 [00:36<01:49, 18.20s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  38%|███▊      | 3/8 [00:47<01:14, 14.96s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 4/8 [00:53<00:45, 11.34s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:54<00:23,  7.79s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:56<00:11,  5.65s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:56<00:04,  4.09s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:57<00:00,  3.08s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:57<00:00,  7.23s/it]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3190] 2023-05-19 01:03:18,456 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3190] 2023-05-19 01:03:18,456 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:3192] 2023-05-19 01:03:18,456 >> Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at THUDM/chatglm-6b and are newly initialized: ['transformer.prefix_encoder.embedding.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:3192] 2023-05-19 01:03:18,456 >> Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at THUDM/chatglm-6b and are newly initialized: ['transformer.prefix_encoder.embedding.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2839] 2023-05-19 01:03:18,597 >> Generation config file not found, using a generation config created from the model config.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2839] 2023-05-19 01:03:18,597 >> Generation config file not found, using a generation config created from the model config.\u001b[0m\n",
      "\u001b[34mQuantized to 4 bit\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   0%|          | 0/115 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   1%|          | 1/115 [00:00<01:34,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   2%|▏         | 2/115 [00:01<01:32,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 3/115 [00:02<01:31,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 4/115 [00:03<01:30,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   4%|▍         | 5/115 [00:04<01:29,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   5%|▌         | 6/115 [00:04<01:28,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   6%|▌         | 7/115 [00:05<01:27,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   7%|▋         | 8/115 [00:06<01:26,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   8%|▊         | 9/115 [00:07<01:26,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   9%|▊         | 10/115 [00:08<01:26,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|▉         | 11/115 [00:09<01:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|█         | 12/115 [00:09<01:24,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  11%|█▏        | 13/115 [00:10<01:22,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  12%|█▏        | 14/115 [00:11<01:22,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  13%|█▎        | 15/115 [00:12<01:21,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  14%|█▍        | 16/115 [00:13<01:20,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  15%|█▍        | 17/115 [00:13<01:19,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  16%|█▌        | 18/115 [00:14<01:19,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 19/115 [00:15<01:17,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 20/115 [00:16<01:16,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  18%|█▊        | 21/115 [00:17<01:15,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  19%|█▉        | 22/115 [00:17<01:14,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  20%|██        | 23/115 [00:18<01:13,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  21%|██        | 24/115 [00:19<01:13,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  22%|██▏       | 25/115 [00:20<01:12,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 26/115 [00:21<01:11,  1.25ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 27/115 [00:21<01:10,  1.25ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  24%|██▍       | 28/115 [00:22<01:09,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  25%|██▌       | 29/115 [00:23<01:09,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  26%|██▌       | 30/115 [00:24<01:08,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  27%|██▋       | 31/115 [00:25<01:08,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  28%|██▊       | 32/115 [00:25<01:07,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  29%|██▊       | 33/115 [00:26<01:06,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|██▉       | 34/115 [00:27<01:05,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|███       | 35/115 [00:28<01:04,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  31%|███▏      | 36/115 [00:29<01:03,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  32%|███▏      | 37/115 [00:30<01:03,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  33%|███▎      | 38/115 [00:30<01:03,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  34%|███▍      | 39/115 [00:31<01:02,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  35%|███▍      | 40/115 [00:32<01:01,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  36%|███▌      | 41/115 [00:33<01:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 42/115 [00:34<00:59,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 43/115 [00:34<00:59,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  38%|███▊      | 44/115 [00:35<00:57,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  39%|███▉      | 45/115 [00:36<00:56,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  40%|████      | 46/115 [00:37<00:55,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  41%|████      | 47/115 [00:38<00:55,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  42%|████▏     | 48/115 [00:38<00:54,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 49/115 [00:39<00:53,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 50/115 [00:40<00:52,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  44%|████▍     | 51/115 [00:41<00:52,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  45%|████▌     | 52/115 [00:42<00:52,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  46%|████▌     | 53/115 [00:43<00:51,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  47%|████▋     | 54/115 [00:43<00:50,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  48%|████▊     | 55/115 [00:44<00:49,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  49%|████▊     | 56/115 [00:45<00:48,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|████▉     | 57/115 [00:46<00:47,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|█████     | 58/115 [00:47<00:46,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  51%|█████▏    | 59/115 [00:47<00:45,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  52%|█████▏    | 60/115 [00:48<00:44,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  53%|█████▎    | 61/115 [00:49<00:43,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  54%|█████▍    | 62/115 [00:50<00:42,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  55%|█████▍    | 63/115 [00:51<00:41,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  56%|█████▌    | 64/115 [00:52<00:41,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 65/115 [00:52<00:40,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 66/115 [00:53<00:40,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  58%|█████▊    | 67/115 [00:54<00:39,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  59%|█████▉    | 68/115 [00:55<00:38,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  60%|██████    | 69/115 [00:56<00:37,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  61%|██████    | 70/115 [00:56<00:36,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  62%|██████▏   | 71/115 [00:57<00:35,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 72/115 [00:58<00:34,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 73/115 [00:59<00:34,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  64%|██████▍   | 74/115 [01:00<00:33,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  65%|██████▌   | 75/115 [01:00<00:32,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  66%|██████▌   | 76/115 [01:01<00:31,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  67%|██████▋   | 77/115 [01:02<00:30,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  68%|██████▊   | 78/115 [01:03<00:29,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  69%|██████▊   | 79/115 [01:04<00:29,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|██████▉   | 80/115 [01:04<00:28,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|███████   | 81/115 [01:05<00:27,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  71%|███████▏  | 82/115 [01:06<00:26,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  72%|███████▏  | 83/115 [01:07<00:25,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  73%|███████▎  | 84/115 [01:08<00:24,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  74%|███████▍  | 85/115 [01:09<00:24,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  75%|███████▍  | 86/115 [01:09<00:23,  1.25ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  76%|███████▌  | 87/115 [01:10<00:22,  1.25ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 88/115 [01:11<00:21,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 89/115 [01:12<00:21,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  78%|███████▊  | 90/115 [01:13<00:20,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  79%|███████▉  | 91/115 [01:13<00:19,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  80%|████████  | 92/115 [01:14<00:18,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  81%|████████  | 93/115 [01:15<00:18,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  82%|████████▏ | 94/115 [01:16<00:17,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 95/115 [01:17<00:16,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 96/115 [01:18<00:15,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  84%|████████▍ | 97/115 [01:18<00:14,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  85%|████████▌ | 98/115 [01:19<00:13,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  86%|████████▌ | 99/115 [01:20<00:13,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  87%|████████▋ | 100/115 [01:21<00:12,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  88%|████████▊ | 101/115 [01:22<00:11,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  89%|████████▊ | 102/115 [01:22<00:10,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|████████▉ | 103/115 [01:23<00:09,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|█████████ | 104/115 [01:24<00:08,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  91%|█████████▏| 105/115 [01:25<00:08,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  92%|█████████▏| 106/115 [01:26<00:07,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  93%|█████████▎| 107/115 [01:27<00:06,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  94%|█████████▍| 108/115 [01:27<00:05,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  95%|█████████▍| 109/115 [01:28<00:04,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  96%|█████████▌| 110/115 [01:29<00:04,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 111/115 [01:30<00:03,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 112/115 [01:31<00:02,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  98%|█████████▊| 113/115 [01:31<00:01,  1.19ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  99%|█████████▉| 114/115 [01:32<00:00,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:33<00:00,  1.38ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:33<00:00,  1.23ba/s]\u001b[0m\n",
      "\u001b[34minput_ids\u001b[0m\n",
      "\u001b[34m[5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\u001b[0m\n",
      "\u001b[34minputs 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[34mlabel_ids\u001b[0m\n",
      "\u001b[34m[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[34mlabels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m0%|          | 0/50 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-05-19 01:07:05.160: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-19 01:07:05.198 algo-1:159 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-19 01:07:05.235 algo-1:159 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-19 01:07:05.235 algo-1:159 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-19 01:07:05.235 algo-1:159 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-19 01:07:05.236 algo-1:159 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-19 01:07:05.236 algo-1:159 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m05/19/2023 01:07:05 - WARNING - transformers_modules.THUDM.chatglm-6b.a10da4c68b5d616030d3531fc37a13bb44ea814d.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m2%|▏         | 1/50 [00:05<04:27,  5.47s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 2/50 [00:08<03:05,  3.87s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 3/50 [00:10<02:37,  3.36s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 4/50 [00:13<02:23,  3.12s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 5/50 [00:16<02:14,  2.99s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 6/50 [00:19<02:08,  2.91s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 7/50 [00:22<02:03,  2.87s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 8/50 [00:24<01:59,  2.83s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 9/50 [00:27<01:55,  2.81s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 10/50 [00:30<01:52,  2.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 5.8084, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m20%|██        | 10/50 [00:30<01:52,  2.80s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 11/50 [00:33<01:48,  2.79s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 12/50 [00:35<01:45,  2.79s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 13/50 [00:38<01:43,  2.78s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 14/50 [00:41<01:40,  2.78s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 15/50 [00:44<01:37,  2.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 16/50 [00:46<01:34,  2.78s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 17/50 [00:49<01:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 18/50 [00:52<01:28,  2.78s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 19/50 [00:55<01:26,  2.78s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 20/50 [00:58<01:23,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 5.8998, 'learning_rate': 6e-06, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m40%|████      | 20/50 [00:58<01:23,  2.78s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 21/50 [01:00<01:20,  2.78s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 22/50 [01:03<01:17,  2.78s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 23/50 [01:06<01:15,  2.79s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 24/50 [01:09<01:13,  2.81s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 25/50 [01:12<01:10,  2.82s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 26/50 [01:15<01:07,  2.83s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 27/50 [01:17<01:05,  2.84s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 28/50 [01:20<01:02,  2.85s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 29/50 [01:23<00:59,  2.85s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 30/50 [01:26<00:57,  2.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 6.0313, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m60%|██████    | 30/50 [01:26<00:57,  2.86s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 31/50 [01:29<00:54,  2.86s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 32/50 [01:32<00:51,  2.86s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 33/50 [01:35<00:48,  2.86s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 34/50 [01:37<00:45,  2.86s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 35/50 [01:40<00:42,  2.87s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 36/50 [01:43<00:40,  2.87s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 37/50 [01:46<00:37,  2.87s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 38/50 [01:49<00:34,  2.87s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 39/50 [01:52<00:31,  2.88s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 40/50 [01:55<00:28,  2.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 5.8613, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m80%|████████  | 40/50 [01:55<00:28,  2.88s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 41/50 [01:58<00:26,  2.89s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 42/50 [02:01<00:23,  2.89s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 43/50 [02:03<00:20,  2.89s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 44/50 [02:06<00:17,  2.89s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 45/50 [02:09<00:14,  2.89s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 46/50 [02:12<00:11,  2.89s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 47/50 [02:15<00:08,  2.89s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 48/50 [02:18<00:05,  2.89s/it]\u001b[0m\n",
      "\n",
      "2023-05-19 01:09:38 Uploading - Uploading generated training model\u001b[34m98%|█████████▊| 49/50 [02:21<00:02,  2.89s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 50/50 [02:24<00:00,  2.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 5.5462, 'learning_rate': 0.0, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 50/50 [02:24<00:00,  2.90s/it]\u001b[0m\n",
      "\u001b[34mSaving PrefixEncoder\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-05-19 01:09:29,242 >> Configuration saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-05-19 01:09:29,242 >> Configuration saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-05-19 01:09:29,242 >> Configuration saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-05-19 01:09:29,242 >> Configuration saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1847] 2023-05-19 01:09:29,436 >> Model weights saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1847] 2023-05-19 01:09:29,436 >> Model weights saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-05-19 01:09:29,438 >> tokenizer config file saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-05-19 01:09:29,438 >> tokenizer config file saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-05-19 01:09:29,438 >> Special tokens file saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-05-19 01:09:29,438 >> Special tokens file saved in /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m{'train_runtime': 144.8177, 'train_samples_per_second': 1.381, 'train_steps_per_second': 0.345, 'train_loss': 5.8294140625, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 50/50 [02:24<00:00,  2.90s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 50/50 [02:24<00:00,  2.90s/it]\u001b[0m\n",
      "\u001b[34m***** train metrics *****\u001b[0m\n",
      "\u001b[34mepoch                    =        0.0\n",
      "  train_loss               =     5.8294\n",
      "  train_runtime            = 0:02:24.81\n",
      "  train_samples            =     114599\n",
      "  train_samples_per_second =      1.381\n",
      "  train_steps_per_second   =      0.345\u001b[0m\n",
      "\u001b[34m------saving model!-----\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-05-19 01:09:29,842 >> tokenizer config file saved in /opt/ml/model/adgen-chatglm-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-05-19 01:09:29,842 >> tokenizer config file saved in /opt/ml/model/adgen-chatglm-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-05-19 01:09:29,842 >> Special tokens file saved in /opt/ml/model/adgen-chatglm-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-05-19 01:09:29,842 >> Special tokens file saved in /opt/ml/model/adgen-chatglm-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSaving PrefixEncoder\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-05-19 01:09:29,851 >> Configuration saved in /opt/ml/model/adgen-chatglm-6b-ft/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-05-19 01:09:29,851 >> Configuration saved in /opt/ml/model/adgen-chatglm-6b-ft/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-05-19 01:09:29,851 >> Configuration saved in /opt/ml/model/adgen-chatglm-6b-ft/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-05-19 01:09:29,851 >> Configuration saved in /opt/ml/model/adgen-chatglm-6b-ft/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1847] 2023-05-19 01:09:30,045 >> Model weights saved in /opt/ml/model/adgen-chatglm-6b-ft/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1847] 2023-05-19 01:09:30,045 >> Model weights saved in /opt/ml/model/adgen-chatglm-6b-ft/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-05-19 01:09:30,047 >> tokenizer config file saved in /opt/ml/model/adgen-chatglm-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-05-19 01:09:30,047 >> tokenizer config file saved in /opt/ml/model/adgen-chatglm-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-05-19 01:09:30,047 >> Special tokens file saved in /opt/ml/model/adgen-chatglm-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-05-19 01:09:30,047 >> Special tokens file saved in /opt/ml/model/adgen-chatglm-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m------model is saved!-----\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/generation_config.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/checkpoint-50/generation_config.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/special_tokens_map.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/special_tokens_map.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/checkpoint-50/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/configuration_chatglm.py s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/configuration_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/training_args.bin s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/checkpoint-50/training_args.bin\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/trainer_state.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/trainer_state.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/generation_config.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/generation_config.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/config.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/config.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/trainer_state.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/checkpoint-50/trainer_state.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/tokenizer_config.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/configuration_chatglm.py s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/checkpoint-50/configuration_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/config.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/checkpoint-50/config.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/training_args.bin s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/training_args.bin\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/quantization.py s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/quantization.py\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/scheduler.pt s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/checkpoint-50/scheduler.pt\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/rng_state.pth s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/checkpoint-50/rng_state.pth\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/tokenization_chatglm.py s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/tokenization_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/train_results.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/train_results.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/tokenizer_config.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/checkpoint-50/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/tokenization_chatglm.py s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/checkpoint-50/tokenization_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/quantization.py s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/checkpoint-50/quantization.py\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/all_results.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/all_results.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/modeling_chatglm.py s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/checkpoint-50/modeling_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/modeling_chatglm.py s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/modeling_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/ice_text.model s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/checkpoint-50/ice_text.model\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/ice_text.model s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/ice_text.model\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/pytorch_model.bin s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/pytorch_model.bin s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/checkpoint-50/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/adgen-chatglm-6b-ft/checkpoint-50/optimizer.pt s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/checkpoint-50/optimizer.pt\u001b[0m\n",
      "\u001b[34m2023-05-19 01:09:32,812 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-05-19 01:09:32,813 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-05-19 01:09:32,813 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-05-19 01:10:43 Completed - Training job completed\n",
      "Training seconds: 849\n",
      "Billable seconds: 849\n"
     ]
    }
   ],
   "source": [
    "# define Training Job Name \n",
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "instance_type=\"ml.g4dn.2xlarge\"\n",
    "\n",
    "job_name = f'huggingface-chatglm-simple-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "#define the model s3 path which will store your trained model asset\n",
    "#Note: you should use your real s3 path to configure model_s3_path\n",
    "model_s3_path='s3://{}/llm/models/chatglm/simple/'.format(sagemaker_session.default_bucket())\n",
    "output_dir = '/opt/ml/model/adgen-chatglm-6b-ft'\n",
    "model_name_or_path = 'THUDM/chatglm-6b'\n",
    "\n",
    "\n",
    "instance_count = 1\n",
    "#define the enviroment variables for your scripts.\n",
    "environment = {\n",
    "              'MODEL_S3_PATH'          : model_uri,\n",
    "              'PYTORCH_CUDA_ALLOC_CONF': 'max_split_size_mb:32',\n",
    "              #'LD_LIBRARY_PATH'        : '${LD_LIBRARY_PATH}:/opt/conda/lib/',\n",
    "              'TRAIN_DATASET'          : '/opt/ml/input/data/AdvertiseGen/train.json',\n",
    "              'TEST_DATASET'           : '/opt/ml/input/data/AdvertiseGen/dev.json',\n",
    "              'PROMPT_COLUMN'          : 'content',\n",
    "              'RESPONSE_COLUMN'        : 'summary',\n",
    "              'MODEL_NAME_OR_PATH'     : model_name_or_path,\n",
    "              'OUTPUT_DIR'             : output_dir,\n",
    "              'MODEL_OUTPUT_S3_PATH'   : model_s3_path,\n",
    "              'TRAIN_STEPS'            : '50'\n",
    "}\n",
    "\n",
    "inputs={\n",
    "   'AdvertiseGen': f\"s3://{bucket}/llm/chatglm/datasets/\"\n",
    "}\n",
    "\n",
    "\n",
    "# create the Estimator\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'start_simple.py',          # user endpoint script\n",
    "    source_dir           = './ptuning',               # directory which includes all the files needed for training\n",
    "    instance_type        = instance_type, # instances type used for the training job\n",
    "    instance_count       = instance_count,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,           # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    script_mode          = True,\n",
    "    transformers_version = '4.26',            # the transformers version used in the training job\n",
    "    pytorch_version      = '1.13',            # the pytorch_version version used in the training job\n",
    "    py_version           = 'py39',            # the python version used in the training job\n",
    "    environment = environment\n",
    ")\n",
    "\n",
    "huggingface_estimator.fit(inputs=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d799c69-96b4-4f7e-8f8a-df387f431106",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-simple-2023-05-19-0-2023-05-19-00-55-21-706/output/model.tar.gz'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8e7992f-e414-40a0-9883-4d3dd1b6ea73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE checkpoint-100/\n",
      "                           PRE checkpoint-200/\n",
      "                           PRE checkpoint-300/\n",
      "                           PRE checkpoint-50/\n",
      "2023-05-19 01:09:31        188 all_results.json\n",
      "2023-05-19 01:09:31        869 config.json\n",
      "2023-05-19 01:09:31       4276 configuration_chatglm.py\n",
      "2023-05-19 01:09:31        142 generation_config.json\n",
      "2023-05-19 01:09:31    2706249 ice_text.model\n",
      "2023-05-19 01:09:31      57620 modeling_chatglm.py\n",
      "2023-05-19 01:09:31  117441341 pytorch_model.bin\n",
      "2023-05-19 01:09:31      15054 quantization.py\n",
      "2023-05-19 01:09:31        125 special_tokens_map.json\n",
      "2023-05-19 01:09:31      17047 tokenization_chatglm.py\n",
      "2023-05-19 01:09:31        495 tokenizer_config.json\n",
      "2023-05-19 01:09:31        188 train_results.json\n",
      "2023-05-19 01:09:31       1155 trainer_state.json\n",
      "2023-05-19 01:09:31       3771 training_args.bin\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e130efd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### chatglm 官方deepspeed方式（全参数的Finetune,单机多卡）\n",
    "1: 准备deepspeed lib，并修改deepspeed.json    \n",
    "2：数据集（以上一致）  \n",
    "3：entrypoint start-single-node.py,设置num-gpus  \n",
    "4：触发bash ds_train_finetune_single_node.sh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56a48210-48ff-4840-a9a0-c8d0e7085adb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processes_per_host is set to: 8\n"
     ]
    }
   ],
   "source": [
    "instance_type = 'ml.g5.48xlarge'\n",
    "if instance_type in [\n",
    "    \"ml.p3.16xlarge\",\n",
    "    \"ml.p3dn.24xlarge\",\n",
    "    \"ml.g5.48xlarge\",\n",
    "    \"ml.p4d.24xlarge\"    \n",
    "]:\n",
    "    processes_per_host = 8\n",
    "elif instance_type == \"ml.p2.16xlarge\":\n",
    "    processes_per_host = 16\n",
    "elif instance_type == \"ml.g4dn.2xlarge\":\n",
    "    processes_per_host = 1\n",
    "else:\n",
    "    processes_per_host = 4\n",
    "    \n",
    "\n",
    "print(\"processes_per_host is set to:\", processes_per_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3f648f6-7a86-4d18-a087-37401bdd2188",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-chatglm-deepspeed-2023-05-1-2023-05-11-09-58-06-102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "2023-05-11 09:58:08 Starting - Starting the training job......\n",
      "2023-05-11 09:58:44 Starting - Preparing the instances for training......\n",
      "2023-05-11 10:00:02 Downloading - Downloading input data...\n",
      "2023-05-11 10:00:22 Training - Downloading the training image..................\n",
      "2023-05-11 10:03:18 Training - Training image download completed. Training in progress.....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-05-11 10:04:05,588 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-05-11 10:04:05,651 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-11 10:04:05,661 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-05-11 10:04:05,664 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-05-11 10:04:06,206 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (3.20.2)\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.28.0\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 66.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting cpm_kernels\u001b[0m\n",
      "\u001b[34mDownloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 416.6/416.6 kB 67.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mCollecting gradio\u001b[0m\n",
      "\u001b[34mDownloading gradio-3.29.0-py3-none-any.whl (17.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 102.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting mdtex2html\u001b[0m\n",
      "\u001b[34mDownloading mdtex2html-1.2.0-py3-none-any.whl (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (0.1.97)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (0.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (2.9.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface\u001b[0m\n",
      "\u001b[34mDownloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting jieba\u001b[0m\n",
      "\u001b[34mDownloading jieba-0.42.1.tar.gz (19.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/19.2 MB 88.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting rouge_chinese\u001b[0m\n",
      "\u001b[34mDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\u001b[0m\n",
      "\u001b[34mCollecting nltk\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 100.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting deepspeed==0.8.3\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.8.3.tar.gz (765 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 765.4/765.4 kB 82.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (3.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (0.13.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (2022.10.31)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 15)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 15)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 15)) (5.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 15)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 15)) (1.10.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.10->-r requirements.txt (line 5)) (4.4.0)\u001b[0m\n",
      "\u001b[34mCollecting fastapi\u001b[0m\n",
      "\u001b[34mDownloading fastapi-0.95.1-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.0/57.0 kB 12.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiofiles\u001b[0m\n",
      "\u001b[34mDownloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mCollecting ffmpy\u001b[0m\n",
      "\u001b[34mDownloading ffmpy-0.3.0.tar.gz (4.8 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting pydub\u001b[0m\n",
      "\u001b[34mDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (3.6.3)\u001b[0m\n",
      "\u001b[34mCollecting altair>=4.2.0\u001b[0m\n",
      "\u001b[34mDownloading altair-5.0.0-py3-none-any.whl (477 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 477.4/477.4 kB 64.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting mdit-py-plugins<=0.3.3\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.5/50.5 kB 18.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting orjson\u001b[0m\n",
      "\u001b[34mDownloading orjson-3.8.12-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 137.2/137.2 kB 30.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (1.5.3)\u001b[0m\n",
      "\u001b[34mCollecting httpx\u001b[0m\n",
      "\u001b[34mDownloading httpx-0.24.0-py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.3/75.3 kB 24.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting uvicorn>=0.14.0\u001b[0m\n",
      "\u001b[34mDownloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 21.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (3.8.4)\u001b[0m\n",
      "\u001b[34mCollecting websockets>=10.0\u001b[0m\n",
      "\u001b[34mDownloading websockets-11.0.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.7/129.7 kB 38.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments>=2.12.0 in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (2.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (9.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markupsafe in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (2.1.2)\u001b[0m\n",
      "\u001b[34mCollecting semantic-version\u001b[0m\n",
      "\u001b[34mDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mCollecting gradio-client>=0.2.1\u001b[0m\n",
      "\u001b[34mDownloading gradio_client-0.2.3-py3-none-any.whl (287 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 287.9/287.9 kB 53.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.11.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 55.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (3.1.2)\u001b[0m\n",
      "\u001b[34mCollecting markdown-it-py[linkify]>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.5/84.5 kB 25.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting python-multipart\u001b[0m\n",
      "\u001b[34mDownloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.7/45.7 kB 15.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting latex2mathml\u001b[0m\n",
      "\u001b[34mDownloading latex2mathml-3.75.5-py3-none-any.whl (73 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.3/73.3 kB 23.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown in /opt/conda/lib/python3.9/site-packages (from mdtex2html->-r requirements.txt (line 7)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 10)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 10)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 10)) (11.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 10)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 10)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 10)) (2023.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from rouge_chinese->-r requirements.txt (line 13)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 14)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 14)) (8.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: toolz in /opt/conda/lib/python3.9/site-packages (from altair>=4.2.0->gradio->-r requirements.txt (line 6)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.9/site-packages (from altair>=4.2.0->gradio->-r requirements.txt (line 6)) (4.17.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (1.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (4.0.2)\u001b[0m\n",
      "\u001b[34mCollecting mdurl~=0.1\u001b[0m\n",
      "\u001b[34mDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting linkify-it-py<3,>=1\u001b[0m\n",
      "\u001b[34mDownloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->gradio->-r requirements.txt (line 6)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->gradio->-r requirements.txt (line 6)) (2022.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.0->-r requirements.txt (line 3)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.0->-r requirements.txt (line 3)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.0->-r requirements.txt (line 3)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mCollecting h11>=0.8\u001b[0m\n",
      "\u001b[34mDownloading h11-0.14.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 20.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting starlette<0.27.0,>=0.26.1\u001b[0m\n",
      "\u001b[34mDownloading starlette-0.26.1-py3-none-any.whl (66 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.9/66.9 kB 23.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting httpcore<0.18.0,>=0.15.0\u001b[0m\n",
      "\u001b[34mDownloading httpcore-0.17.0-py3-none-any.whl (70 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 70.6/70.6 kB 23.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting sniffio\u001b[0m\n",
      "\u001b[34mDownloading sniffio-1.3.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown->mdtex2html->-r requirements.txt (line 7)) (4.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 6)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 6)) (1.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 6)) (4.38.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 6)) (1.0.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 6)) (0.11.0)\u001b[0m\n",
      "\u001b[34mCollecting anyio<5.0,>=3.0\u001b[0m\n",
      "\u001b[34mDownloading anyio-3.6.2-py3-none-any.whl (80 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.6/80.6 kB 27.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown->mdtex2html->-r requirements.txt (line 7)) (3.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio->-r requirements.txt (line 6)) (0.19.3)\u001b[0m\n",
      "\u001b[34mCollecting uc-micro-py\u001b[0m\n",
      "\u001b[34mDownloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed, jieba, ffmpy\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.8.3-py3-none-any.whl size=776407 sha256=26dafaa5e84f6c721290bc9cecc4d953af5872cc657526a24486a1aa6279c7b4\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/f8/ea/8f/0768328ba436ed66f602d8d3b809624448c9eb627434176d04\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=2680ac6284765c6e78671cff395b686f4956d4f2653e2399378b117b70601bca\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/7d/74/cf/08c94db4b784e2c1ef675a600b7b5b281fd25240dcb954ee7e\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4693 sha256=5c86fff38de06a0c1acb9c09afd5505ffb2bc216be20213bca9d7730bc178c12\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/91/e2/96/f676aa08bfd789328c6576cd0f1fde4a3d686703bb0c247697\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed jieba ffmpy\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pydub, jieba, huggingface, ffmpy, cpm_kernels, websockets, uc-micro-py, sniffio, semantic-version, rouge_chinese, python-multipart, orjson, nltk, mdurl, latex2mathml, h11, aiofiles, uvicorn, markdown-it-py, linkify-it-py, huggingface-hub, deepspeed, anyio, transformers, starlette, mdtex2html, mdit-py-plugins, httpcore, altair, httpx, fastapi, gradio-client, gradio\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.12.0\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.12.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.12.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+06f2048\u001b[0m\n",
      "\u001b[34mUninstalling deepspeed-0.6.1+06f2048:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled deepspeed-0.6.1+06f2048\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.26.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.26.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.26.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiofiles-23.1.0 altair-5.0.0 anyio-3.6.2 cpm_kernels-1.0.11 deepspeed-0.8.3 fastapi-0.95.1 ffmpy-0.3.0 gradio-3.29.0 gradio-client-0.2.3 h11-0.14.0 httpcore-0.17.0 httpx-0.24.0 huggingface-0.0.1 huggingface-hub-0.14.1 jieba-0.42.1 latex2mathml-3.75.5 linkify-it-py-2.0.2 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 mdtex2html-1.2.0 mdurl-0.1.2 nltk-3.8.1 orjson-3.8.12 pydub-0.25.1 python-multipart-0.0.6 rouge_chinese-1.0.3 semantic-version-2.10.0 sniffio-1.3.0 starlette-0.26.1 transformers-4.28.0 uc-micro-py-1.0.2 uvicorn-0.22.0 websockets-11.0.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.1.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-05-11 10:04:33,250 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-05-11 10:04:33,250 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-05-11 10:04:33,317 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-11 10:04:33,393 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-11 10:04:33,469 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-11 10:04:33,481 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"AdvertiseGen\": \"/opt/ml/input/data/AdvertiseGen\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.48xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"AdvertiseGen\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.48xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-chatglm-deepspeed-2023-05-1-2023-05-11-09-58-06-102\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-deepspeed-2023-05-1-2023-05-11-09-58-06-102/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"start-single-node\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 192,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.48xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.48xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"start-single-node.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=start-single-node.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"AdvertiseGen\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"AdvertiseGen\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.48xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=start-single-node\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=192\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-deepspeed-2023-05-1-2023-05-11-09-58-06-102/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"AdvertiseGen\":\"/opt/ml/input/data/AdvertiseGen\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.48xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"AdvertiseGen\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-chatglm-deepspeed-2023-05-1-2023-05-11-09-58-06-102\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-deepspeed-2023-05-1-2023-05-11-09-58-06-102/source/sourcedir.tar.gz\",\"module_name\":\"start-single-node\",\"network_interface_name\":\"eth0\",\"num_cpus\":192,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"start-single-node.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_ADVERTISEGEN=/opt/ml/input/data/AdvertiseGen\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 start-single-node.py\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:04:37.868: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-05-11 10:04:37,872 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-05-11 10:04:37,893 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/LICENSE /tmp/chatglm/LICENSE\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/MODEL_LICENSE /tmp/chatglm/MODEL_LICENSE\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/README.md /tmp/chatglm/README.md\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/configuration_chatglm.py /tmp/chatglm/configuration_chatglm.py\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/pytorch_model.bin.index.json /tmp/chatglm/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/.gitattributes /tmp/chatglm/.gitattributes\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/quantization.py /tmp/chatglm/quantization.py\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/tokenization_chatglm.py /tmp/chatglm/tokenization_chatglm.py\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/test_modeling_chatglm.py /tmp/chatglm/test_modeling_chatglm.py\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/tokenizer_config.json /tmp/chatglm/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/config.json /tmp/chatglm/config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/modeling_chatglm.py /tmp/chatglm/modeling_chatglm.py\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/ice_text.model /tmp/chatglm/ice_text.model\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/pytorch_model-00007-of-00008.bin /tmp/chatglm/pytorch_model-00007-of-00008.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/pytorch_model-00008-of-00008.bin /tmp/chatglm/pytorch_model-00008-of-00008.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/pytorch_model-00001-of-00008.bin /tmp/chatglm/pytorch_model-00001-of-00008.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/pytorch_model-00002-of-00008.bin /tmp/chatglm/pytorch_model-00002-of-00008.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/pytorch_model-00004-of-00008.bin /tmp/chatglm/pytorch_model-00004-of-00008.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/pytorch_model-00005-of-00008.bin /tmp/chatglm/pytorch_model-00005-of-00008.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/pytorch_model-00006-of-00008.bin /tmp/chatglm/pytorch_model-00006-of-00008.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/pytorch_model-00003-of-00008.bin /tmp/chatglm/pytorch_model-00003-of-00008.bin\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:04:47,149] [WARNING] [runner.py:186:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:04:47,203] [INFO] [runner.py:550:main] cmd = /opt/conda/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=23456 --enable_each_rank_log=None main_tuning.py --deepspeed deepspeed.json --do_train --train_file /opt/ml/input/data/AdvertiseGen/train.json --test_file /opt/ml/input/data/AdvertiseGen/dev.json --prompt_column content --response_column summary --overwrite_cache --model_name_or_path THUDM/chatglm-6b --output_dir /tmp/model/adgen-chatglm-6b-ft --model_output_s3_path s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/ --overwrite_output_dir --max_source_length 64 --max_target_length 64 --per_device_train_batch_size 1 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --predict_with_generate --max_steps 20 --logging_steps 10 --save_steps 20 --learning_rate 1e-4 --fp16\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:04:49,256] [INFO] [launch.py:135:main] 0 NCCL_VERSION=2.14.3\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:04:49,257] [INFO] [launch.py:135:main] 0 NCCL_SOCKET_IFNAME=eth0\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:04:49,257] [INFO] [launch.py:135:main] 0 NCCL_DEBUG=WARN\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:04:49,257] [INFO] [launch.py:135:main] 0 NCCL_IB_DISABLE=1\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:04:49,257] [INFO] [launch.py:142:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:04:49,257] [INFO] [launch.py:148:main] nnodes=1, num_local_procs=8, node_rank=0\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:04:49,257] [INFO] [launch.py:161:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:04:49,257] [INFO] [launch.py:162:main] dist_world_size=8\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:04:49,257] [INFO] [launch.py:164:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:04:54,050] [INFO] [comm.py:652:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m05/11/2023 10:04:55 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/11/2023 10:04:55 - WARNING - __main__ - Process rank: 7, device: cuda:7, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/11/2023 10:04:55 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/11/2023 10:04:55 - WARNING - __main__ - Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/11/2023 10:04:55 - WARNING - __main__ - Process rank: 6, device: cuda:6, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/11/2023 10:04:55 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/11/2023 10:04:55 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/11/2023 10:04:55 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/11/2023 10:04:55 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=False,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=deepspeed.json,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdo_eval=False,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=no,\u001b[0m\n",
      "\u001b[34mfp16=True,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgeneration_config=None,\u001b[0m\n",
      "\u001b[34mgeneration_max_length=None,\u001b[0m\n",
      "\u001b[34mgeneration_num_beams=None,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=1,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=False,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=0.0001,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=0,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/tmp/model/adgen-chatglm-6b-ft/runs/May11_10-04-54_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=10,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=linear,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=20,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=3.0,\u001b[0m\n",
      "\u001b[34moptim=adamw_hf,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moutput_dir=/tmp/model/adgen-chatglm-6b-ft,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=True,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=4,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=1,\u001b[0m\n",
      "\u001b[34mpredict_with_generate=True,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/tmp/model/adgen-chatglm-6b-ft,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_safetensors=False,\u001b[0m\n",
      "\u001b[34msave_steps=20,\u001b[0m\n",
      "\u001b[34msave_strategy=steps,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=42,\u001b[0m\n",
      "\u001b[34msharded_ddp=[],\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34msortish_sampler=False,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,\u001b[0m\n",
      "\u001b[34mxpu_backend=None,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m05/11/2023 10:04:55 - WARNING - datasets.builder - Using custom data configuration default-1a1a1eecb7977703\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-1a1a1eecb7977703/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 2/2 [00:00<00:00, 13189.64it/s]\u001b[0m\n",
      "\u001b[34m05/11/2023 10:04:55 - WARNING - datasets.builder - Using custom data configuration default-1a1a1eecb7977703\u001b[0m\n",
      "\u001b[34m05/11/2023 10:04:55 - WARNING - datasets.builder - Using custom data configuration default-1a1a1eecb7977703\u001b[0m\n",
      "\u001b[34m05/11/2023 10:04:55 - WARNING - datasets.builder - Using custom data configuration default-1a1a1eecb7977703\u001b[0m\n",
      "\u001b[34m05/11/2023 10:04:55 - WARNING - datasets.builder - Using custom data configuration default-1a1a1eecb7977703\u001b[0m\n",
      "\u001b[34m05/11/2023 10:04:55 - WARNING - datasets.builder - Using custom data configuration default-1a1a1eecb7977703\u001b[0m\n",
      "\u001b[34m05/11/2023 10:04:55 - WARNING - datasets.builder - Using custom data configuration default-1a1a1eecb7977703\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 2/2 [00:00<00:00, 1920.91it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34m05/11/2023 10:04:55 - WARNING - datasets.builder - Using custom data configuration default-1a1a1eecb7977703\u001b[0m\n",
      "\u001b[34mGenerating test split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-1a1a1eecb7977703/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m05/11/2023 10:04:55 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1a1a1eecb7977703/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 590.04it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 611.28it/s]\u001b[0m\n",
      "\u001b[34m05/11/2023 10:04:55 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1a1a1eecb7977703/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 513.00it/s]\u001b[0m\n",
      "\u001b[34m05/11/2023 10:04:55 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1a1a1eecb7977703/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m05/11/2023 10:04:55 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1a1a1eecb7977703/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m05/11/2023 10:04:55 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1a1a1eecb7977703/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 501.41it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 575.47it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 572.60it/s]\u001b[0m\n",
      "\u001b[34m05/11/2023 10:04:55 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1a1a1eecb7977703/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m05/11/2023 10:04:55 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1a1a1eecb7977703/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 578.52it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 620.87it/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/773 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 773/773 [00:00<00:00, 227kB/s]\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:04:56,051 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:04:56,051 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:04:56,080 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:04:56,080 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:04:56,081 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:04:56,081 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:04:56,089 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:04:56,089 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:04:56,089 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:04:56,089 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:04:56,093 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:04:56,093 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:04:56,093 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:04:56,093 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:668] 2023-05-11 10:04:56,097 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:668] 2023-05-11 10:04:56,097 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/config.json\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:04:56,097 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:04:56,097 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mDownloading (…)iguration_chatglm.py:   0%|          | 0.00/4.28k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)iguration_chatglm.py: 100%|██████████| 4.28k/4.28k [00:00<00:00, 1.94MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:668] 2023-05-11 10:04:56,332 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:668] 2023-05-11 10:04:56,332 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:720] 2023-05-11 10:04:56,333 >> Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm-6b\",\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\"\n",
      "  },\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"gmask_token_id\": 130001,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"inner_hidden_size\": 16384,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"mask_token_id\": 130000,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_encoding_2d\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.28.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 130528\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:720] 2023-05-11 10:04:56,333 >> Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm-6b\",\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\"\n",
      "  },\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"gmask_token_id\": 130001,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"inner_hidden_size\": 16384,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"mask_token_id\": 130000,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_encoding_2d\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.28.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 130528\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json:   0%|          | 0.00/441 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json: 100%|██████████| 441/441 [00:00<00:00, 705kB/s]\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:04:56,513 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:04:56,513 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:04:56,514 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:04:56,514 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:04:56,525 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:04:56,525 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:04:56,554 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:04:56,554 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:04:56,556 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:04:56,556 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:04:56,559 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:04:56,559 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:04:56,559 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:04:56,559 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:04:56,563 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:04:56,563 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mDownloading (…)enization_chatglm.py:   0%|          | 0.00/17.0k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)enization_chatglm.py: 100%|██████████| 17.0k/17.0k [00:00<00:00, 29.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ice_text.model:   0%|          | 0.00/2.71M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ice_text.model: 100%|██████████| 2.71M/2.71M [00:00<00:00, 74.7MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1809] 2023-05-11 10:04:57,088 >> loading file ice_text.model from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/ice_text.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1809] 2023-05-11 10:04:57,088 >> loading file ice_text.model from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/ice_text.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1809] 2023-05-11 10:04:57,088 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1809] 2023-05-11 10:04:57,088 >> loading file special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1809] 2023-05-11 10:04:57,088 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1809] 2023-05-11 10:04:57,088 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1809] 2023-05-11 10:04:57,088 >> loading file special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1809] 2023-05-11 10:04:57,088 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:04:57,277 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:04:57,277 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:04:57,287 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:04:57,287 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:04:57,289 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:04:57,289 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:04:57,293 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:04:57,293 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:04:57,296 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:04:57,296 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:04:57,298 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:04:57,298 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:04:57,302 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:04:57,302 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:04:57,313 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:04:57,313 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mDownloading (…)/modeling_chatglm.py:   0%|          | 0.00/57.6k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/modeling_chatglm.py: 100%|██████████| 57.6k/57.6k [00:00<00:00, 22.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)main/quantization.py:   0%|          | 0.00/15.1k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)main/quantization.py: 100%|██████████| 15.1k/15.1k [00:00<00:00, 23.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)model.bin.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)model.bin.index.json: 100%|██████████| 33.4k/33.4k [00:00<00:00, 474kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2534] 2023-05-11 10:04:58,136 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2534] 2023-05-11 10:04:58,136 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   0%|          | 0.00/1.74G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   1%|          | 10.5M/1.74G [00:00<00:20, 85.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   1%|          | 21.0M/1.74G [00:00<00:19, 87.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   2%|▏         | 31.5M/1.74G [00:00<00:19, 86.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   2%|▏         | 41.9M/1.74G [00:00<00:19, 87.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   3%|▎         | 52.4M/1.74G [00:00<00:19, 88.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   4%|▎         | 62.9M/1.74G [00:00<00:19, 87.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   4%|▍         | 73.4M/1.74G [00:00<00:18, 89.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   5%|▍         | 83.9M/1.74G [00:00<00:18, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   5%|▌         | 94.4M/1.74G [00:01<00:19, 86.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   6%|▌         | 105M/1.74G [00:01<00:18, 89.0MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   7%|▋         | 115M/1.74G [00:01<00:18, 90.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   7%|▋         | 126M/1.74G [00:01<00:17, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   8%|▊         | 136M/1.74G [00:01<00:17, 92.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   8%|▊         | 147M/1.74G [00:01<00:17, 92.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   9%|▉         | 157M/1.74G [00:01<00:18, 85.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  10%|▉         | 168M/1.74G [00:01<00:18, 86.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  10%|█         | 178M/1.74G [00:02<00:17, 87.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  11%|█         | 189M/1.74G [00:02<00:17, 86.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  11%|█▏        | 199M/1.74G [00:02<00:17, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  12%|█▏        | 210M/1.74G [00:02<00:16, 91.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  13%|█▎        | 220M/1.74G [00:02<00:17, 89.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  13%|█▎        | 231M/1.74G [00:02<00:16, 91.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  14%|█▍        | 241M/1.74G [00:02<00:16, 92.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  14%|█▍        | 252M/1.74G [00:02<00:16, 91.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  15%|█▌        | 262M/1.74G [00:02<00:16, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  16%|█▌        | 273M/1.74G [00:03<00:16, 88.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  16%|█▋        | 283M/1.74G [00:03<00:16, 87.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  17%|█▋        | 294M/1.74G [00:03<00:16, 89.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  17%|█▋        | 304M/1.74G [00:03<00:16, 88.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  18%|█▊        | 315M/1.74G [00:03<00:15, 89.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  19%|█▊        | 325M/1.74G [00:03<00:18, 78.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  19%|█▉        | 336M/1.74G [00:03<00:18, 78.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  20%|█▉        | 346M/1.74G [00:03<00:17, 81.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  20%|██        | 357M/1.74G [00:04<00:16, 83.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  21%|██        | 367M/1.74G [00:04<00:16, 82.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  22%|██▏       | 377M/1.74G [00:04<00:16, 82.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  22%|██▏       | 388M/1.74G [00:04<00:16, 82.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  23%|██▎       | 398M/1.74G [00:04<00:16, 82.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  23%|██▎       | 409M/1.74G [00:04<00:16, 82.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  24%|██▍       | 419M/1.74G [00:04<00:15, 84.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  25%|██▍       | 430M/1.74G [00:04<00:15, 82.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  25%|██▌       | 440M/1.74G [00:05<00:16, 78.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  26%|██▌       | 451M/1.74G [00:05<00:15, 82.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  27%|██▋       | 461M/1.74G [00:05<00:15, 84.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  27%|██▋       | 472M/1.74G [00:05<00:14, 84.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  28%|██▊       | 482M/1.74G [00:05<00:14, 87.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  28%|██▊       | 493M/1.74G [00:05<00:15, 78.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  29%|██▉       | 503M/1.74G [00:05<00:15, 81.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  30%|██▉       | 514M/1.74G [00:05<00:14, 84.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  30%|███       | 524M/1.74G [00:06<00:13, 87.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  31%|███       | 535M/1.74G [00:06<00:13, 87.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  31%|███▏      | 545M/1.74G [00:06<00:13, 85.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  32%|███▏      | 556M/1.74G [00:06<00:13, 87.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  33%|███▎      | 566M/1.74G [00:06<00:12, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  33%|███▎      | 577M/1.74G [00:06<00:14, 82.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  34%|███▎      | 587M/1.74G [00:06<00:13, 85.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  34%|███▍      | 598M/1.74G [00:06<00:14, 80.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  35%|███▍      | 608M/1.74G [00:07<00:13, 83.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  36%|███▌      | 619M/1.74G [00:07<00:13, 84.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  36%|███▌      | 629M/1.74G [00:07<00:12, 87.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  37%|███▋      | 640M/1.74G [00:07<00:12, 87.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  37%|███▋      | 650M/1.74G [00:07<00:12, 89.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  38%|███▊      | 661M/1.74G [00:07<00:11, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  39%|███▊      | 671M/1.74G [00:07<00:11, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  39%|███▉      | 682M/1.74G [00:07<00:11, 92.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  40%|███▉      | 692M/1.74G [00:07<00:11, 92.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  40%|████      | 703M/1.74G [00:08<00:11, 93.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  41%|████      | 713M/1.74G [00:08<00:11, 92.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  42%|████▏     | 724M/1.74G [00:08<00:11, 91.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  42%|████▏     | 734M/1.74G [00:08<00:10, 92.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  43%|████▎     | 744M/1.74G [00:08<00:10, 93.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  43%|████▎     | 755M/1.74G [00:08<00:10, 93.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  44%|████▍     | 765M/1.74G [00:08<00:10, 93.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  45%|████▍     | 776M/1.74G [00:08<00:10, 94.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  45%|████▌     | 786M/1.74G [00:09<00:11, 83.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  46%|████▌     | 797M/1.74G [00:09<00:11, 85.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  46%|████▋     | 807M/1.74G [00:09<00:10, 87.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  47%|████▋     | 818M/1.74G [00:09<00:10, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  48%|████▊     | 828M/1.74G [00:09<00:10, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  48%|████▊     | 839M/1.74G [00:09<00:10, 87.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  49%|████▉     | 849M/1.74G [00:09<00:10, 86.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  49%|████▉     | 860M/1.74G [00:09<00:09, 89.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  50%|████▉     | 870M/1.74G [00:09<00:09, 88.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  51%|█████     | 881M/1.74G [00:10<00:09, 90.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  51%|█████     | 891M/1.74G [00:10<00:09, 91.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  52%|█████▏    | 902M/1.74G [00:10<00:09, 92.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  52%|█████▏    | 912M/1.74G [00:10<00:08, 92.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  53%|█████▎    | 923M/1.74G [00:10<00:08, 93.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  54%|█████▎    | 933M/1.74G [00:10<00:08, 93.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  54%|█████▍    | 944M/1.74G [00:10<00:08, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  55%|█████▍    | 954M/1.74G [00:10<00:08, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  55%|█████▌    | 965M/1.74G [00:11<00:08, 91.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  56%|█████▌    | 975M/1.74G [00:11<00:08, 93.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  57%|█████▋    | 986M/1.74G [00:11<00:08, 94.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  57%|█████▋    | 996M/1.74G [00:11<00:07, 94.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  58%|█████▊    | 1.01G/1.74G [00:11<00:07, 93.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  58%|█████▊    | 1.02G/1.74G [00:11<00:07, 91.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  59%|█████▉    | 1.03G/1.74G [00:11<00:07, 90.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  60%|█████▉    | 1.04G/1.74G [00:11<00:07, 90.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  60%|██████    | 1.05G/1.74G [00:11<00:07, 91.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  61%|██████    | 1.06G/1.74G [00:12<00:07, 92.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  61%|██████▏   | 1.07G/1.74G [00:12<00:07, 93.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  62%|██████▏   | 1.08G/1.74G [00:12<00:07, 93.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  63%|██████▎   | 1.09G/1.74G [00:12<00:07, 92.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  63%|██████▎   | 1.10G/1.74G [00:12<00:07, 90.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  64%|██████▍   | 1.11G/1.74G [00:12<00:06, 91.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  64%|██████▍   | 1.12G/1.74G [00:12<00:06, 92.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  65%|██████▌   | 1.13G/1.74G [00:12<00:06, 91.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  66%|██████▌   | 1.14G/1.74G [00:12<00:06, 93.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  66%|██████▋   | 1.15G/1.74G [00:13<00:06, 92.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  67%|██████▋   | 1.16G/1.74G [00:13<00:06, 89.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  67%|██████▋   | 1.17G/1.74G [00:13<00:06, 88.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  68%|██████▊   | 1.18G/1.74G [00:13<00:06, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  69%|██████▊   | 1.20G/1.74G [00:13<00:06, 88.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  69%|██████▉   | 1.21G/1.74G [00:13<00:06, 88.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  70%|██████▉   | 1.22G/1.74G [00:13<00:05, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  70%|███████   | 1.23G/1.74G [00:13<00:05, 90.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  71%|███████   | 1.24G/1.74G [00:13<00:05, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  72%|███████▏  | 1.25G/1.74G [00:14<00:05, 90.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  72%|███████▏  | 1.26G/1.74G [00:14<00:05, 91.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  73%|███████▎  | 1.27G/1.74G [00:14<00:05, 92.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  73%|███████▎  | 1.28G/1.74G [00:14<00:04, 94.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  74%|███████▍  | 1.29G/1.74G [00:14<00:04, 94.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  75%|███████▍  | 1.30G/1.74G [00:14<00:04, 94.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  75%|███████▌  | 1.31G/1.74G [00:14<00:04, 94.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  76%|███████▌  | 1.32G/1.74G [00:14<00:04, 88.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  77%|███████▋  | 1.33G/1.74G [00:15<00:04, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  77%|███████▋  | 1.34G/1.74G [00:15<00:04, 92.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  78%|███████▊  | 1.35G/1.74G [00:15<00:04, 93.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  78%|███████▊  | 1.36G/1.74G [00:15<00:03, 94.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  79%|███████▉  | 1.37G/1.74G [00:15<00:03, 93.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  80%|███████▉  | 1.38G/1.74G [00:15<00:03, 93.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  80%|████████  | 1.39G/1.74G [00:15<00:03, 93.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  81%|████████  | 1.41G/1.74G [00:15<00:03, 93.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  81%|████████▏ | 1.42G/1.74G [00:15<00:03, 92.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  82%|████████▏ | 1.43G/1.74G [00:16<00:03, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  83%|████████▎ | 1.44G/1.74G [00:16<00:03, 91.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  83%|████████▎ | 1.45G/1.74G [00:16<00:03, 91.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  84%|████████▎ | 1.46G/1.74G [00:16<00:03, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  84%|████████▍ | 1.47G/1.74G [00:16<00:03, 89.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  85%|████████▍ | 1.48G/1.74G [00:16<00:02, 88.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  86%|████████▌ | 1.49G/1.74G [00:16<00:02, 88.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  86%|████████▌ | 1.50G/1.74G [00:16<00:02, 88.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  87%|████████▋ | 1.51G/1.74G [00:16<00:02, 87.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  87%|████████▋ | 1.52G/1.74G [00:17<00:02, 83.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  88%|████████▊ | 1.53G/1.74G [00:17<00:02, 82.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  89%|████████▊ | 1.54G/1.74G [00:17<00:02, 85.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  89%|████████▉ | 1.55G/1.74G [00:17<00:02, 85.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  90%|████████▉ | 1.56G/1.74G [00:17<00:02, 76.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  90%|█████████ | 1.57G/1.74G [00:17<00:02, 78.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  91%|█████████ | 1.58G/1.74G [00:17<00:01, 82.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  92%|█████████▏| 1.59G/1.74G [00:17<00:01, 84.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  92%|█████████▏| 1.60G/1.74G [00:18<00:01, 87.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  93%|█████████▎| 1.61G/1.74G [00:18<00:01, 87.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  93%|█████████▎| 1.63G/1.74G [00:18<00:01, 89.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  94%|█████████▍| 1.64G/1.74G [00:18<00:01, 90.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  95%|█████████▍| 1.65G/1.74G [00:18<00:01, 90.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  95%|█████████▌| 1.66G/1.74G [00:18<00:00, 91.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  96%|█████████▌| 1.67G/1.74G [00:18<00:00, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  96%|█████████▋| 1.68G/1.74G [00:18<00:00, 90.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  97%|█████████▋| 1.69G/1.74G [00:19<00:00, 88.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  98%|█████████▊| 1.70G/1.74G [00:19<00:00, 89.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  98%|█████████▊| 1.71G/1.74G [00:19<00:00, 89.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  99%|█████████▉| 1.72G/1.74G [00:19<00:00, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  99%|█████████▉| 1.73G/1.74G [00:19<00:00, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin: 100%|█████████▉| 1.74G/1.74G [00:19<00:00, 91.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin: 100%|██████████| 1.74G/1.74G [00:19<00:00, 88.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  12%|█▎        | 1/8 [00:19<02:18, 19.76s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  12%|█▎        | 1/8 [00:19<02:18, 19.73s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  12%|█▎        | 1/8 [00:19<02:18, 19.74s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  12%|█▎        | 1/8 [00:19<02:18, 19.78s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  12%|█▎        | 1/8 [00:19<02:18, 19.77s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  12%|█▎        | 1/8 [00:19<02:18, 19.78s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  12%|█▎        | 1/8 [00:19<02:18, 19.78s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  12%|█▎        | 1/8 [00:19<02:18, 19.79s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   1%|          | 10.5M/1.88G [00:00<00:22, 82.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   1%|          | 21.0M/1.88G [00:00<00:21, 87.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   2%|▏         | 31.5M/1.88G [00:00<00:21, 85.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   2%|▏         | 41.9M/1.88G [00:00<00:22, 83.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   3%|▎         | 52.4M/1.88G [00:00<00:21, 84.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   3%|▎         | 62.9M/1.88G [00:00<00:22, 82.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   4%|▍         | 73.4M/1.88G [00:00<00:21, 84.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   4%|▍         | 83.9M/1.88G [00:00<00:20, 86.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   5%|▌         | 94.4M/1.88G [00:01<00:22, 80.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   6%|▌         | 105M/1.88G [00:01<00:21, 83.8MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   6%|▌         | 115M/1.88G [00:01<00:20, 84.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   7%|▋         | 126M/1.88G [00:01<00:21, 80.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   7%|▋         | 136M/1.88G [00:01<00:20, 83.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   8%|▊         | 147M/1.88G [00:01<00:20, 86.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   8%|▊         | 157M/1.88G [00:01<00:19, 88.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   9%|▉         | 168M/1.88G [00:01<00:19, 88.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   9%|▉         | 178M/1.88G [00:02<00:19, 86.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  10%|█         | 189M/1.88G [00:02<00:19, 88.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  11%|█         | 199M/1.88G [00:02<00:19, 87.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  11%|█         | 210M/1.88G [00:02<00:18, 89.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  12%|█▏        | 220M/1.88G [00:02<00:18, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  12%|█▏        | 231M/1.88G [00:02<00:18, 88.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  13%|█▎        | 241M/1.88G [00:02<00:19, 83.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  13%|█▎        | 252M/1.88G [00:02<00:19, 85.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  14%|█▍        | 262M/1.88G [00:03<00:18, 85.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  15%|█▍        | 273M/1.88G [00:03<00:18, 85.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  15%|█▌        | 283M/1.88G [00:03<00:18, 87.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  16%|█▌        | 294M/1.88G [00:03<00:17, 88.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  16%|█▌        | 304M/1.88G [00:03<00:17, 88.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  17%|█▋        | 315M/1.88G [00:03<00:17, 90.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  17%|█▋        | 325M/1.88G [00:03<00:18, 83.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  18%|█▊        | 336M/1.88G [00:03<00:18, 84.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  18%|█▊        | 346M/1.88G [00:04<00:18, 81.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  19%|█▉        | 357M/1.88G [00:04<00:18, 82.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  20%|█▉        | 367M/1.88G [00:04<00:17, 85.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  20%|██        | 377M/1.88G [00:04<00:17, 86.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  21%|██        | 388M/1.88G [00:04<00:17, 85.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  21%|██        | 398M/1.88G [00:04<00:18, 80.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  22%|██▏       | 409M/1.88G [00:04<00:17, 84.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  22%|██▏       | 419M/1.88G [00:04<00:16, 86.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  23%|██▎       | 430M/1.88G [00:05<00:16, 85.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  23%|██▎       | 440M/1.88G [00:05<00:16, 88.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  24%|██▍       | 451M/1.88G [00:05<00:15, 89.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  25%|██▍       | 461M/1.88G [00:05<00:15, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  25%|██▌       | 472M/1.88G [00:05<00:15, 90.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  26%|██▌       | 482M/1.88G [00:05<00:15, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  26%|██▌       | 493M/1.88G [00:05<00:15, 92.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  27%|██▋       | 503M/1.88G [00:05<00:14, 92.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  27%|██▋       | 514M/1.88G [00:05<00:14, 91.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  28%|██▊       | 524M/1.88G [00:06<00:15, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  28%|██▊       | 535M/1.88G [00:06<00:15, 87.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  29%|██▉       | 545M/1.88G [00:06<00:14, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  30%|██▉       | 556M/1.88G [00:06<00:14, 91.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  30%|███       | 566M/1.88G [00:06<00:14, 92.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  31%|███       | 577M/1.88G [00:06<00:14, 92.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  31%|███       | 587M/1.88G [00:06<00:14, 92.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  32%|███▏      | 598M/1.88G [00:06<00:14, 91.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  32%|███▏      | 608M/1.88G [00:06<00:13, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  33%|███▎      | 619M/1.88G [00:07<00:14, 89.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  33%|███▎      | 629M/1.88G [00:07<00:13, 91.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  34%|███▍      | 640M/1.88G [00:07<00:13, 92.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  35%|███▍      | 650M/1.88G [00:07<00:17, 70.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  35%|███▌      | 661M/1.88G [00:07<00:17, 68.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  36%|███▌      | 671M/1.88G [00:07<00:16, 73.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  36%|███▋      | 682M/1.88G [00:07<00:15, 77.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  37%|███▋      | 692M/1.88G [00:08<00:14, 80.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  37%|███▋      | 703M/1.88G [00:08<00:14, 83.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  38%|███▊      | 713M/1.88G [00:08<00:13, 84.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  38%|███▊      | 724M/1.88G [00:08<00:13, 84.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  39%|███▉      | 734M/1.88G [00:08<00:13, 84.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  40%|███▉      | 744M/1.88G [00:08<00:13, 85.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  40%|████      | 755M/1.88G [00:08<00:12, 86.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  41%|████      | 765M/1.88G [00:08<00:13, 81.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  41%|████▏     | 776M/1.88G [00:09<00:13, 84.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  42%|████▏     | 786M/1.88G [00:09<00:12, 86.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  42%|████▏     | 797M/1.88G [00:09<00:12, 85.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  43%|████▎     | 807M/1.88G [00:09<00:12, 87.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  44%|████▎     | 818M/1.88G [00:09<00:12, 85.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  44%|████▍     | 828M/1.88G [00:09<00:12, 85.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  45%|████▍     | 839M/1.88G [00:09<00:12, 84.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  45%|████▌     | 849M/1.88G [00:09<00:12, 85.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  46%|████▌     | 860M/1.88G [00:10<00:11, 85.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  46%|████▋     | 870M/1.88G [00:10<00:12, 82.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  47%|████▋     | 881M/1.88G [00:10<00:11, 85.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  47%|████▋     | 891M/1.88G [00:10<00:11, 86.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  48%|████▊     | 902M/1.88G [00:10<00:11, 84.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  49%|████▊     | 912M/1.88G [00:10<00:11, 86.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  49%|████▉     | 923M/1.88G [00:10<00:10, 88.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  50%|████▉     | 933M/1.88G [00:10<00:10, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  50%|█████     | 944M/1.88G [00:11<00:10, 85.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  51%|█████     | 954M/1.88G [00:11<00:10, 88.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  51%|█████▏    | 965M/1.88G [00:11<00:10, 87.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  52%|█████▏    | 975M/1.88G [00:11<00:10, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  52%|█████▏    | 986M/1.88G [00:11<00:09, 90.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  53%|█████▎    | 996M/1.88G [00:11<00:10, 85.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  54%|█████▎    | 1.01G/1.88G [00:11<00:10, 85.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  54%|█████▍    | 1.02G/1.88G [00:11<00:09, 86.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  55%|█████▍    | 1.03G/1.88G [00:11<00:09, 87.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  55%|█████▌    | 1.04G/1.88G [00:12<00:09, 88.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  56%|█████▌    | 1.05G/1.88G [00:12<00:09, 87.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  56%|█████▋    | 1.06G/1.88G [00:12<00:09, 85.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  57%|█████▋    | 1.07G/1.88G [00:12<00:09, 88.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  57%|█████▋    | 1.08G/1.88G [00:12<00:08, 90.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  58%|█████▊    | 1.09G/1.88G [00:12<00:08, 90.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  59%|█████▊    | 1.10G/1.88G [00:12<00:08, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  59%|█████▉    | 1.11G/1.88G [00:12<00:08, 90.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  60%|█████▉    | 1.12G/1.88G [00:13<00:08, 91.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  60%|██████    | 1.13G/1.88G [00:13<00:08, 91.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  61%|██████    | 1.14G/1.88G [00:13<00:07, 92.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  61%|██████▏   | 1.15G/1.88G [00:13<00:07, 92.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  62%|██████▏   | 1.16G/1.88G [00:13<00:07, 93.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  62%|██████▏   | 1.17G/1.88G [00:13<00:08, 83.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  63%|██████▎   | 1.18G/1.88G [00:13<00:08, 82.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  64%|██████▎   | 1.20G/1.88G [00:13<00:08, 84.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  64%|██████▍   | 1.21G/1.88G [00:13<00:07, 87.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  65%|██████▍   | 1.22G/1.88G [00:14<00:07, 88.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  65%|██████▌   | 1.23G/1.88G [00:14<00:07, 88.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  66%|██████▌   | 1.24G/1.88G [00:14<00:07, 90.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  66%|██████▋   | 1.25G/1.88G [00:14<00:06, 92.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  67%|██████▋   | 1.26G/1.88G [00:14<00:06, 91.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  67%|██████▋   | 1.27G/1.88G [00:14<00:06, 93.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  68%|██████▊   | 1.28G/1.88G [00:14<00:06, 92.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  69%|██████▊   | 1.29G/1.88G [00:14<00:06, 93.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  69%|██████▉   | 1.30G/1.88G [00:14<00:06, 93.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  70%|██████▉   | 1.31G/1.88G [00:15<00:06, 94.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  70%|███████   | 1.32G/1.88G [00:15<00:05, 95.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  71%|███████   | 1.33G/1.88G [00:15<00:05, 94.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  71%|███████▏  | 1.34G/1.88G [00:15<00:05, 92.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  72%|███████▏  | 1.35G/1.88G [00:15<00:05, 94.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  73%|███████▎  | 1.36G/1.88G [00:15<00:05, 88.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  73%|███████▎  | 1.37G/1.88G [00:15<00:05, 91.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  74%|███████▎  | 1.38G/1.88G [00:15<00:05, 91.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  74%|███████▍  | 1.39G/1.88G [00:16<00:05, 92.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  75%|███████▍  | 1.41G/1.88G [00:16<00:05, 90.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  75%|███████▌  | 1.42G/1.88G [00:16<00:05, 91.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  76%|███████▌  | 1.43G/1.88G [00:16<00:04, 92.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  76%|███████▋  | 1.44G/1.88G [00:16<00:04, 92.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  77%|███████▋  | 1.45G/1.88G [00:16<00:04, 93.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  78%|███████▊  | 1.46G/1.88G [00:16<00:04, 94.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  78%|███████▊  | 1.47G/1.88G [00:16<00:04, 92.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  79%|███████▊  | 1.48G/1.88G [00:16<00:04, 88.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  79%|███████▉  | 1.49G/1.88G [00:17<00:04, 80.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  80%|███████▉  | 1.50G/1.88G [00:17<00:04, 85.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  80%|████████  | 1.51G/1.88G [00:17<00:04, 87.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  81%|████████  | 1.52G/1.88G [00:17<00:04, 88.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  81%|████████▏ | 1.53G/1.88G [00:17<00:04, 86.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  82%|████████▏ | 1.54G/1.88G [00:17<00:03, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  83%|████████▎ | 1.55G/1.88G [00:17<00:03, 89.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  83%|████████▎ | 1.56G/1.88G [00:17<00:03, 91.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  84%|████████▎ | 1.57G/1.88G [00:17<00:03, 92.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  84%|████████▍ | 1.58G/1.88G [00:18<00:03, 91.1MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  85%|████████▍ | 1.59G/1.88G [00:18<00:03, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  85%|████████▌ | 1.60G/1.88G [00:18<00:03, 90.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  86%|████████▌ | 1.61G/1.88G [00:18<00:02, 92.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  86%|████████▋ | 1.63G/1.88G [00:18<00:02, 88.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  87%|████████▋ | 1.64G/1.88G [00:18<00:02, 82.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  88%|████████▊ | 1.65G/1.88G [00:18<00:02, 84.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  88%|████████▊ | 1.66G/1.88G [00:18<00:02, 86.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  89%|████████▊ | 1.67G/1.88G [00:19<00:02, 89.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  89%|████████▉ | 1.68G/1.88G [00:19<00:02, 88.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  90%|████████▉ | 1.69G/1.88G [00:19<00:02, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  90%|█████████ | 1.70G/1.88G [00:19<00:01, 91.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  91%|█████████ | 1.71G/1.88G [00:19<00:01, 92.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  91%|█████████▏| 1.72G/1.88G [00:19<00:01, 90.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  92%|█████████▏| 1.73G/1.88G [00:19<00:01, 89.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  93%|█████████▎| 1.74G/1.88G [00:19<00:01, 87.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  93%|█████████▎| 1.75G/1.88G [00:20<00:01, 89.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  94%|█████████▎| 1.76G/1.88G [00:20<00:01, 89.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  94%|█████████▍| 1.77G/1.88G [00:20<00:01, 90.5MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  95%|█████████▍| 1.78G/1.88G [00:20<00:01, 91.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  95%|█████████▌| 1.79G/1.88G [00:20<00:00, 93.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  96%|█████████▌| 1.80G/1.88G [00:20<00:00, 91.9MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  97%|█████████▋| 1.81G/1.88G [00:20<00:00, 92.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  97%|█████████▋| 1.82G/1.88G [00:20<00:00, 91.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  98%|█████████▊| 1.84G/1.88G [00:20<00:00, 91.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  98%|█████████▊| 1.85G/1.88G [00:21<00:00, 85.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  99%|█████████▊| 1.86G/1.88G [00:21<00:00, 88.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  99%|█████████▉| 1.87G/1.88G [00:21<00:00, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin: 100%|█████████▉| 1.88G/1.88G [00:21<00:00, 85.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin: 100%|██████████| 1.88G/1.88G [00:21<00:00, 87.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 2/8 [00:41<02:04, 20.82s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 2/8 [00:41<02:04, 20.82s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 2/8 [00:41<02:04, 20.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 2/8 [00:41<02:04, 20.80s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 2/8 [00:41<02:04, 20.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 2/8 [00:41<02:04, 20.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 2/8 [00:41<02:05, 20.84s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 2/8 [00:41<02:05, 20.83s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   1%|          | 10.5M/1.98G [00:00<00:24, 80.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   1%|          | 21.0M/1.98G [00:00<00:22, 85.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   2%|▏         | 31.5M/1.98G [00:00<00:21, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   2%|▏         | 41.9M/1.98G [00:00<00:21, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   3%|▎         | 52.4M/1.98G [00:00<00:25, 76.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   3%|▎         | 62.9M/1.98G [00:00<00:23, 80.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   4%|▎         | 73.4M/1.98G [00:00<00:23, 82.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   4%|▍         | 83.9M/1.98G [00:01<00:22, 82.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   5%|▍         | 94.4M/1.98G [00:01<00:22, 84.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   5%|▌         | 105M/1.98G [00:01<00:21, 86.8MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   6%|▌         | 115M/1.98G [00:01<00:21, 88.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   6%|▋         | 126M/1.98G [00:01<00:20, 88.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   7%|▋         | 136M/1.98G [00:01<00:20, 88.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   7%|▋         | 147M/1.98G [00:01<00:23, 78.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   8%|▊         | 157M/1.98G [00:01<00:22, 81.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   8%|▊         | 168M/1.98G [00:02<00:21, 82.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   9%|▉         | 178M/1.98G [00:02<00:21, 84.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  10%|▉         | 189M/1.98G [00:02<00:20, 86.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  10%|█         | 199M/1.98G [00:02<00:20, 88.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  11%|█         | 210M/1.98G [00:02<00:19, 89.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  11%|█         | 220M/1.98G [00:02<00:19, 88.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  12%|█▏        | 231M/1.98G [00:02<00:20, 86.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  12%|█▏        | 241M/1.98G [00:02<00:21, 80.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  13%|█▎        | 252M/1.98G [00:02<00:21, 80.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  13%|█▎        | 262M/1.98G [00:03<00:20, 84.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  14%|█▍        | 273M/1.98G [00:03<00:19, 86.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  14%|█▍        | 283M/1.98G [00:03<00:19, 87.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  15%|█▍        | 294M/1.98G [00:03<00:18, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  15%|█▌        | 304M/1.98G [00:03<00:18, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  16%|█▌        | 315M/1.98G [00:03<00:18, 92.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  16%|█▋        | 325M/1.98G [00:03<00:17, 92.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  17%|█▋        | 336M/1.98G [00:03<00:19, 83.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  17%|█▋        | 346M/1.98G [00:04<00:19, 85.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  18%|█▊        | 357M/1.98G [00:04<00:18, 87.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  19%|█▊        | 367M/1.98G [00:04<00:19, 81.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  19%|█▉        | 377M/1.98G [00:04<00:18, 84.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  20%|█▉        | 388M/1.98G [00:04<00:18, 87.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  20%|██        | 398M/1.98G [00:04<00:17, 88.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  21%|██        | 409M/1.98G [00:04<00:17, 88.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  21%|██        | 419M/1.98G [00:04<00:17, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  22%|██▏       | 430M/1.98G [00:05<00:18, 81.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  22%|██▏       | 440M/1.98G [00:05<00:18, 84.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  23%|██▎       | 451M/1.98G [00:05<00:17, 87.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  23%|██▎       | 461M/1.98G [00:05<00:17, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  24%|██▍       | 472M/1.98G [00:05<00:16, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  24%|██▍       | 482M/1.98G [00:05<00:16, 92.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  25%|██▍       | 493M/1.98G [00:05<00:16, 92.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  25%|██▌       | 503M/1.98G [00:05<00:16, 92.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  26%|██▌       | 514M/1.98G [00:05<00:17, 83.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  26%|██▋       | 524M/1.98G [00:06<00:17, 84.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  27%|██▋       | 535M/1.98G [00:06<00:17, 80.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  28%|██▊       | 545M/1.98G [00:06<00:17, 84.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  28%|██▊       | 556M/1.98G [00:06<00:16, 87.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  29%|██▊       | 566M/1.98G [00:06<00:15, 88.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  29%|██▉       | 577M/1.98G [00:06<00:15, 90.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  30%|██▉       | 587M/1.98G [00:06<00:15, 91.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  30%|███       | 598M/1.98G [00:06<00:15, 91.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  31%|███       | 608M/1.98G [00:07<00:17, 80.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  31%|███       | 619M/1.98G [00:07<00:16, 84.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  32%|███▏      | 629M/1.98G [00:07<00:15, 87.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  32%|███▏      | 640M/1.98G [00:07<00:14, 89.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  33%|███▎      | 650M/1.98G [00:07<00:15, 88.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  33%|███▎      | 661M/1.98G [00:07<00:14, 89.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  34%|███▍      | 671M/1.98G [00:07<00:14, 91.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  34%|███▍      | 682M/1.98G [00:07<00:14, 90.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  35%|███▍      | 692M/1.98G [00:07<00:14, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  35%|███▌      | 703M/1.98G [00:08<00:15, 82.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  36%|███▌      | 713M/1.98G [00:08<00:14, 85.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  37%|███▋      | 724M/1.98G [00:08<00:14, 83.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  37%|███▋      | 734M/1.98G [00:08<00:14, 86.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  38%|███▊      | 744M/1.98G [00:08<00:13, 88.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  38%|███▊      | 755M/1.98G [00:08<00:14, 85.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  39%|███▊      | 765M/1.98G [00:08<00:14, 86.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  39%|███▉      | 776M/1.98G [00:08<00:13, 88.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  40%|███▉      | 786M/1.98G [00:09<00:13, 89.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  40%|████      | 797M/1.98G [00:09<00:14, 83.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  41%|████      | 807M/1.98G [00:09<00:13, 84.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  41%|████▏     | 818M/1.98G [00:09<00:13, 88.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  42%|████▏     | 828M/1.98G [00:09<00:12, 90.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  42%|████▏     | 839M/1.98G [00:09<00:12, 91.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  43%|████▎     | 849M/1.98G [00:09<00:12, 92.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  43%|████▎     | 860M/1.98G [00:09<00:12, 91.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  44%|████▍     | 870M/1.98G [00:10<00:11, 93.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  44%|████▍     | 881M/1.98G [00:10<00:14, 78.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  45%|████▌     | 891M/1.98G [00:10<00:14, 76.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  46%|████▌     | 902M/1.98G [00:10<00:13, 80.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  46%|████▌     | 912M/1.98G [00:10<00:12, 85.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  47%|████▋     | 923M/1.98G [00:10<00:12, 87.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  47%|████▋     | 933M/1.98G [00:10<00:11, 88.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  48%|████▊     | 944M/1.98G [00:10<00:11, 88.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  48%|████▊     | 954M/1.98G [00:11<00:12, 84.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  49%|████▊     | 965M/1.98G [00:11<00:11, 87.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  49%|████▉     | 975M/1.98G [00:11<00:11, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  50%|████▉     | 986M/1.98G [00:11<00:10, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  50%|█████     | 996M/1.98G [00:11<00:12, 81.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  51%|█████     | 1.01G/1.98G [00:11<00:11, 84.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  51%|█████▏    | 1.02G/1.98G [00:11<00:11, 87.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  52%|█████▏    | 1.03G/1.98G [00:11<00:10, 88.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  52%|█████▏    | 1.04G/1.98G [00:11<00:10, 89.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  53%|█████▎    | 1.05G/1.98G [00:12<00:10, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  53%|█████▎    | 1.06G/1.98G [00:12<00:09, 92.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  54%|█████▍    | 1.07G/1.98G [00:12<00:14, 63.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  55%|█████▌    | 1.09G/1.98G [00:12<00:10, 86.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  56%|█████▌    | 1.10G/1.98G [00:12<00:10, 86.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  56%|█████▌    | 1.11G/1.98G [00:12<00:09, 87.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  57%|█████▋    | 1.12G/1.98G [00:12<00:09, 87.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  57%|█████▋    | 1.13G/1.98G [00:13<00:09, 88.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  58%|█████▊    | 1.14G/1.98G [00:13<00:09, 89.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  58%|█████▊    | 1.15G/1.98G [00:13<00:09, 90.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  59%|█████▉    | 1.16G/1.98G [00:13<00:09, 85.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  59%|█████▉    | 1.17G/1.98G [00:13<00:09, 87.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  60%|█████▉    | 1.18G/1.98G [00:13<00:09, 81.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  60%|██████    | 1.20G/1.98G [00:13<00:09, 84.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  61%|██████    | 1.21G/1.98G [00:13<00:08, 86.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  61%|██████▏   | 1.22G/1.98G [00:14<00:09, 84.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  62%|██████▏   | 1.23G/1.98G [00:14<00:08, 86.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  62%|██████▏   | 1.24G/1.98G [00:14<00:08, 87.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  63%|██████▎   | 1.25G/1.98G [00:14<00:08, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  64%|██████▎   | 1.26G/1.98G [00:14<00:07, 90.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  64%|██████▍   | 1.27G/1.98G [00:14<00:07, 92.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  65%|██████▍   | 1.28G/1.98G [00:14<00:07, 91.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  65%|██████▌   | 1.29G/1.98G [00:14<00:08, 81.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  66%|██████▌   | 1.30G/1.98G [00:15<00:07, 85.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  66%|██████▌   | 1.31G/1.98G [00:15<00:07, 84.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  67%|██████▋   | 1.32G/1.98G [00:15<00:07, 86.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  67%|██████▋   | 1.33G/1.98G [00:15<00:07, 89.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  68%|██████▊   | 1.34G/1.98G [00:15<00:07, 88.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  68%|██████▊   | 1.35G/1.98G [00:15<00:07, 81.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  69%|██████▉   | 1.36G/1.98G [00:15<00:07, 84.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  69%|██████▉   | 1.37G/1.98G [00:15<00:07, 85.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  70%|██████▉   | 1.38G/1.98G [00:16<00:07, 79.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  70%|███████   | 1.39G/1.98G [00:16<00:07, 82.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  71%|███████   | 1.41G/1.98G [00:16<00:06, 84.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  71%|███████▏  | 1.42G/1.98G [00:16<00:06, 86.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  72%|███████▏  | 1.43G/1.98G [00:16<00:06, 88.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  73%|███████▎  | 1.44G/1.98G [00:16<00:06, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  73%|███████▎  | 1.45G/1.98G [00:16<00:06, 87.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  74%|███████▎  | 1.46G/1.98G [00:16<00:05, 88.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  74%|███████▍  | 1.47G/1.98G [00:17<00:06, 78.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  75%|███████▍  | 1.48G/1.98G [00:17<00:06, 82.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  75%|███████▌  | 1.49G/1.98G [00:17<00:05, 83.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  76%|███████▌  | 1.50G/1.98G [00:17<00:05, 84.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  76%|███████▌  | 1.51G/1.98G [00:17<00:05, 87.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  77%|███████▋  | 1.52G/1.98G [00:17<00:05, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  77%|███████▋  | 1.53G/1.98G [00:17<00:04, 90.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  78%|███████▊  | 1.54G/1.98G [00:17<00:04, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  78%|███████▊  | 1.55G/1.98G [00:17<00:04, 92.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  79%|███████▉  | 1.56G/1.98G [00:18<00:04, 92.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  79%|███████▉  | 1.57G/1.98G [00:18<00:04, 82.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  80%|███████▉  | 1.58G/1.98G [00:18<00:04, 85.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  80%|████████  | 1.59G/1.98G [00:18<00:04, 87.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  81%|████████  | 1.60G/1.98G [00:18<00:04, 89.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  82%|████████▏ | 1.61G/1.98G [00:18<00:03, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  82%|████████▏ | 1.63G/1.98G [00:18<00:03, 93.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  83%|████████▎ | 1.64G/1.98G [00:18<00:03, 93.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  83%|████████▎ | 1.65G/1.98G [00:18<00:03, 94.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  84%|████████▎ | 1.66G/1.98G [00:19<00:03, 93.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  84%|████████▍ | 1.67G/1.98G [00:19<00:03, 85.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  85%|████████▍ | 1.68G/1.98G [00:19<00:03, 87.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  85%|████████▌ | 1.69G/1.98G [00:19<00:03, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  86%|████████▌ | 1.70G/1.98G [00:19<00:03, 90.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  86%|████████▋ | 1.71G/1.98G [00:19<00:02, 92.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  87%|████████▋ | 1.72G/1.98G [00:19<00:02, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  87%|████████▋ | 1.73G/1.98G [00:19<00:02, 91.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  88%|████████▊ | 1.74G/1.98G [00:20<00:02, 92.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  88%|████████▊ | 1.75G/1.98G [00:20<00:02, 90.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  89%|████████▉ | 1.76G/1.98G [00:20<00:02, 76.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  89%|████████▉ | 1.77G/1.98G [00:20<00:02, 81.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  90%|█████████ | 1.78G/1.98G [00:20<00:02, 84.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  91%|█████████ | 1.79G/1.98G [00:20<00:02, 83.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  91%|█████████ | 1.80G/1.98G [00:20<00:02, 85.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  92%|█████████▏| 1.81G/1.98G [00:20<00:01, 87.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  92%|█████████▏| 1.82G/1.98G [00:21<00:01, 90.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  93%|█████████▎| 1.84G/1.98G [00:21<00:01, 90.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  93%|█████████▎| 1.85G/1.98G [00:21<00:01, 92.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  94%|█████████▎| 1.86G/1.98G [00:21<00:01, 88.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  94%|█████████▍| 1.87G/1.98G [00:21<00:01, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  95%|█████████▍| 1.88G/1.98G [00:21<00:01, 84.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  95%|█████████▌| 1.89G/1.98G [00:21<00:01, 81.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  96%|█████████▌| 1.90G/1.98G [00:21<00:00, 83.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  96%|█████████▋| 1.91G/1.98G [00:22<00:00, 86.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  97%|█████████▋| 1.92G/1.98G [00:22<00:00, 88.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  97%|█████████▋| 1.93G/1.98G [00:22<00:00, 89.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  98%|█████████▊| 1.94G/1.98G [00:22<00:00, 88.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  98%|█████████▊| 1.95G/1.98G [00:22<00:00, 88.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  99%|█████████▉| 1.96G/1.98G [00:22<00:00, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin: 100%|█████████▉| 1.97G/1.98G [00:22<00:00, 81.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin: 100%|██████████| 1.98G/1.98G [00:22<00:00, 86.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  38%|███▊      | 3/8 [01:04<01:48, 21.80s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  38%|███▊      | 3/8 [01:04<01:48, 21.79s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  38%|███▊      | 3/8 [01:04<01:48, 21.79s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  38%|███▊      | 3/8 [01:04<01:49, 21.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  38%|███▊      | 3/8 [01:04<01:49, 21.80s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  38%|███▊      | 3/8 [01:04<01:49, 21.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  38%|███▊      | 3/8 [01:04<01:49, 21.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  38%|███▊      | 3/8 [01:04<01:49, 21.80s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   0%|          | 0.00/1.91G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   2%|▏         | 41.9M/1.91G [00:00<00:05, 355MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   4%|▍         | 83.9M/1.91G [00:00<00:04, 377MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   7%|▋         | 126M/1.91G [00:00<00:04, 385MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   9%|▉         | 168M/1.91G [00:00<00:04, 390MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  11%|█         | 210M/1.91G [00:00<00:04, 392MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  13%|█▎        | 252M/1.91G [00:00<00:04, 397MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  15%|█▌        | 294M/1.91G [00:00<00:04, 399MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  18%|█▊        | 336M/1.91G [00:00<00:03, 400MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  20%|█▉        | 377M/1.91G [00:00<00:03, 397MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  22%|██▏       | 419M/1.91G [00:01<00:03, 399MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  24%|██▍       | 461M/1.91G [00:01<00:03, 399MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  26%|██▋       | 503M/1.91G [00:01<00:03, 399MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  28%|██▊       | 545M/1.91G [00:01<00:03, 399MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  31%|███       | 587M/1.91G [00:01<00:03, 398MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  33%|███▎      | 629M/1.91G [00:01<00:03, 398MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  35%|███▌      | 671M/1.91G [00:01<00:03, 399MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  37%|███▋      | 713M/1.91G [00:01<00:02, 401MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  39%|███▉      | 755M/1.91G [00:01<00:02, 402MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  42%|████▏     | 797M/1.91G [00:02<00:02, 398MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  44%|████▍     | 839M/1.91G [00:02<00:02, 397MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  46%|████▌     | 881M/1.91G [00:02<00:02, 397MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  48%|████▊     | 923M/1.91G [00:02<00:02, 397MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  50%|█████     | 965M/1.91G [00:02<00:02, 398MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  53%|█████▎    | 1.01G/1.91G [00:02<00:02, 397MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  55%|█████▍    | 1.05G/1.91G [00:02<00:02, 396MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  57%|█████▋    | 1.09G/1.91G [00:02<00:02, 397MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  59%|█████▉    | 1.13G/1.91G [00:02<00:01, 395MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  61%|██████▏   | 1.17G/1.91G [00:02<00:01, 398MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  64%|██████▎   | 1.22G/1.91G [00:03<00:01, 400MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  66%|██████▌   | 1.26G/1.91G [00:03<00:01, 400MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  68%|██████▊   | 1.30G/1.91G [00:03<00:01, 383MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  70%|███████   | 1.34G/1.91G [00:03<00:01, 385MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  72%|███████▏  | 1.38G/1.91G [00:03<00:01, 386MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  75%|███████▍  | 1.43G/1.91G [00:03<00:01, 391MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  77%|███████▋  | 1.47G/1.91G [00:03<00:01, 393MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  79%|███████▉  | 1.51G/1.91G [00:03<00:01, 394MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  81%|████████  | 1.55G/1.91G [00:03<00:00, 397MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  83%|████████▎ | 1.59G/1.91G [00:04<00:00, 397MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  85%|████████▌ | 1.64G/1.91G [00:04<00:00, 398MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  88%|████████▊ | 1.68G/1.91G [00:04<00:00, 397MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  90%|████████▉ | 1.72G/1.91G [00:04<00:00, 399MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  92%|█████████▏| 1.76G/1.91G [00:04<00:00, 398MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  94%|█████████▍| 1.80G/1.91G [00:04<00:00, 246MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  96%|█████████▋| 1.85G/1.91G [00:04<00:00, 278MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  99%|█████████▊| 1.89G/1.91G [00:04<00:00, 306MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin: 100%|██████████| 1.91G/1.91G [00:05<00:00, 378MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 4/8 [01:09<01:00, 15.23s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 4/8 [01:09<01:00, 15.22s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 4/8 [01:09<01:00, 15.22s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 4/8 [01:09<01:00, 15.23s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 4/8 [01:09<01:00, 15.23s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 4/8 [01:09<01:00, 15.23s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 4/8 [01:09<01:00, 15.23s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 4/8 [01:09<01:00, 15.23s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   1%|          | 10.5M/1.88G [00:00<00:22, 84.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   1%|          | 21.0M/1.88G [00:00<00:20, 88.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   2%|▏         | 31.5M/1.88G [00:00<00:20, 91.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   2%|▏         | 41.9M/1.88G [00:00<00:19, 92.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   3%|▎         | 52.4M/1.88G [00:00<00:19, 93.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   3%|▎         | 62.9M/1.88G [00:00<00:19, 93.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   4%|▍         | 73.4M/1.88G [00:00<00:19, 93.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   4%|▍         | 83.9M/1.88G [00:00<00:19, 93.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   5%|▌         | 94.4M/1.88G [00:01<00:20, 88.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   6%|▌         | 105M/1.88G [00:01<00:19, 90.3MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   6%|▌         | 115M/1.88G [00:01<00:19, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   7%|▋         | 126M/1.88G [00:01<00:19, 90.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   7%|▋         | 136M/1.88G [00:01<00:19, 91.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   8%|▊         | 147M/1.88G [00:01<00:19, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   8%|▊         | 157M/1.88G [00:01<00:18, 92.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   9%|▉         | 168M/1.88G [00:01<00:18, 92.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   9%|▉         | 178M/1.88G [00:01<00:19, 89.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  10%|█         | 189M/1.88G [00:02<00:18, 89.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  11%|█         | 199M/1.88G [00:02<00:19, 87.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  11%|█         | 210M/1.88G [00:02<00:19, 87.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  12%|█▏        | 220M/1.88G [00:02<00:19, 83.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  12%|█▏        | 231M/1.88G [00:02<00:19, 83.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  13%|█▎        | 241M/1.88G [00:02<00:19, 85.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  13%|█▎        | 252M/1.88G [00:02<00:18, 87.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  14%|█▍        | 262M/1.88G [00:02<00:18, 87.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  15%|█▍        | 273M/1.88G [00:03<00:17, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  15%|█▌        | 283M/1.88G [00:03<00:17, 92.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  16%|█▌        | 294M/1.88G [00:03<00:17, 92.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  16%|█▌        | 304M/1.88G [00:03<00:16, 93.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  17%|█▋        | 315M/1.88G [00:03<00:17, 92.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  17%|█▋        | 325M/1.88G [00:03<00:17, 87.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  18%|█▊        | 336M/1.88G [00:03<00:17, 88.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  18%|█▊        | 346M/1.88G [00:03<00:17, 88.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  19%|█▉        | 357M/1.88G [00:03<00:16, 90.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  20%|█▉        | 367M/1.88G [00:04<00:17, 84.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  20%|██        | 377M/1.88G [00:04<00:17, 87.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  21%|██        | 388M/1.88G [00:04<00:16, 88.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  21%|██        | 398M/1.88G [00:04<00:16, 89.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  22%|██▏       | 409M/1.88G [00:04<00:16, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  22%|██▏       | 419M/1.88G [00:04<00:16, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  23%|██▎       | 430M/1.88G [00:04<00:15, 90.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  23%|██▎       | 440M/1.88G [00:04<00:16, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  24%|██▍       | 451M/1.88G [00:05<00:15, 90.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  25%|██▍       | 461M/1.88G [00:05<00:15, 92.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  25%|██▌       | 472M/1.88G [00:05<00:15, 93.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  26%|██▌       | 482M/1.88G [00:05<00:15, 93.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  26%|██▌       | 493M/1.88G [00:05<00:15, 88.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  27%|██▋       | 503M/1.88G [00:05<00:15, 89.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  27%|██▋       | 514M/1.88G [00:05<00:15, 88.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  28%|██▊       | 524M/1.88G [00:05<00:14, 90.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  28%|██▊       | 535M/1.88G [00:05<00:14, 92.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  29%|██▉       | 545M/1.88G [00:06<00:14, 90.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  30%|██▉       | 556M/1.88G [00:06<00:14, 92.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  30%|███       | 566M/1.88G [00:06<00:14, 93.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  31%|███       | 577M/1.88G [00:06<00:14, 91.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  31%|███       | 587M/1.88G [00:06<00:14, 91.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  32%|███▏      | 598M/1.88G [00:06<00:13, 92.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  32%|███▏      | 608M/1.88G [00:06<00:13, 92.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  33%|███▎      | 619M/1.88G [00:06<00:13, 90.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  33%|███▎      | 629M/1.88G [00:06<00:13, 90.0MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  34%|███▍      | 640M/1.88G [00:07<00:13, 91.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  35%|███▍      | 650M/1.88G [00:07<00:13, 90.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  35%|███▌      | 661M/1.88G [00:07<00:13, 92.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  36%|███▌      | 671M/1.88G [00:07<00:13, 90.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  36%|███▋      | 682M/1.88G [00:07<00:13, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  37%|███▋      | 692M/1.88G [00:07<00:13, 90.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  37%|███▋      | 703M/1.88G [00:07<00:12, 92.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  38%|███▊      | 713M/1.88G [00:07<00:13, 87.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  38%|███▊      | 724M/1.88G [00:08<00:13, 88.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  39%|███▉      | 734M/1.88G [00:08<00:12, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  40%|███▉      | 744M/1.88G [00:08<00:12, 88.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  40%|████      | 755M/1.88G [00:08<00:12, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  41%|████      | 765M/1.88G [00:08<00:12, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  41%|████▏     | 776M/1.88G [00:08<00:12, 91.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  42%|████▏     | 786M/1.88G [00:08<00:11, 92.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  42%|████▏     | 797M/1.88G [00:08<00:11, 93.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  43%|████▎     | 807M/1.88G [00:08<00:11, 93.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  44%|████▎     | 818M/1.88G [00:09<00:11, 92.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  44%|████▍     | 828M/1.88G [00:09<00:11, 92.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  45%|████▍     | 839M/1.88G [00:09<00:11, 91.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  45%|████▌     | 849M/1.88G [00:09<00:11, 93.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  46%|████▌     | 860M/1.88G [00:09<00:10, 94.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  46%|████▋     | 870M/1.88G [00:09<00:10, 95.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  47%|████▋     | 881M/1.88G [00:09<00:10, 95.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  47%|████▋     | 891M/1.88G [00:09<00:10, 93.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  48%|████▊     | 902M/1.88G [00:09<00:10, 94.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  49%|████▊     | 912M/1.88G [00:10<00:10, 90.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  49%|████▉     | 923M/1.88G [00:10<00:10, 91.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  50%|████▉     | 933M/1.88G [00:10<00:10, 91.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  50%|█████     | 944M/1.88G [00:10<00:10, 92.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  51%|█████     | 954M/1.88G [00:10<00:09, 93.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  51%|█████▏    | 965M/1.88G [00:10<00:09, 93.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  52%|█████▏    | 975M/1.88G [00:10<00:09, 92.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  52%|█████▏    | 986M/1.88G [00:10<00:09, 93.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  53%|█████▎    | 996M/1.88G [00:10<00:09, 93.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  54%|█████▎    | 1.01G/1.88G [00:11<00:09, 92.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  54%|█████▍    | 1.02G/1.88G [00:11<00:09, 87.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  55%|█████▍    | 1.03G/1.88G [00:11<00:09, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  55%|█████▌    | 1.04G/1.88G [00:11<00:09, 90.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  56%|█████▌    | 1.05G/1.88G [00:11<00:09, 92.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  56%|█████▋    | 1.06G/1.88G [00:11<00:08, 92.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  57%|█████▋    | 1.07G/1.88G [00:11<00:08, 94.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  57%|█████▋    | 1.08G/1.88G [00:11<00:08, 93.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  58%|█████▊    | 1.09G/1.88G [00:11<00:08, 93.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  59%|█████▊    | 1.10G/1.88G [00:12<00:08, 93.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  59%|█████▉    | 1.11G/1.88G [00:12<00:08, 92.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  60%|█████▉    | 1.12G/1.88G [00:12<00:08, 94.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  60%|██████    | 1.13G/1.88G [00:12<00:07, 94.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  61%|██████    | 1.14G/1.88G [00:12<00:07, 93.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  61%|██████▏   | 1.15G/1.88G [00:12<00:07, 92.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  62%|██████▏   | 1.16G/1.88G [00:12<00:07, 93.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  62%|██████▏   | 1.17G/1.88G [00:12<00:07, 93.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  63%|██████▎   | 1.18G/1.88G [00:13<00:07, 93.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  64%|██████▎   | 1.20G/1.88G [00:13<00:07, 93.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  64%|██████▍   | 1.21G/1.88G [00:13<00:07, 93.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  65%|██████▍   | 1.22G/1.88G [00:13<00:07, 92.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  65%|██████▌   | 1.23G/1.88G [00:13<00:07, 92.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  66%|██████▌   | 1.24G/1.88G [00:13<00:06, 94.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  66%|██████▋   | 1.25G/1.88G [00:13<00:06, 95.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  67%|██████▋   | 1.26G/1.88G [00:13<00:06, 96.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  67%|██████▋   | 1.27G/1.88G [00:13<00:06, 95.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  68%|██████▊   | 1.28G/1.88G [00:14<00:06, 91.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  69%|██████▊   | 1.29G/1.88G [00:14<00:06, 90.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  69%|██████▉   | 1.30G/1.88G [00:14<00:06, 84.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  70%|██████▉   | 1.31G/1.88G [00:14<00:06, 86.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  70%|███████   | 1.32G/1.88G [00:14<00:06, 88.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  71%|███████   | 1.33G/1.88G [00:14<00:06, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  71%|███████▏  | 1.34G/1.88G [00:14<00:06, 87.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  72%|███████▏  | 1.35G/1.88G [00:14<00:05, 90.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  73%|███████▎  | 1.36G/1.88G [00:14<00:05, 90.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  73%|███████▎  | 1.37G/1.88G [00:15<00:05, 92.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  74%|███████▎  | 1.38G/1.88G [00:15<00:05, 90.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  74%|███████▍  | 1.39G/1.88G [00:15<00:05, 87.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  75%|███████▍  | 1.41G/1.88G [00:15<00:05, 88.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  75%|███████▌  | 1.42G/1.88G [00:15<00:05, 90.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  76%|███████▌  | 1.43G/1.88G [00:15<00:05, 82.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  76%|███████▋  | 1.44G/1.88G [00:15<00:05, 74.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  77%|███████▋  | 1.45G/1.88G [00:16<00:05, 77.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  78%|███████▊  | 1.46G/1.88G [00:16<00:05, 78.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  78%|███████▊  | 1.47G/1.88G [00:16<00:05, 81.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  79%|███████▊  | 1.48G/1.88G [00:16<00:04, 82.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  79%|███████▉  | 1.49G/1.88G [00:16<00:04, 79.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  80%|███████▉  | 1.50G/1.88G [00:16<00:04, 80.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  80%|████████  | 1.51G/1.88G [00:16<00:04, 83.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  81%|████████  | 1.52G/1.88G [00:16<00:04, 85.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  81%|████████▏ | 1.53G/1.88G [00:16<00:04, 86.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  82%|████████▏ | 1.54G/1.88G [00:17<00:04, 83.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  83%|████████▎ | 1.55G/1.88G [00:17<00:03, 83.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  83%|████████▎ | 1.56G/1.88G [00:17<00:03, 83.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  84%|████████▎ | 1.57G/1.88G [00:17<00:03, 85.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  84%|████████▍ | 1.58G/1.88G [00:17<00:03, 84.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  85%|████████▍ | 1.59G/1.88G [00:17<00:03, 85.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  85%|████████▌ | 1.60G/1.88G [00:17<00:03, 87.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  86%|████████▌ | 1.61G/1.88G [00:18<00:03, 82.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  86%|████████▋ | 1.63G/1.88G [00:18<00:03, 82.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  87%|████████▋ | 1.64G/1.88G [00:18<00:04, 60.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  88%|████████▊ | 1.66G/1.88G [00:18<00:02, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  89%|████████▉ | 1.68G/1.88G [00:18<00:02, 85.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  90%|████████▉ | 1.69G/1.88G [00:18<00:02, 85.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  90%|█████████ | 1.70G/1.88G [00:19<00:02, 84.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  91%|█████████ | 1.71G/1.88G [00:19<00:02, 83.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  91%|█████████▏| 1.72G/1.88G [00:19<00:01, 80.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  92%|█████████▏| 1.73G/1.88G [00:19<00:01, 82.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  93%|█████████▎| 1.74G/1.88G [00:19<00:01, 84.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  93%|█████████▎| 1.75G/1.88G [00:19<00:01, 79.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  94%|█████████▎| 1.76G/1.88G [00:19<00:01, 82.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  94%|█████████▍| 1.77G/1.88G [00:19<00:01, 77.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  95%|█████████▍| 1.78G/1.88G [00:20<00:01, 76.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  95%|█████████▌| 1.79G/1.88G [00:20<00:01, 78.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  96%|█████████▌| 1.80G/1.88G [00:20<00:00, 82.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  97%|█████████▋| 1.81G/1.88G [00:20<00:00, 82.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  97%|█████████▋| 1.82G/1.88G [00:20<00:00, 83.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  98%|█████████▊| 1.84G/1.88G [00:20<00:00, 85.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  98%|█████████▊| 1.85G/1.88G [00:20<00:00, 87.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  99%|█████████▊| 1.86G/1.88G [00:20<00:00, 89.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  99%|█████████▉| 1.87G/1.88G [00:21<00:00, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin: 100%|█████████▉| 1.88G/1.88G [00:21<00:00, 88.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin: 100%|██████████| 1.88G/1.88G [00:21<00:00, 88.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  62%|██████▎   | 5/8 [01:30<00:52, 17.42s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  62%|██████▎   | 5/8 [01:30<00:52, 17.41s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  62%|██████▎   | 5/8 [01:30<00:52, 17.41s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  62%|██████▎   | 5/8 [01:30<00:52, 17.42s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  62%|██████▎   | 5/8 [01:30<00:52, 17.42s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  62%|██████▎   | 5/8 [01:30<00:52, 17.43s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  62%|██████▎   | 5/8 [01:30<00:52, 17.43s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  62%|██████▎   | 5/8 [01:30<00:52, 17.43s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   2%|▏         | 41.9M/1.88G [00:00<00:04, 389MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   4%|▍         | 83.9M/1.88G [00:00<00:04, 397MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   7%|▋         | 126M/1.88G [00:00<00:04, 403MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   9%|▉         | 168M/1.88G [00:00<00:04, 408MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  11%|█         | 210M/1.88G [00:00<00:04, 407MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  13%|█▎        | 252M/1.88G [00:00<00:04, 405MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  16%|█▌        | 294M/1.88G [00:00<00:03, 398MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  18%|█▊        | 336M/1.88G [00:00<00:04, 382MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  20%|██        | 377M/1.88G [00:00<00:03, 387MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  22%|██▏       | 419M/1.88G [00:01<00:03, 391MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  25%|██▍       | 461M/1.88G [00:01<00:03, 394MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  27%|██▋       | 503M/1.88G [00:01<00:03, 400MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  29%|██▉       | 545M/1.88G [00:01<00:03, 400MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  31%|███       | 587M/1.88G [00:01<00:03, 405MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  33%|███▎      | 629M/1.88G [00:01<00:03, 401MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  36%|███▌      | 671M/1.88G [00:01<00:03, 399MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  38%|███▊      | 713M/1.88G [00:01<00:02, 400MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  40%|████      | 755M/1.88G [00:01<00:02, 401MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  42%|████▏     | 797M/1.88G [00:01<00:02, 403MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  45%|████▍     | 839M/1.88G [00:02<00:02, 406MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  47%|████▋     | 881M/1.88G [00:02<00:02, 405MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  49%|████▉     | 923M/1.88G [00:02<00:02, 405MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  51%|█████▏    | 965M/1.88G [00:02<00:02, 406MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  54%|█████▎    | 1.01G/1.88G [00:02<00:02, 406MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  56%|█████▌    | 1.05G/1.88G [00:02<00:02, 407MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  58%|█████▊    | 1.09G/1.88G [00:02<00:01, 408MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  60%|██████    | 1.13G/1.88G [00:02<00:01, 406MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  62%|██████▏   | 1.17G/1.88G [00:02<00:01, 403MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  65%|██████▍   | 1.22G/1.88G [00:03<00:01, 403MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  67%|██████▋   | 1.26G/1.88G [00:03<00:01, 401MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  69%|██████▉   | 1.30G/1.88G [00:03<00:01, 403MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  71%|███████▏  | 1.34G/1.88G [00:03<00:01, 404MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  74%|███████▎  | 1.38G/1.88G [00:03<00:01, 405MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  76%|███████▌  | 1.43G/1.88G [00:03<00:01, 402MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  78%|███████▊  | 1.47G/1.88G [00:03<00:01, 400MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  80%|████████  | 1.51G/1.88G [00:03<00:00, 400MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  83%|████████▎ | 1.55G/1.88G [00:03<00:00, 401MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  85%|████████▍ | 1.59G/1.88G [00:03<00:00, 400MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  87%|████████▋ | 1.64G/1.88G [00:04<00:00, 402MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  89%|████████▉ | 1.68G/1.88G [00:04<00:00, 403MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  91%|█████████▏| 1.72G/1.88G [00:04<00:00, 404MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  94%|█████████▎| 1.76G/1.88G [00:04<00:00, 405MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  96%|█████████▌| 1.80G/1.88G [00:04<00:00, 407MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  98%|█████████▊| 1.85G/1.88G [00:04<00:00, 408MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin: 100%|██████████| 1.88G/1.88G [00:04<00:00, 402MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 6/8 [01:35<00:26, 13.12s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 6/8 [01:35<00:26, 13.12s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 6/8 [01:35<00:26, 13.12s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 6/8 [01:35<00:26, 13.13s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 6/8 [01:35<00:26, 13.12s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 6/8 [01:35<00:26, 13.13s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 6/8 [01:35<00:26, 13.13s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 6/8 [01:35<00:26, 13.13s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   0%|          | 0.00/1.07G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   1%|          | 10.5M/1.07G [00:00<00:12, 83.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   2%|▏         | 21.0M/1.07G [00:00<00:11, 89.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   3%|▎         | 31.5M/1.07G [00:00<00:11, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   4%|▍         | 41.9M/1.07G [00:00<00:11, 91.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   5%|▍         | 52.4M/1.07G [00:00<00:11, 91.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   6%|▌         | 62.9M/1.07G [00:00<00:11, 89.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   7%|▋         | 73.4M/1.07G [00:00<00:11, 87.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   8%|▊         | 83.9M/1.07G [00:00<00:11, 87.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   9%|▉         | 94.4M/1.07G [00:01<00:11, 87.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  10%|▉         | 105M/1.07G [00:01<00:10, 90.0MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  11%|█         | 115M/1.07G [00:01<00:10, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  12%|█▏        | 126M/1.07G [00:01<00:10, 89.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  13%|█▎        | 136M/1.07G [00:01<00:11, 85.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  14%|█▎        | 147M/1.07G [00:01<00:10, 87.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  15%|█▍        | 157M/1.07G [00:01<00:10, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  16%|█▌        | 168M/1.07G [00:01<00:10, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  17%|█▋        | 178M/1.07G [00:02<00:09, 90.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  18%|█▊        | 189M/1.07G [00:02<00:09, 91.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  19%|█▊        | 199M/1.07G [00:02<00:09, 92.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  20%|█▉        | 210M/1.07G [00:02<00:09, 92.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  21%|██        | 220M/1.07G [00:02<00:14, 58.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  23%|██▎       | 252M/1.07G [00:02<00:08, 92.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  24%|██▍       | 262M/1.07G [00:02<00:09, 89.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  25%|██▌       | 273M/1.07G [00:03<00:09, 88.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  26%|██▋       | 283M/1.07G [00:03<00:09, 87.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  27%|██▋       | 294M/1.07G [00:03<00:08, 88.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  28%|██▊       | 304M/1.07G [00:03<00:08, 88.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  29%|██▉       | 315M/1.07G [00:03<00:08, 90.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  30%|███       | 325M/1.07G [00:03<00:08, 91.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  31%|███       | 336M/1.07G [00:03<00:08, 91.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  32%|███▏      | 346M/1.07G [00:03<00:07, 93.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  33%|███▎      | 357M/1.07G [00:04<00:08, 86.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  34%|███▍      | 367M/1.07G [00:04<00:08, 85.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  35%|███▌      | 377M/1.07G [00:04<00:08, 86.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  36%|███▌      | 388M/1.07G [00:04<00:07, 87.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  37%|███▋      | 398M/1.07G [00:04<00:07, 88.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  38%|███▊      | 409M/1.07G [00:04<00:07, 89.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  39%|███▉      | 419M/1.07G [00:04<00:07, 86.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  40%|████      | 430M/1.07G [00:04<00:07, 88.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  41%|████      | 440M/1.07G [00:05<00:07, 87.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  42%|████▏     | 451M/1.07G [00:05<00:06, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  43%|████▎     | 461M/1.07G [00:05<00:06, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  44%|████▍     | 472M/1.07G [00:05<00:06, 90.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  45%|████▍     | 482M/1.07G [00:05<00:06, 89.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  46%|████▌     | 493M/1.07G [00:05<00:06, 88.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  47%|████▋     | 503M/1.07G [00:05<00:06, 84.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  48%|████▊     | 514M/1.07G [00:05<00:06, 84.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  49%|████▉     | 524M/1.07G [00:05<00:06, 86.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  50%|████▉     | 535M/1.07G [00:06<00:06, 87.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  51%|█████     | 545M/1.07G [00:06<00:05, 89.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  52%|█████▏    | 556M/1.07G [00:06<00:05, 88.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  53%|█████▎    | 566M/1.07G [00:06<00:05, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  54%|█████▎    | 577M/1.07G [00:06<00:05, 89.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  55%|█████▍    | 587M/1.07G [00:06<00:05, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  56%|█████▌    | 598M/1.07G [00:06<00:05, 87.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  57%|█████▋    | 608M/1.07G [00:06<00:05, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  58%|█████▊    | 619M/1.07G [00:07<00:05, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  59%|█████▊    | 629M/1.07G [00:07<00:04, 90.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  60%|█████▉    | 640M/1.07G [00:07<00:04, 89.1MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  61%|██████    | 650M/1.07G [00:07<00:04, 88.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  62%|██████▏   | 661M/1.07G [00:07<00:04, 89.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  62%|██████▏   | 671M/1.07G [00:07<00:04, 90.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  63%|██████▎   | 682M/1.07G [00:07<00:04, 91.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  64%|██████▍   | 692M/1.07G [00:07<00:04, 92.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  65%|██████▌   | 703M/1.07G [00:07<00:04, 92.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  66%|██████▋   | 713M/1.07G [00:08<00:03, 91.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  67%|██████▋   | 724M/1.07G [00:08<00:03, 89.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  68%|██████▊   | 734M/1.07G [00:08<00:03, 90.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  69%|██████▉   | 744M/1.07G [00:08<00:03, 91.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  70%|███████   | 755M/1.07G [00:08<00:03, 90.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  71%|███████▏  | 765M/1.07G [00:08<00:03, 91.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  72%|███████▏  | 776M/1.07G [00:08<00:03, 90.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  73%|███████▎  | 786M/1.07G [00:08<00:03, 81.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  74%|███████▍  | 797M/1.07G [00:09<00:03, 84.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  75%|███████▌  | 807M/1.07G [00:09<00:03, 85.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  76%|███████▌  | 818M/1.07G [00:09<00:02, 87.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  77%|███████▋  | 828M/1.07G [00:09<00:02, 87.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  78%|███████▊  | 839M/1.07G [00:09<00:02, 88.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  79%|███████▉  | 849M/1.07G [00:09<00:02, 88.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  80%|████████  | 860M/1.07G [00:09<00:02, 85.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  81%|████████  | 870M/1.07G [00:09<00:02, 86.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  82%|████████▏ | 881M/1.07G [00:09<00:02, 87.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  83%|████████▎ | 891M/1.07G [00:10<00:02, 87.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  84%|████████▍ | 902M/1.07G [00:10<00:01, 89.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  85%|████████▍ | 912M/1.07G [00:10<00:01, 90.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  86%|████████▌ | 923M/1.07G [00:10<00:01, 91.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  87%|████████▋ | 933M/1.07G [00:10<00:01, 89.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  88%|████████▊ | 944M/1.07G [00:10<00:01, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  89%|████████▉ | 954M/1.07G [00:10<00:01, 86.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  90%|████████▉ | 965M/1.07G [00:10<00:01, 88.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  91%|█████████ | 975M/1.07G [00:11<00:01, 89.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  92%|█████████▏| 986M/1.07G [00:11<00:00, 91.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  93%|█████████▎| 996M/1.07G [00:11<00:00, 92.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  94%|█████████▎| 1.01G/1.07G [00:11<00:00, 92.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  95%|█████████▍| 1.02G/1.07G [00:11<00:00, 93.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  96%|█████████▌| 1.03G/1.07G [00:11<00:00, 94.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  97%|█████████▋| 1.04G/1.07G [00:11<00:00, 94.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  98%|█████████▊| 1.05G/1.07G [00:11<00:00, 95.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  99%|█████████▊| 1.06G/1.07G [00:11<00:00, 95.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin: 100%|█████████▉| 1.07G/1.07G [00:12<00:00, 95.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin: 100%|██████████| 1.07G/1.07G [00:12<00:00, 89.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  88%|████████▊ | 7/8 [01:47<00:12, 12.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  88%|████████▊ | 7/8 [01:47<00:12, 12.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  88%|████████▊ | 7/8 [01:47<00:12, 12.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  88%|████████▊ | 7/8 [01:47<00:12, 12.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  88%|████████▊ | 7/8 [01:47<00:12, 12.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  88%|████████▊ | 7/8 [01:47<00:12, 12.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  88%|████████▊ | 7/8 [01:47<00:12, 12.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  88%|████████▊ | 7/8 [01:47<00:12, 12.82s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:   0%|          | 0.00/1.07G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:   2%|▏         | 21.0M/1.07G [00:00<00:07, 147MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:   5%|▍         | 52.4M/1.07G [00:00<00:04, 221MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:   8%|▊         | 83.9M/1.07G [00:00<00:03, 252MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  11%|█         | 115M/1.07G [00:00<00:03, 268MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  14%|█▎        | 147M/1.07G [00:00<00:03, 277MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  17%|█▋        | 178M/1.07G [00:00<00:03, 284MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  20%|█▉        | 210M/1.07G [00:00<00:03, 285MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  23%|██▎       | 241M/1.07G [00:00<00:02, 288MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  25%|██▌       | 273M/1.07G [00:01<00:02, 285MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  28%|██▊       | 304M/1.07G [00:01<00:07, 109MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  31%|███▏      | 336M/1.07G [00:01<00:05, 135MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  34%|███▍      | 367M/1.07G [00:01<00:04, 152MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  37%|███▋      | 398M/1.07G [00:02<00:05, 124MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  39%|███▉      | 419M/1.07G [00:03<00:10, 64.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  42%|████▏     | 451M/1.07G [00:03<00:07, 86.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  44%|████▍     | 472M/1.07G [00:04<00:13, 44.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  47%|████▋     | 503M/1.07G [00:04<00:09, 62.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  50%|█████     | 535M/1.07G [00:04<00:06, 82.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  53%|█████▎    | 566M/1.07G [00:04<00:04, 106MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  56%|█████▌    | 598M/1.07G [00:04<00:03, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  59%|█████▉    | 629M/1.07G [00:05<00:02, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  62%|██████▏   | 661M/1.07G [00:05<00:02, 184MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  65%|██████▍   | 692M/1.07G [00:05<00:01, 208MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  68%|██████▊   | 724M/1.07G [00:05<00:01, 225MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  72%|███████▏  | 765M/1.07G [00:05<00:01, 251MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  75%|███████▍  | 797M/1.07G [00:05<00:01, 265MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  77%|███████▋  | 828M/1.07G [00:05<00:00, 264MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  80%|████████  | 860M/1.07G [00:05<00:00, 274MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  83%|████████▎ | 891M/1.07G [00:05<00:00, 279MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  86%|████████▋ | 923M/1.07G [00:06<00:00, 285MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  89%|████████▉ | 954M/1.07G [00:06<00:00, 289MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  92%|█████████▏| 986M/1.07G [00:06<00:00, 293MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  95%|█████████▌| 1.02G/1.07G [00:06<00:00, 297MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  98%|█████████▊| 1.05G/1.07G [00:06<00:00, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin: 100%|██████████| 1.07G/1.07G [00:06<00:00, 164MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [01:54<00:00, 10.86s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [01:54<00:00, 14.30s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [01:54<00:00, 10.86s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [01:54<00:00, 14.30s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [01:54<00:00, 10.87s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [01:54<00:00, 14.30s/it]\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:575] 2023-05-11 10:06:52,552 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"transformers_version\": \"4.28.0\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:575] 2023-05-11 10:06:52,552 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"transformers_version\": \"4.28.0\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [01:54<00:00, 10.87s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [01:54<00:00, 14.30s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [01:54<00:00, 10.87s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [01:54<00:00, 14.30s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [01:54<00:00, 10.87s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [01:54<00:00, 14.30s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [01:54<00:00, 10.87s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [01:54<00:00, 14.30s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [01:54<00:00, 10.87s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [01:54<00:00, 14.30s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:10,  1.56s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:10,  1.56s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:11,  1.59s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:11,  1.62s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:11,  1.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:11,  1.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:11,  1.64s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:11,  1.62s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 2/8 [00:02<00:07,  1.24s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 2/8 [00:03<00:11,  1.85s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 2/8 [00:03<00:11,  1.86s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 2/8 [00:03<00:11,  1.89s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 2/8 [00:03<00:11,  1.91s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 2/8 [00:03<00:11,  1.91s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 2/8 [00:03<00:11,  1.93s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 2/8 [00:03<00:11,  1.93s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  38%|███▊      | 3/8 [00:03<00:06,  1.26s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  38%|███▊      | 3/8 [00:05<00:09,  1.97s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 4/8 [00:05<00:06,  1.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  38%|███▊      | 3/8 [00:05<00:10,  2.02s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  38%|███▊      | 3/8 [00:05<00:10,  2.04s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  38%|███▊      | 3/8 [00:05<00:10,  2.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  38%|███▊      | 3/8 [00:05<00:10,  2.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  38%|███▊      | 3/8 [00:06<00:10,  2.08s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  38%|███▊      | 3/8 [00:06<00:10,  2.09s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:06<00:04,  1.38s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 4/8 [00:07<00:07,  1.92s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 4/8 [00:07<00:07,  1.92s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 4/8 [00:07<00:07,  1.95s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 4/8 [00:07<00:07,  1.97s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 4/8 [00:07<00:07,  1.98s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 4/8 [00:07<00:07,  2.00s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 4/8 [00:07<00:08,  2.02s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:08<00:02,  1.30s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:08<00:01,  1.10s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:09<00:00,  1.02it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:09<00:00,  1.19s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:09<00:05,  1.92s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:09<00:05,  1.93s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:09<00:05,  1.94s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:09<00:05,  1.96s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:09<00:05,  1.97s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:09<00:05,  1.99s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:09<00:05,  1.99s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:12<00:04,  2.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:12<00:04,  2.10s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:12<00:04,  2.11s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:12<00:04,  2.10s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:12<00:04,  2.15s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:12<00:04,  2.16s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:12<00:04,  2.16s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:13<00:01,  1.78s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:13<00:01,  1.78s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:13<00:01,  1.80s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:13<00:01,  1.83s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:13<00:01,  1.83s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:13<00:01,  1.83s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:13<00:01,  1.81s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:13<00:00,  1.44s/it]#015Loading checkpoint shards: 100%|██████████| 8/8 [00:13<00:00,  1.74s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:14<00:00,  1.45s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:14<00:00,  1.75s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:14<00:00,  1.46s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:14<00:00,  1.76s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:14<00:00,  1.48s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:14<00:00,  1.76s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:14<00:00,  1.48s/it][INFO|modeling_utils.py:3190] 2023-05-11 10:07:06,844 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3190] 2023-05-11 10:07:06,844 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3198] 2023-05-11 10:07:06,844 >> All the weights of ChatGLMForConditionalGeneration were initialized from the model checkpoint at THUDM/chatglm-6b.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use ChatGLMForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards: 100%|██████████| 8/8 [00:14<00:00,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3198] 2023-05-11 10:07:06,844 >> All the weights of ChatGLMForConditionalGeneration were initialized from the model checkpoint at THUDM/chatglm-6b.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use ChatGLMForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:14<00:00,  1.49s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:14<00:00,  1.76s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:14<00:00,  1.48s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:14<00:00,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2839] 2023-05-11 10:07:07,074 >> Generation config file not found, using a generation config created from the model config.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2839] 2023-05-11 10:07:07,074 >> Generation config file not found, using a generation config created from the model config.\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   0%|          | 0/115 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   1%|          | 1/115 [00:00<01:40,  1.13ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   2%|▏         | 2/115 [00:01<01:37,  1.16ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 3/115 [00:02<01:34,  1.18ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 4/115 [00:03<01:33,  1.19ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   4%|▍         | 5/115 [00:04<01:31,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   5%|▌         | 6/115 [00:05<01:31,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   6%|▌         | 7/115 [00:05<01:30,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   7%|▋         | 8/115 [00:06<01:28,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   8%|▊         | 9/115 [00:07<01:28,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   9%|▊         | 10/115 [00:08<01:26,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|▉         | 11/115 [00:09<01:26,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|█         | 12/115 [00:10<01:25,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  11%|█▏        | 13/115 [00:10<01:24,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  12%|█▏        | 14/115 [00:11<01:23,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  13%|█▎        | 15/115 [00:12<01:23,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  14%|█▍        | 16/115 [00:13<01:22,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  15%|█▍        | 17/115 [00:14<01:21,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  16%|█▌        | 18/115 [00:15<01:20,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 19/115 [00:15<01:19,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 20/115 [00:16<01:18,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  18%|█▊        | 21/115 [00:17<01:18,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  19%|█▉        | 22/115 [00:18<01:17,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  20%|██        | 23/115 [00:19<01:16,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  21%|██        | 24/115 [00:19<01:15,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  22%|██▏       | 25/115 [00:20<01:14,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 26/115 [00:21<01:13,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 27/115 [00:22<01:13,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  24%|██▍       | 28/115 [00:23<01:12,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  25%|██▌       | 29/115 [00:24<01:11,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  26%|██▌       | 30/115 [00:25<01:10,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  27%|██▋       | 31/115 [00:25<01:10,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  28%|██▊       | 32/115 [00:26<01:09,  1.19ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  29%|██▊       | 33/115 [00:27<01:08,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|██▉       | 34/115 [00:28<01:07,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|███       | 35/115 [00:29<01:06,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  31%|███▏      | 36/115 [00:30<01:05,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  32%|███▏      | 37/115 [00:30<01:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  33%|███▎      | 38/115 [00:31<01:04,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  34%|███▍      | 39/115 [00:32<01:03,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  35%|███▍      | 40/115 [00:33<01:02,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  36%|███▌      | 41/115 [00:34<01:01,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 42/115 [00:35<01:00,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 43/115 [00:35<00:59,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  38%|███▊      | 44/115 [00:36<00:59,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  39%|███▉      | 45/115 [00:37<00:58,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  40%|████      | 46/115 [00:38<00:57,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  41%|████      | 47/115 [00:39<00:56,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  42%|████▏     | 48/115 [00:39<00:55,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 49/115 [00:40<00:55,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 50/115 [00:41<00:54,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  44%|████▍     | 51/115 [00:42<00:53,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  45%|████▌     | 52/115 [00:43<00:52,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  46%|████▌     | 53/115 [00:44<00:51,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  47%|████▋     | 54/115 [00:44<00:50,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  48%|████▊     | 55/115 [00:45<00:49,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  49%|████▊     | 56/115 [00:46<00:49,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|████▉     | 57/115 [00:47<00:48,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|█████     | 58/115 [00:48<00:47,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  51%|█████▏    | 59/115 [00:49<00:46,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  52%|█████▏    | 60/115 [00:49<00:45,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  53%|█████▎    | 61/115 [00:50<00:44,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  54%|█████▍    | 62/115 [00:51<00:44,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  55%|█████▍    | 63/115 [00:52<00:43,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  56%|█████▌    | 64/115 [00:53<00:42,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 65/115 [00:54<00:41,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 66/115 [00:54<00:40,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  58%|█████▊    | 67/115 [00:55<00:39,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  59%|█████▉    | 68/115 [00:56<00:39,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  60%|██████    | 69/115 [00:57<00:38,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  61%|██████    | 70/115 [00:58<00:37,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  62%|██████▏   | 71/115 [00:59<00:36,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 72/115 [00:59<00:35,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 73/115 [01:00<00:34,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  64%|██████▍   | 74/115 [01:01<00:34,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  65%|██████▌   | 75/115 [01:02<00:33,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  66%|██████▌   | 76/115 [01:03<00:32,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  67%|██████▋   | 77/115 [01:04<00:31,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  68%|██████▊   | 78/115 [01:04<00:30,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  69%|██████▊   | 79/115 [01:05<00:29,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|██████▉   | 80/115 [01:06<00:29,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|███████   | 81/115 [01:07<00:28,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  71%|███████▏  | 82/115 [01:08<00:27,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  72%|███████▏  | 83/115 [01:09<00:26,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  73%|███████▎  | 84/115 [01:09<00:25,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  74%|███████▍  | 85/115 [01:10<00:25,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  75%|███████▍  | 86/115 [01:11<00:24,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  76%|███████▌  | 87/115 [01:12<00:23,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 88/115 [01:13<00:22,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 89/115 [01:14<00:21,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  78%|███████▊  | 90/115 [01:14<00:20,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  79%|███████▉  | 91/115 [01:15<00:20,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  80%|████████  | 92/115 [01:16<00:19,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  81%|████████  | 93/115 [01:17<00:18,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  82%|████████▏ | 94/115 [01:18<00:17,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 95/115 [01:19<00:16,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 96/115 [01:19<00:15,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  84%|████████▍ | 97/115 [01:20<00:14,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  85%|████████▌ | 98/115 [01:21<00:14,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  86%|████████▌ | 99/115 [01:22<00:13,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  87%|████████▋ | 100/115 [01:23<00:12,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  88%|████████▊ | 101/115 [01:24<00:11,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  89%|████████▊ | 102/115 [01:24<00:10,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|████████▉ | 103/115 [01:25<00:10,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|█████████ | 104/115 [01:26<00:09,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  91%|█████████▏| 105/115 [01:27<00:08,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  92%|█████████▏| 106/115 [01:28<00:07,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  93%|█████████▎| 107/115 [01:29<00:06,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  94%|█████████▍| 108/115 [01:29<00:05,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  95%|█████████▍| 109/115 [01:30<00:04,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  96%|█████████▌| 110/115 [01:31<00:04,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 111/115 [01:32<00:03,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 112/115 [01:33<00:02,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  98%|█████████▊| 113/115 [01:34<00:01,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  99%|█████████▉| 114/115 [01:34<00:00,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:35<00:00,  1.37ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:35<00:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34minput_ids\u001b[0m\n",
      "\u001b[34m[5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\u001b[0m\n",
      "\u001b[34minputs 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[34mlabel_ids\u001b[0m\n",
      "\u001b[34m[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[34mlabels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:08:50,925] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed info: version=0.8.3, git-hash=unknown, git-branch=unknown\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   0%|          | 0/115 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   0%|          | 0/115 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   0%|          | 0/115 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   0%|          | 0/115 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   0%|          | 0/115 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   0%|          | 0/115 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   0%|          | 0/115 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   1%|          | 1/115 [00:00<01:35,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   1%|          | 1/115 [00:00<01:35,  1.19ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   1%|          | 1/115 [00:00<01:35,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   1%|          | 1/115 [00:00<01:36,  1.18ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   1%|          | 1/115 [00:00<01:36,  1.18ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   1%|          | 1/115 [00:00<01:40,  1.14ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   1%|          | 1/115 [00:00<01:49,  1.04ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   2%|▏         | 2/115 [00:01<01:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   2%|▏         | 2/115 [00:01<01:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   2%|▏         | 2/115 [00:01<01:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   2%|▏         | 2/115 [00:01<01:34,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   2%|▏         | 2/115 [00:01<01:34,  1.19ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   2%|▏         | 2/115 [00:01<01:36,  1.17ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   2%|▏         | 2/115 [00:01<01:46,  1.06ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 3/115 [00:02<01:31,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 3/115 [00:02<01:32,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 3/115 [00:02<01:33,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 3/115 [00:02<01:33,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 3/115 [00:02<01:33,  1.19ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 3/115 [00:02<01:34,  1.18ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 3/115 [00:02<01:40,  1.12ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 4/115 [00:03<01:30,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 4/115 [00:03<01:31,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 4/115 [00:03<01:31,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 4/115 [00:03<01:32,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 4/115 [00:03<01:32,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 4/115 [00:03<01:33,  1.19ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 4/115 [00:03<01:35,  1.16ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   4%|▍         | 5/115 [00:04<01:29,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   4%|▍         | 5/115 [00:04<01:30,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   4%|▍         | 5/115 [00:04<01:30,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   4%|▍         | 5/115 [00:04<01:31,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   4%|▍         | 5/115 [00:04<01:31,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   4%|▍         | 5/115 [00:04<01:32,  1.19ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   4%|▍         | 5/115 [00:04<01:33,  1.18ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   5%|▌         | 6/115 [00:04<01:28,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   5%|▌         | 6/115 [00:04<01:30,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   5%|▌         | 6/115 [00:04<01:29,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   5%|▌         | 6/115 [00:05<01:30,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   5%|▌         | 6/115 [00:05<01:30,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   5%|▌         | 6/115 [00:05<01:31,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   5%|▌         | 6/115 [00:05<01:31,  1.19ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   6%|▌         | 7/115 [00:05<01:27,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   6%|▌         | 7/115 [00:05<01:29,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   6%|▌         | 7/115 [00:05<01:29,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   6%|▌         | 7/115 [00:05<01:29,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   6%|▌         | 7/115 [00:05<01:29,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   6%|▌         | 7/115 [00:05<01:29,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   6%|▌         | 7/115 [00:06<01:29,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   7%|▋         | 8/115 [00:06<01:26,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   7%|▋         | 8/115 [00:06<01:28,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   7%|▋         | 8/115 [00:06<01:27,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   7%|▋         | 8/115 [00:06<01:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   7%|▋         | 8/115 [00:06<01:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   7%|▋         | 8/115 [00:06<01:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   7%|▋         | 8/115 [00:06<01:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   8%|▊         | 9/115 [00:07<01:25,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   8%|▊         | 9/115 [00:07<01:27,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   8%|▊         | 9/115 [00:07<01:26,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   8%|▊         | 9/115 [00:07<01:27,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   8%|▊         | 9/115 [00:07<01:27,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   8%|▊         | 9/115 [00:07<01:27,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   8%|▊         | 9/115 [00:07<01:27,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   9%|▊         | 10/115 [00:08<01:24,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   9%|▊         | 10/115 [00:08<01:25,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   9%|▊         | 10/115 [00:08<01:26,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   9%|▊         | 10/115 [00:08<01:26,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   9%|▊         | 10/115 [00:08<01:26,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   9%|▊         | 10/115 [00:08<01:26,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   9%|▊         | 10/115 [00:08<01:26,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|▉         | 11/115 [00:08<01:24,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|▉         | 11/115 [00:09<01:25,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|▉         | 11/115 [00:09<01:25,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|▉         | 11/115 [00:09<01:26,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|▉         | 11/115 [00:09<01:26,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|▉         | 11/115 [00:09<01:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|▉         | 11/115 [00:09<01:25,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|█         | 12/115 [00:09<01:23,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|█         | 12/115 [00:09<01:24,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|█         | 12/115 [00:09<01:24,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|█         | 12/115 [00:09<01:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|█         | 12/115 [00:09<01:24,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|█         | 12/115 [00:09<01:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|█         | 12/115 [00:10<01:24,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  11%|█▏        | 13/115 [00:10<01:22,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  11%|█▏        | 13/115 [00:10<01:23,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  11%|█▏        | 13/115 [00:10<01:23,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  11%|█▏        | 13/115 [00:10<01:24,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  11%|█▏        | 13/115 [00:10<01:24,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  11%|█▏        | 13/115 [00:10<01:24,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  11%|█▏        | 13/115 [00:10<01:23,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  12%|█▏        | 14/115 [00:11<01:22,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  12%|█▏        | 14/115 [00:11<01:22,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  12%|█▏        | 14/115 [00:11<01:23,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  12%|█▏        | 14/115 [00:11<01:23,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  12%|█▏        | 14/115 [00:11<01:23,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  12%|█▏        | 14/115 [00:11<01:24,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  12%|█▏        | 14/115 [00:11<01:22,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  13%|█▎        | 15/115 [00:12<01:21,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  13%|█▎        | 15/115 [00:12<01:21,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  13%|█▎        | 15/115 [00:12<01:22,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  13%|█▎        | 15/115 [00:12<01:22,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  13%|█▎        | 15/115 [00:12<01:22,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  13%|█▎        | 15/115 [00:12<01:23,  1.19ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  13%|█▎        | 15/115 [00:12<01:21,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  14%|█▍        | 16/115 [00:12<01:19,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  14%|█▍        | 16/115 [00:13<01:20,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  14%|█▍        | 16/115 [00:13<01:20,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  14%|█▍        | 16/115 [00:13<01:21,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  14%|█▍        | 16/115 [00:13<01:21,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  14%|█▍        | 16/115 [00:13<01:22,  1.19ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  14%|█▍        | 16/115 [00:13<01:20,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  15%|█▍        | 17/115 [00:13<01:18,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  15%|█▍        | 17/115 [00:13<01:19,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  15%|█▍        | 17/115 [00:13<01:19,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  15%|█▍        | 17/115 [00:14<01:20,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  15%|█▍        | 17/115 [00:14<01:20,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  15%|█▍        | 17/115 [00:14<01:22,  1.19ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  15%|█▍        | 17/115 [00:14<01:19,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  16%|█▌        | 18/115 [00:14<01:18,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  16%|█▌        | 18/115 [00:14<01:19,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  16%|█▌        | 18/115 [00:14<01:19,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  16%|█▌        | 18/115 [00:14<01:19,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  16%|█▌        | 18/115 [00:14<01:19,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  16%|█▌        | 18/115 [00:14<01:18,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  16%|█▌        | 18/115 [00:14<01:20,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 19/115 [00:15<01:17,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 19/115 [00:15<01:18,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 19/115 [00:15<01:18,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 19/115 [00:15<01:19,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 19/115 [00:15<01:19,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 19/115 [00:15<01:18,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 19/115 [00:15<01:20,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 20/115 [00:16<01:16,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 20/115 [00:16<01:17,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 20/115 [00:16<01:17,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 20/115 [00:16<01:18,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 20/115 [00:16<01:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 20/115 [00:16<01:17,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 20/115 [00:16<01:19,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  18%|█▊        | 21/115 [00:17<01:15,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  18%|█▊        | 21/115 [00:17<01:16,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  18%|█▊        | 21/115 [00:17<01:16,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  18%|█▊        | 21/115 [00:17<01:17,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  18%|█▊        | 21/115 [00:17<01:17,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  18%|█▊        | 21/115 [00:17<01:16,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  18%|█▊        | 21/115 [00:17<01:18,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  19%|█▉        | 22/115 [00:17<01:15,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  19%|█▉        | 22/115 [00:18<01:16,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  19%|█▉        | 22/115 [00:18<01:16,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  19%|█▉        | 22/115 [00:18<01:16,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  19%|█▉        | 22/115 [00:18<01:16,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  19%|█▉        | 22/115 [00:18<01:16,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  19%|█▉        | 22/115 [00:18<01:17,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  20%|██        | 23/115 [00:18<01:14,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  20%|██        | 23/115 [00:18<01:15,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  20%|██        | 23/115 [00:18<01:15,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  20%|██        | 23/115 [00:19<01:15,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  20%|██        | 23/115 [00:19<01:15,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  20%|██        | 23/115 [00:19<01:15,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  20%|██        | 23/115 [00:19<01:16,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  21%|██        | 24/115 [00:19<01:13,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  21%|██        | 24/115 [00:19<01:14,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  21%|██        | 24/115 [00:19<01:14,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  21%|██        | 24/115 [00:19<01:15,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  21%|██        | 24/115 [00:19<01:15,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  21%|██        | 24/115 [00:19<01:14,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  21%|██        | 24/115 [00:19<01:15,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  22%|██▏       | 25/115 [00:20<01:13,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  22%|██▏       | 25/115 [00:20<01:13,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  22%|██▏       | 25/115 [00:20<01:13,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  22%|██▏       | 25/115 [00:20<01:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  22%|██▏       | 25/115 [00:20<01:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  22%|██▏       | 25/115 [00:20<01:13,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  22%|██▏       | 25/115 [00:20<01:14,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 26/115 [00:21<01:12,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 26/115 [00:21<01:12,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 26/115 [00:21<01:12,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 26/115 [00:21<01:13,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 26/115 [00:21<01:13,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 26/115 [00:21<01:12,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 26/115 [00:21<01:13,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 27/115 [00:21<01:11,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 27/115 [00:22<01:12,  1.22ba/s]#015Running tokenizer on train dataset:  23%|██▎       | 27/115 [00:22<01:12,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 27/115 [00:22<01:12,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 27/115 [00:22<01:12,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 27/115 [00:22<01:12,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 27/115 [00:22<01:13,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  24%|██▍       | 28/115 [00:22<01:10,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  24%|██▍       | 28/115 [00:22<01:11,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  24%|██▍       | 28/115 [00:22<01:11,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  24%|██▍       | 28/115 [00:23<01:11,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  24%|██▍       | 28/115 [00:23<01:11,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  24%|██▍       | 28/115 [00:23<01:11,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  24%|██▍       | 28/115 [00:23<01:12,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  25%|██▌       | 29/115 [00:23<01:09,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  25%|██▌       | 29/115 [00:23<01:10,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  25%|██▌       | 29/115 [00:23<01:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  25%|██▌       | 29/115 [00:24<01:11,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  25%|██▌       | 29/115 [00:24<01:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  25%|██▌       | 29/115 [00:24<01:11,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  25%|██▌       | 29/115 [00:24<01:11,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  26%|██▌       | 30/115 [00:24<01:09,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  26%|██▌       | 30/115 [00:24<01:09,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  26%|██▌       | 30/115 [00:24<01:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  26%|██▌       | 30/115 [00:24<01:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  26%|██▌       | 30/115 [00:24<01:09,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  26%|██▌       | 30/115 [00:24<01:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  26%|██▌       | 30/115 [00:24<01:10,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  27%|██▋       | 31/115 [00:25<01:08,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  27%|██▋       | 31/115 [00:25<01:09,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  27%|██▋       | 31/115 [00:25<01:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  27%|██▋       | 31/115 [00:25<01:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  27%|██▋       | 31/115 [00:25<01:09,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  27%|██▋       | 31/115 [00:25<01:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  27%|██▋       | 31/115 [00:25<01:09,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  28%|██▊       | 32/115 [00:25<01:07,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  28%|██▊       | 32/115 [00:26<01:08,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  28%|██▊       | 32/115 [00:26<01:08,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  28%|██▊       | 32/115 [00:26<01:08,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  28%|██▊       | 32/115 [00:26<01:08,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  28%|██▊       | 32/115 [00:26<01:08,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  28%|██▊       | 32/115 [00:26<01:09,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  29%|██▊       | 33/115 [00:26<01:06,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  29%|██▊       | 33/115 [00:27<01:07,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  29%|██▊       | 33/115 [00:27<01:07,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  29%|██▊       | 33/115 [00:27<01:07,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  29%|██▊       | 33/115 [00:27<01:07,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  29%|██▊       | 33/115 [00:27<01:07,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  29%|██▊       | 33/115 [00:27<01:08,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|██▉       | 34/115 [00:27<01:05,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|██▉       | 34/115 [00:27<01:06,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|██▉       | 34/115 [00:27<01:06,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|██▉       | 34/115 [00:28<01:06,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|██▉       | 34/115 [00:28<01:06,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|██▉       | 34/115 [00:28<01:07,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|██▉       | 34/115 [00:28<01:07,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|███       | 35/115 [00:28<01:05,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|███       | 35/115 [00:28<01:05,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|███       | 35/115 [00:28<01:05,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|███       | 35/115 [00:28<01:05,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|███       | 35/115 [00:28<01:05,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|███       | 35/115 [00:28<01:05,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|███       | 35/115 [00:29<01:06,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  31%|███▏      | 36/115 [00:29<01:04,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  31%|███▏      | 36/115 [00:29<01:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  31%|███▏      | 36/115 [00:29<01:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  31%|███▏      | 36/115 [00:29<01:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  31%|███▏      | 36/115 [00:29<01:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  31%|███▏      | 36/115 [00:29<01:05,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  31%|███▏      | 36/115 [00:29<01:05,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  32%|███▏      | 37/115 [00:30<01:03,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  32%|███▏      | 37/115 [00:30<01:03,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  32%|███▏      | 37/115 [00:30<01:03,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  32%|███▏      | 37/115 [00:30<01:03,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  32%|███▏      | 37/115 [00:30<01:03,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  32%|███▏      | 37/115 [00:30<01:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  32%|███▏      | 37/115 [00:30<01:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  33%|███▎      | 38/115 [00:30<01:02,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  33%|███▎      | 38/115 [00:31<01:03,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  33%|███▎      | 38/115 [00:31<01:03,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  33%|███▎      | 38/115 [00:31<01:03,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  33%|███▎      | 38/115 [00:31<01:03,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  33%|███▎      | 38/115 [00:31<01:03,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  33%|███▎      | 38/115 [00:31<01:03,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  34%|███▍      | 39/115 [00:31<01:01,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  34%|███▍      | 39/115 [00:32<01:02,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  34%|███▍      | 39/115 [00:32<01:02,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  34%|███▍      | 39/115 [00:32<01:02,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  34%|███▍      | 39/115 [00:32<01:02,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  34%|███▍      | 39/115 [00:32<01:02,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  34%|███▍      | 39/115 [00:32<01:02,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  35%|███▍      | 40/115 [00:32<01:00,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  35%|███▍      | 40/115 [00:32<01:01,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  35%|███▍      | 40/115 [00:32<01:01,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  35%|███▍      | 40/115 [00:33<01:01,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  35%|███▍      | 40/115 [00:33<01:01,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  35%|███▍      | 40/115 [00:33<01:01,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  35%|███▍      | 40/115 [00:33<01:02,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  36%|███▌      | 41/115 [00:33<01:00,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  36%|███▌      | 41/115 [00:33<01:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  36%|███▌      | 41/115 [00:33<01:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  36%|███▌      | 41/115 [00:33<01:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  36%|███▌      | 41/115 [00:33<01:01,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  36%|███▌      | 41/115 [00:33<01:01,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  36%|███▌      | 41/115 [00:34<01:01,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 42/115 [00:34<00:59,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 42/115 [00:34<00:59,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 42/115 [00:34<00:59,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 42/115 [00:34<01:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 42/115 [00:34<01:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 42/115 [00:34<01:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 43/115 [00:34<00:58,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 42/115 [00:34<01:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 43/115 [00:35<00:59,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 43/115 [00:35<00:59,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 43/115 [00:35<00:59,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 43/115 [00:35<00:59,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 43/115 [00:35<00:59,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  38%|███▊      | 44/115 [00:35<00:57,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 43/115 [00:35<00:59,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  38%|███▊      | 44/115 [00:36<00:58,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  38%|███▊      | 44/115 [00:36<00:58,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  38%|███▊      | 44/115 [00:36<00:58,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  38%|███▊      | 44/115 [00:36<00:58,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  38%|███▊      | 44/115 [00:36<00:58,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  39%|███▉      | 45/115 [00:36<00:56,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  38%|███▊      | 44/115 [00:36<00:58,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  39%|███▉      | 45/115 [00:36<00:57,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  39%|███▉      | 45/115 [00:36<00:57,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  39%|███▉      | 45/115 [00:37<00:57,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  39%|███▉      | 45/115 [00:37<00:57,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  39%|███▉      | 45/115 [00:37<00:57,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  40%|████      | 46/115 [00:37<00:56,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  39%|███▉      | 45/115 [00:37<00:57,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  40%|████      | 46/115 [00:37<00:56,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  40%|████      | 46/115 [00:37<00:56,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  40%|████      | 46/115 [00:37<00:56,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  40%|████      | 46/115 [00:37<00:56,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  40%|████      | 46/115 [00:38<00:56,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  41%|████      | 47/115 [00:38<00:55,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  40%|████      | 46/115 [00:38<00:57,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  41%|████      | 47/115 [00:38<00:55,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  41%|████      | 47/115 [00:38<00:55,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  41%|████      | 47/115 [00:38<00:55,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  41%|████      | 47/115 [00:38<00:56,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  41%|████      | 47/115 [00:38<00:56,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  42%|████▏     | 48/115 [00:38<00:54,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  41%|████      | 47/115 [00:39<00:56,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  42%|████▏     | 48/115 [00:39<00:54,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  42%|████▏     | 48/115 [00:39<00:54,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  42%|████▏     | 48/115 [00:39<00:55,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  42%|████▏     | 48/115 [00:39<00:54,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  42%|████▏     | 48/115 [00:39<00:55,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 49/115 [00:39<00:53,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  42%|████▏     | 48/115 [00:39<00:55,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 49/115 [00:40<00:54,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 49/115 [00:40<00:54,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 49/115 [00:40<00:54,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 49/115 [00:40<00:54,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 49/115 [00:40<00:54,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 50/115 [00:40<00:52,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 49/115 [00:40<00:54,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 50/115 [00:41<00:53,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 50/115 [00:41<00:53,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 50/115 [00:41<00:53,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 50/115 [00:41<00:53,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 50/115 [00:41<00:53,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  44%|████▍     | 51/115 [00:41<00:51,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 50/115 [00:41<00:53,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  44%|████▍     | 51/115 [00:41<00:52,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  44%|████▍     | 51/115 [00:41<00:52,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  44%|████▍     | 51/115 [00:42<00:52,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  44%|████▍     | 51/115 [00:42<00:52,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  44%|████▍     | 51/115 [00:42<00:52,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  45%|████▌     | 52/115 [00:42<00:51,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  44%|████▍     | 51/115 [00:42<00:53,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  45%|████▌     | 52/115 [00:42<00:51,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  45%|████▌     | 52/115 [00:42<00:51,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  45%|████▌     | 52/115 [00:42<00:51,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  45%|████▌     | 52/115 [00:42<00:51,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  45%|████▌     | 52/115 [00:42<00:52,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  46%|████▌     | 53/115 [00:43<00:50,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  45%|████▌     | 52/115 [00:43<00:52,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  46%|████▌     | 53/115 [00:43<00:50,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  46%|████▌     | 53/115 [00:43<00:50,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  46%|████▌     | 53/115 [00:43<00:50,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  46%|████▌     | 53/115 [00:43<00:51,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  46%|████▌     | 53/115 [00:43<00:51,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  47%|████▋     | 54/115 [00:43<00:49,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  46%|████▌     | 53/115 [00:44<00:51,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  47%|████▋     | 54/115 [00:44<00:49,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  47%|████▋     | 54/115 [00:44<00:50,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  47%|████▋     | 54/115 [00:44<00:50,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  47%|████▋     | 54/115 [00:44<00:50,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  48%|████▊     | 55/115 [00:44<00:48,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  47%|████▋     | 54/115 [00:44<00:50,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  47%|████▋     | 54/115 [00:44<00:50,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  48%|████▊     | 55/115 [00:45<00:49,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  48%|████▊     | 55/115 [00:45<00:49,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  48%|████▊     | 55/115 [00:45<00:49,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  48%|████▊     | 55/115 [00:45<00:49,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  49%|████▊     | 56/115 [00:45<00:47,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  48%|████▊     | 55/115 [00:45<00:49,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  48%|████▊     | 55/115 [00:45<00:49,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  49%|████▊     | 56/115 [00:45<00:48,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  49%|████▊     | 56/115 [00:45<00:48,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  49%|████▊     | 56/115 [00:46<00:48,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  49%|████▊     | 56/115 [00:46<00:48,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|████▉     | 57/115 [00:46<00:47,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  49%|████▊     | 56/115 [00:46<00:48,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  49%|████▊     | 56/115 [00:46<00:48,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|████▉     | 57/115 [00:46<00:47,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|████▉     | 57/115 [00:46<00:47,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|████▉     | 57/115 [00:47<00:47,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|████▉     | 57/115 [00:47<00:47,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|█████     | 58/115 [00:47<00:46,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|████▉     | 57/115 [00:47<00:47,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|████▉     | 57/115 [00:47<00:48,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|█████     | 58/115 [00:47<00:46,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|█████     | 58/115 [00:47<00:46,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|█████     | 58/115 [00:47<00:46,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|█████     | 58/115 [00:47<00:47,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  51%|█████▏    | 59/115 [00:47<00:45,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|█████     | 58/115 [00:47<00:47,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|█████     | 58/115 [00:48<00:47,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  51%|█████▏    | 59/115 [00:48<00:45,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  51%|█████▏    | 59/115 [00:48<00:46,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  51%|█████▏    | 59/115 [00:48<00:46,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  51%|█████▏    | 59/115 [00:48<00:46,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  52%|█████▏    | 60/115 [00:48<00:44,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  51%|█████▏    | 59/115 [00:48<00:46,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  51%|█████▏    | 59/115 [00:48<00:46,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  52%|█████▏    | 60/115 [00:49<00:45,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  52%|█████▏    | 60/115 [00:49<00:45,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  52%|█████▏    | 60/115 [00:49<00:45,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  53%|█████▎    | 61/115 [00:49<00:43,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  52%|█████▏    | 60/115 [00:49<00:45,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  52%|█████▏    | 60/115 [00:49<00:45,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  52%|█████▏    | 60/115 [00:49<00:45,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  53%|█████▎    | 61/115 [00:50<00:44,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  53%|█████▎    | 61/115 [00:50<00:44,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  54%|█████▍    | 62/115 [00:50<00:42,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  53%|█████▎    | 61/115 [00:50<00:44,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  53%|█████▎    | 61/115 [00:50<00:44,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  53%|█████▎    | 61/115 [00:50<00:44,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  53%|█████▎    | 61/115 [00:50<00:44,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  54%|█████▍    | 62/115 [00:50<00:43,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  54%|█████▍    | 62/115 [00:50<00:43,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  55%|█████▍    | 63/115 [00:51<00:42,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  54%|█████▍    | 62/115 [00:51<00:43,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  54%|█████▍    | 62/115 [00:51<00:43,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  54%|█████▍    | 62/115 [00:51<00:43,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  54%|█████▍    | 62/115 [00:51<00:43,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  55%|█████▍    | 63/115 [00:51<00:42,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  55%|█████▍    | 63/115 [00:51<00:42,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  56%|█████▌    | 64/115 [00:51<00:41,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  55%|█████▍    | 63/115 [00:51<00:42,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  55%|█████▍    | 63/115 [00:51<00:42,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  55%|█████▍    | 63/115 [00:52<00:43,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  55%|█████▍    | 63/115 [00:52<00:42,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  56%|█████▌    | 64/115 [00:52<00:41,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  56%|█████▌    | 64/115 [00:52<00:41,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 65/115 [00:52<00:40,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  56%|█████▌    | 64/115 [00:52<00:41,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  56%|█████▌    | 64/115 [00:52<00:41,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  56%|█████▌    | 64/115 [00:52<00:42,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  56%|█████▌    | 64/115 [00:53<00:42,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 65/115 [00:53<00:40,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 65/115 [00:53<00:40,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 66/115 [00:53<00:39,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 65/115 [00:53<00:40,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 65/115 [00:53<00:41,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 65/115 [00:53<00:41,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 65/115 [00:53<00:41,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 66/115 [00:54<00:40,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 66/115 [00:54<00:39,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  58%|█████▊    | 67/115 [00:54<00:38,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 66/115 [00:54<00:40,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 66/115 [00:54<00:40,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 66/115 [00:54<00:40,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 66/115 [00:54<00:40,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  58%|█████▊    | 67/115 [00:54<00:39,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  58%|█████▊    | 67/115 [00:54<00:39,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  59%|█████▉    | 68/115 [00:55<00:38,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  58%|█████▊    | 67/115 [00:55<00:39,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  58%|█████▊    | 67/115 [00:55<00:39,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  58%|█████▊    | 67/115 [00:55<00:39,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  58%|█████▊    | 67/115 [00:55<00:39,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  59%|█████▉    | 68/115 [00:55<00:38,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  59%|█████▉    | 68/115 [00:55<00:38,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  60%|██████    | 69/115 [00:55<00:37,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  59%|█████▉    | 68/115 [00:56<00:38,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  59%|█████▉    | 68/115 [00:56<00:38,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  59%|█████▉    | 68/115 [00:56<00:38,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  59%|█████▉    | 68/115 [00:56<00:38,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  60%|██████    | 69/115 [00:56<00:37,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  60%|██████    | 69/115 [00:56<00:37,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  61%|██████    | 70/115 [00:56<00:36,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  60%|██████    | 69/115 [00:56<00:37,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  60%|██████    | 69/115 [00:56<00:37,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  60%|██████    | 69/115 [00:57<00:37,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  60%|██████    | 69/115 [00:57<00:38,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  61%|██████    | 70/115 [00:57<00:36,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  61%|██████    | 70/115 [00:57<00:36,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  62%|██████▏   | 71/115 [00:57<00:35,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  61%|██████    | 70/115 [00:57<00:36,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  61%|██████    | 70/115 [00:57<00:36,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  61%|██████    | 70/115 [00:57<00:37,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  61%|██████    | 70/115 [00:58<00:37,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  62%|██████▏   | 71/115 [00:58<00:36,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  62%|██████▏   | 71/115 [00:58<00:36,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 72/115 [00:58<00:34,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  62%|██████▏   | 71/115 [00:58<00:36,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  62%|██████▏   | 71/115 [00:58<00:36,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  62%|██████▏   | 71/115 [00:58<00:36,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  62%|██████▏   | 71/115 [00:58<00:36,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 72/115 [00:59<00:35,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 72/115 [00:59<00:35,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 73/115 [00:59<00:34,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 72/115 [00:59<00:35,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 72/115 [00:59<00:35,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 72/115 [00:59<00:35,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 72/115 [00:59<00:35,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 73/115 [00:59<00:34,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 73/115 [00:59<00:34,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  64%|██████▍   | 74/115 [01:00<00:33,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 73/115 [01:00<00:34,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 73/115 [01:00<00:34,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 73/115 [01:00<00:34,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 73/115 [01:00<00:34,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  64%|██████▍   | 74/115 [01:00<00:33,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  64%|██████▍   | 74/115 [01:00<00:33,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  65%|██████▌   | 75/115 [01:00<00:32,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  64%|██████▍   | 74/115 [01:01<00:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  64%|██████▍   | 74/115 [01:01<00:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  64%|██████▍   | 74/115 [01:01<00:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  64%|██████▍   | 74/115 [01:01<00:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  65%|██████▌   | 75/115 [01:01<00:32,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  65%|██████▌   | 75/115 [01:01<00:32,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  66%|██████▌   | 76/115 [01:01<00:31,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  65%|██████▌   | 75/115 [01:01<00:32,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  65%|██████▌   | 75/115 [01:01<00:32,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  65%|██████▌   | 75/115 [01:01<00:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  65%|██████▌   | 75/115 [01:02<00:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  66%|██████▌   | 76/115 [01:02<00:31,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  66%|██████▌   | 76/115 [01:02<00:31,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  67%|██████▋   | 77/115 [01:02<00:30,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  66%|██████▌   | 76/115 [01:02<00:32,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  66%|██████▌   | 76/115 [01:02<00:32,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  66%|██████▌   | 76/115 [01:02<00:32,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  66%|██████▌   | 76/115 [01:03<00:32,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  67%|██████▋   | 77/115 [01:03<00:31,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  67%|██████▋   | 77/115 [01:03<00:31,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  68%|██████▊   | 78/115 [01:03<00:30,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  67%|██████▋   | 77/115 [01:03<00:31,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  67%|██████▋   | 77/115 [01:03<00:31,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  67%|██████▋   | 77/115 [01:03<00:31,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  67%|██████▋   | 77/115 [01:03<00:31,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  68%|██████▊   | 78/115 [01:04<00:30,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  68%|██████▊   | 78/115 [01:04<00:30,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  69%|██████▊   | 79/115 [01:04<00:29,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  68%|██████▊   | 78/115 [01:04<00:30,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  68%|██████▊   | 78/115 [01:04<00:30,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  68%|██████▊   | 78/115 [01:04<00:30,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  68%|██████▊   | 78/115 [01:04<00:30,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  69%|██████▊   | 79/115 [01:04<00:29,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  69%|██████▊   | 79/115 [01:04<00:29,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|██████▉   | 80/115 [01:04<00:28,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  69%|██████▊   | 79/115 [01:05<00:29,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  69%|██████▊   | 79/115 [01:05<00:29,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  69%|██████▊   | 79/115 [01:05<00:29,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  69%|██████▊   | 79/115 [01:05<00:29,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|██████▉   | 80/115 [01:05<00:28,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|██████▉   | 80/115 [01:05<00:28,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|███████   | 81/115 [01:05<00:27,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|██████▉   | 80/115 [01:05<00:28,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|██████▉   | 80/115 [01:05<00:28,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|██████▉   | 80/115 [01:06<00:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|██████▉   | 80/115 [01:06<00:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|███████   | 81/115 [01:06<00:27,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|███████   | 81/115 [01:06<00:27,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  71%|███████▏  | 82/115 [01:06<00:26,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|███████   | 81/115 [01:06<00:27,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|███████   | 81/115 [01:06<00:27,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|███████   | 81/115 [01:06<00:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|███████   | 81/115 [01:07<00:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  71%|███████▏  | 82/115 [01:07<00:27,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  71%|███████▏  | 82/115 [01:07<00:27,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  72%|███████▏  | 83/115 [01:07<00:25,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  71%|███████▏  | 82/115 [01:07<00:27,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  71%|███████▏  | 82/115 [01:07<00:27,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  71%|███████▏  | 82/115 [01:07<00:27,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  71%|███████▏  | 82/115 [01:08<00:27,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  72%|███████▏  | 83/115 [01:08<00:26,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  72%|███████▏  | 83/115 [01:08<00:26,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  73%|███████▎  | 84/115 [01:08<00:25,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  72%|███████▏  | 83/115 [01:08<00:26,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  72%|███████▏  | 83/115 [01:08<00:26,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  72%|███████▏  | 83/115 [01:08<00:26,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  72%|███████▏  | 83/115 [01:08<00:26,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  73%|███████▎  | 84/115 [01:08<00:25,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  73%|███████▎  | 84/115 [01:08<00:25,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  74%|███████▍  | 85/115 [01:08<00:24,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  73%|███████▎  | 84/115 [01:09<00:25,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  73%|███████▎  | 84/115 [01:09<00:25,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  73%|███████▎  | 84/115 [01:09<00:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  73%|███████▎  | 84/115 [01:09<00:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  74%|███████▍  | 85/115 [01:09<00:24,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  74%|███████▍  | 85/115 [01:09<00:24,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  75%|███████▍  | 86/115 [01:09<00:23,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  74%|███████▍  | 85/115 [01:10<00:24,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  74%|███████▍  | 85/115 [01:10<00:24,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  74%|███████▍  | 85/115 [01:10<00:24,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  74%|███████▍  | 85/115 [01:10<00:24,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  75%|███████▍  | 86/115 [01:10<00:23,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  75%|███████▍  | 86/115 [01:10<00:23,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  76%|███████▌  | 87/115 [01:10<00:22,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  75%|███████▍  | 86/115 [01:10<00:23,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  75%|███████▍  | 86/115 [01:10<00:23,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  75%|███████▍  | 86/115 [01:11<00:23,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  75%|███████▍  | 86/115 [01:11<00:23,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  76%|███████▌  | 87/115 [01:11<00:22,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  76%|███████▌  | 87/115 [01:11<00:22,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 88/115 [01:11<00:21,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  76%|███████▌  | 87/115 [01:11<00:22,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  76%|███████▌  | 87/115 [01:11<00:22,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  76%|███████▌  | 87/115 [01:11<00:23,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  76%|███████▌  | 87/115 [01:12<00:23,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 88/115 [01:12<00:21,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 88/115 [01:12<00:22,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 89/115 [01:12<00:21,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 88/115 [01:12<00:22,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 88/115 [01:12<00:22,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 88/115 [01:12<00:22,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 88/115 [01:12<00:22,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 89/115 [01:12<00:21,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 89/115 [01:13<00:21,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  78%|███████▊  | 90/115 [01:13<00:20,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 89/115 [01:13<00:21,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 89/115 [01:13<00:21,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 89/115 [01:13<00:21,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  78%|███████▊  | 90/115 [01:13<00:20,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 89/115 [01:13<00:21,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  78%|███████▊  | 90/115 [01:13<00:20,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  79%|███████▉  | 91/115 [01:13<00:19,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  78%|███████▊  | 90/115 [01:14<00:20,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  78%|███████▊  | 90/115 [01:14<00:20,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  78%|███████▊  | 90/115 [01:14<00:20,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  79%|███████▉  | 91/115 [01:14<00:19,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  80%|████████  | 92/115 [01:14<00:18,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  79%|███████▉  | 91/115 [01:14<00:19,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  78%|███████▊  | 90/115 [01:14<00:20,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  79%|███████▉  | 91/115 [01:14<00:19,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  79%|███████▉  | 91/115 [01:14<00:19,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  79%|███████▉  | 91/115 [01:15<00:19,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  81%|████████  | 93/115 [01:15<00:17,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  80%|████████  | 92/115 [01:15<00:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  80%|████████  | 92/115 [01:15<00:18,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  79%|███████▉  | 91/115 [01:15<00:19,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  80%|████████  | 92/115 [01:15<00:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  80%|████████  | 92/115 [01:15<00:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  80%|████████  | 92/115 [01:16<00:19,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  82%|████████▏ | 94/115 [01:16<00:17,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  81%|████████  | 93/115 [01:16<00:18,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  81%|████████  | 93/115 [01:16<00:18,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  80%|████████  | 92/115 [01:16<00:19,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  81%|████████  | 93/115 [01:16<00:18,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  81%|████████  | 93/115 [01:16<00:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  81%|████████  | 93/115 [01:16<00:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 95/115 [01:17<00:16,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  82%|████████▏ | 94/115 [01:17<00:17,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  82%|████████▏ | 94/115 [01:17<00:17,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  81%|████████  | 93/115 [01:17<00:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  82%|████████▏ | 94/115 [01:17<00:17,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  82%|████████▏ | 94/115 [01:17<00:17,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  82%|████████▏ | 94/115 [01:17<00:17,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 96/115 [01:17<00:15,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 95/115 [01:17<00:16,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 95/115 [01:17<00:16,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  82%|████████▏ | 94/115 [01:17<00:17,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 95/115 [01:18<00:16,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 95/115 [01:18<00:16,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 95/115 [01:18<00:16,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  84%|████████▍ | 97/115 [01:18<00:14,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 96/115 [01:18<00:15,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 96/115 [01:18<00:15,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 95/115 [01:18<00:16,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 96/115 [01:19<00:15,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 96/115 [01:19<00:15,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 96/115 [01:19<00:15,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  85%|████████▌ | 98/115 [01:19<00:13,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  84%|████████▍ | 97/115 [01:19<00:14,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  84%|████████▍ | 97/115 [01:19<00:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 96/115 [01:19<00:15,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  84%|████████▍ | 97/115 [01:19<00:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  84%|████████▍ | 97/115 [01:19<00:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  84%|████████▍ | 97/115 [01:20<00:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  86%|████████▌ | 99/115 [01:20<00:13,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  85%|████████▌ | 98/115 [01:20<00:13,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  85%|████████▌ | 98/115 [01:20<00:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  84%|████████▍ | 97/115 [01:20<00:14,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  85%|████████▌ | 98/115 [01:20<00:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  85%|████████▌ | 98/115 [01:20<00:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  85%|████████▌ | 98/115 [01:20<00:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  87%|████████▋ | 100/115 [01:21<00:12,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  86%|████████▌ | 99/115 [01:21<00:13,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  86%|████████▌ | 99/115 [01:21<00:13,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  85%|████████▌ | 98/115 [01:21<00:14,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  86%|████████▌ | 99/115 [01:21<00:13,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  86%|████████▌ | 99/115 [01:21<00:13,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  86%|████████▌ | 99/115 [01:21<00:13,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  88%|████████▊ | 101/115 [01:21<00:11,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  87%|████████▋ | 100/115 [01:22<00:12,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  87%|████████▋ | 100/115 [01:22<00:12,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  86%|████████▌ | 99/115 [01:22<00:13,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  87%|████████▋ | 100/115 [01:22<00:12,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  87%|████████▋ | 100/115 [01:22<00:12,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  87%|████████▋ | 100/115 [01:22<00:12,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  89%|████████▊ | 102/115 [01:22<00:10,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  88%|████████▊ | 101/115 [01:22<00:11,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  88%|████████▊ | 101/115 [01:22<00:11,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  87%|████████▋ | 100/115 [01:22<00:12,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  88%|████████▊ | 101/115 [01:23<00:11,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  88%|████████▊ | 101/115 [01:23<00:11,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  88%|████████▊ | 101/115 [01:23<00:11,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|████████▉ | 103/115 [01:23<00:09,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  89%|████████▊ | 102/115 [01:23<00:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  89%|████████▊ | 102/115 [01:23<00:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  88%|████████▊ | 101/115 [01:23<00:11,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  89%|████████▊ | 102/115 [01:24<00:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  89%|████████▊ | 102/115 [01:24<00:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  89%|████████▊ | 102/115 [01:24<00:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|█████████ | 104/115 [01:24<00:08,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|████████▉ | 103/115 [01:24<00:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|████████▉ | 103/115 [01:24<00:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  89%|████████▊ | 102/115 [01:24<00:10,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|████████▉ | 103/115 [01:24<00:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|████████▉ | 103/115 [01:24<00:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|████████▉ | 103/115 [01:25<00:09,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  91%|█████████▏| 105/115 [01:25<00:08,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|█████████ | 104/115 [01:25<00:09,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|█████████ | 104/115 [01:25<00:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|████████▉ | 103/115 [01:25<00:09,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|█████████ | 104/115 [01:25<00:09,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|█████████ | 104/115 [01:25<00:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|█████████ | 104/115 [01:25<00:09,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  92%|█████████▏| 106/115 [01:26<00:07,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  91%|█████████▏| 105/115 [01:26<00:08,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  91%|█████████▏| 105/115 [01:26<00:08,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|█████████ | 104/115 [01:26<00:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  91%|█████████▏| 105/115 [01:26<00:08,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  91%|█████████▏| 105/115 [01:26<00:08,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  91%|█████████▏| 105/115 [01:26<00:08,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  93%|█████████▎| 107/115 [01:26<00:06,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  92%|█████████▏| 106/115 [01:27<00:07,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  92%|█████████▏| 106/115 [01:27<00:07,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  91%|█████████▏| 105/115 [01:27<00:08,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  92%|█████████▏| 106/115 [01:27<00:07,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  92%|█████████▏| 106/115 [01:27<00:07,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  92%|█████████▏| 106/115 [01:27<00:07,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  94%|█████████▍| 108/115 [01:27<00:05,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  93%|█████████▎| 107/115 [01:27<00:06,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  93%|█████████▎| 107/115 [01:27<00:06,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  92%|█████████▏| 106/115 [01:27<00:07,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  93%|█████████▎| 107/115 [01:28<00:06,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  93%|█████████▎| 107/115 [01:28<00:06,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  93%|█████████▎| 107/115 [01:28<00:06,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  95%|█████████▍| 109/115 [01:28<00:04,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  94%|█████████▍| 108/115 [01:28<00:05,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  94%|█████████▍| 108/115 [01:28<00:05,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  93%|█████████▎| 107/115 [01:28<00:06,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  94%|█████████▍| 108/115 [01:28<00:05,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  94%|█████████▍| 108/115 [01:28<00:05,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  96%|█████████▌| 110/115 [01:29<00:04,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  94%|█████████▍| 108/115 [01:29<00:05,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  95%|█████████▍| 109/115 [01:29<00:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  95%|█████████▍| 109/115 [01:29<00:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  94%|█████████▍| 108/115 [01:29<00:05,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  95%|█████████▍| 109/115 [01:29<00:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  95%|█████████▍| 109/115 [01:29<00:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 111/115 [01:30<00:03,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  95%|█████████▍| 109/115 [01:30<00:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  96%|█████████▌| 110/115 [01:30<00:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  96%|█████████▌| 110/115 [01:30<00:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  95%|█████████▍| 109/115 [01:30<00:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  96%|█████████▌| 110/115 [01:30<00:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  96%|█████████▌| 110/115 [01:30<00:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 112/115 [01:30<00:02,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  96%|█████████▌| 110/115 [01:30<00:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 111/115 [01:31<00:03,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 111/115 [01:31<00:03,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  96%|█████████▌| 110/115 [01:31<00:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 111/115 [01:31<00:03,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 111/115 [01:31<00:03,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  98%|█████████▊| 113/115 [01:31<00:01,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 111/115 [01:31<00:03,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 112/115 [01:31<00:02,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 112/115 [01:31<00:02,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 111/115 [01:32<00:03,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 112/115 [01:32<00:02,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 112/115 [01:32<00:02,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  99%|█████████▉| 114/115 [01:32<00:00,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 112/115 [01:32<00:02,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  98%|█████████▊| 113/115 [01:32<00:01,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  98%|█████████▊| 113/115 [01:32<00:01,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 112/115 [01:32<00:02,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:32<00:00,  1.40ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:32<00:00,  1.24ba/s]\u001b[0m\n",
      "\u001b[34minput_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\u001b[0m\n",
      "\u001b[34minputs 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[34mlabel_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[34mlabels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  98%|█████████▊| 113/115 [01:33<00:01,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  98%|█████████▊| 113/115 [01:33<00:01,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  98%|█████████▊| 113/115 [01:33<00:01,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  99%|█████████▉| 114/115 [01:33<00:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  99%|█████████▉| 114/115 [01:33<00:00,  1.19ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  98%|█████████▊| 113/115 [01:33<00:01,  1.19ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  99%|█████████▉| 114/115 [01:33<00:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  99%|█████████▉| 114/115 [01:34<00:00,  1.16ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.38ba/s]#015Running tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[34minput_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\u001b[0m\n",
      "\u001b[34minputs 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[34mlabel_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[34mlabels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.31ba/s]#015Running tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[34minput_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\u001b[0m\n",
      "\u001b[34minputs 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[34mlabel_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[34mlabels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  99%|█████████▉| 114/115 [01:34<00:00,  1.19ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.34ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[34minput_ids\u001b[0m\n",
      "\u001b[34m[5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\u001b[0m\n",
      "\u001b[34minputs\u001b[0m\n",
      "\u001b[34m类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[34mlabel_ids\u001b[0m\n",
      "\u001b[34m[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[34mlabels\u001b[0m\n",
      "\u001b[34m<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.28ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[34minput_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\u001b[0m\n",
      "\u001b[34minputs 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[34mlabel_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[34mlabels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  99%|█████████▉| 114/115 [01:34<00:00,  1.14ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.30ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[34minput_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\u001b[0m\n",
      "\u001b[34minputs\u001b[0m\n",
      "\u001b[34m类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[34mlabel_ids\u001b[0m\n",
      "\u001b[34m[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[34mlabels\u001b[0m\n",
      "\u001b[34m<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:35<00:00,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:35<00:00,  1.20ba/s]\u001b[0m\n",
      "\u001b[34minput_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\u001b[0m\n",
      "\u001b[34minputs 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[34mlabel_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[34mlabels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:10:33,699] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:10:34,950] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:10:34,954] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:10:34,954] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:10:34,955] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:10:34,956] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:10:34,957] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:10:34,958] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:10:34,961] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py39_cu117/cpu_adam...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34m[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o\u001b[0m\n",
      "\u001b[34m[2/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o\u001b[0m\n",
      "\u001b[34m[3/3] c++ cpu_adam.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 28.788599014282227 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 28.756749868392944 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 28.75365424156189 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 28.84813928604126 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 28.74653697013855 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 28.823689937591553 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 28.85432243347168 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 28.85134482383728 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py39_cu117/utils...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mAdam Optimizer #0 is created with AVX2 arithmetic capability.\u001b[0m\n",
      "\u001b[34mConfig: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:05,858] [INFO] [logging.py:93:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:05,879] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:05,879] [INFO] [utils.py:55:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:05,879] [INFO] [logging.py:93:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:05,879] [INFO] [stage_1_and_2.py:144:__init__] Reduce bucket size 500000000\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:05,879] [INFO] [stage_1_and_2.py:145:__init__] Allgather bucket size 500000000\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:05,879] [INFO] [stage_1_and_2.py:146:__init__] CPU Offload: True\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:05,879] [INFO] [stage_1_and_2.py:147:__init__] Round robin gradient partitioning: False\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module utils...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[34m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.361724376678467 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.321313381195068 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.421755313873291 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.322146654129028 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.32215142250061 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.321666955947876 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.321830034255981 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.321960210800171 seconds\u001b[0m\n",
      "\u001b[34mRank: 2 partition count [8] and sizes[(771660800, False)]\u001b[0m\n",
      "\u001b[34mRank: 5 partition count [8] and sizes[(771660800, False)]\u001b[0m\n",
      "\u001b[34mRank: 3 partition count [8] and sizes[(771660800, False)]\u001b[0m\n",
      "\u001b[34mRank: 1 partition count [8] and sizes[(771660800, False)]\u001b[0m\n",
      "\u001b[34mRank: 6 partition count [8] and sizes[(771660800, False)]\u001b[0m\n",
      "\u001b[34mRank: 0 partition count [8] and sizes[(771660800, False)]\u001b[0m\n",
      "\u001b[34mRank: 7 partition count [8] and sizes[(771660800, False)]\u001b[0m\n",
      "\u001b[34mRank: 4 partition count [8] and sizes[(771660800, False)]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:39,179] [INFO] [utils.py:829:see_memory_usage] Before initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:39,180] [INFO] [utils.py:830:see_memory_usage] MA 12.5 GB         Max_MA 12.5 GB         CA 12.5 GB         Max_CA 12 GB\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:39,180] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 127.98 GB, percent = 17.1%\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00040268898010253906 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00046181678771972656 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0004971027374267578 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0005135536193847656 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0003921985626220703 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0003705024719238281 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00036787986755371094 seconds\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.754: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.772: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.781: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.782 algo-1:524 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.782: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.783: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,787] [INFO] [utils.py:829:see_memory_usage] After initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,788] [INFO] [utils.py:830:see_memory_usage] MA 12.5 GB         Max_MA 12.5 GB         CA 12.5 GB         Max_CA 12 GB\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,788] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 187.89 GB, percent = 25.1%\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,788] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.793: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.799 algo-1:523 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.808: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.809 algo-1:524 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.810 algo-1:524 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.810 algo-1:520 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.810 algo-1:524 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.811 algo-1:519 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.811 algo-1:524 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.811 algo-1:524 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.811 algo-1:525 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.820 algo-1:521 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.826 algo-1:523 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.826 algo-1:523 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.826 algo-1:523 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.827 algo-1:523 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.827 algo-1:523 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.836 algo-1:522 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.838 algo-1:520 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.839 algo-1:520 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.839 algo-1:520 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.839 algo-1:519 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.840 algo-1:519 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.840 algo-1:520 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.840 algo-1:520 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.840 algo-1:519 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.840 algo-1:525 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.840 algo-1:519 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.840 algo-1:519 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.841 algo-1:525 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.841 algo-1:525 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.848 algo-1:525 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.848 algo-1:525 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.849 algo-1:521 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.849 algo-1:521 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.849 algo-1:521 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.854 algo-1:521 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.855 algo-1:521 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.876 algo-1:522 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.876 algo-1:522 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.876 algo-1:522 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.877 algo-1:522 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43.877 algo-1:522 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,918] [INFO] [utils.py:829:see_memory_usage] After initializing ZeRO optimizer\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,919] [INFO] [utils.py:830:see_memory_usage] MA 12.5 GB         Max_MA 12.5 GB         CA 12.5 GB         Max_CA 12 GB\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,919] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 188.51 GB, percent = 25.2%\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,921] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,921] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,921] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f96b4d1ff40>\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,921] [INFO] [logging.py:93:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,922] [INFO] [config.py:1018:print] DeepSpeedEngine configuration:\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,922] [INFO] [config.py:1022:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,922] [INFO] [config.py:1022:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,922] [INFO] [config.py:1022:print]   amp_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,922] [INFO] [config.py:1022:print]   amp_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   bfloat16_enabled ............. False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   checkpoint_parallel_write_pipeline  False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   checkpoint_tag_validation_enabled  True\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   checkpoint_tag_validation_fail  False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f96e7fa3f40>\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   communication_data_type ...... None\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   curriculum_enabled_legacy .... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   curriculum_params_legacy ..... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   data_efficiency_enabled ...... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   dataloader_drop_last ......... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   disable_allgather ............ False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   dump_state ................... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   eigenvalue_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   eigenvalue_gas_boundary_resolution  1\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   eigenvalue_layer_name ........ bert.encoder.layer\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   eigenvalue_layer_num ......... 0\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   eigenvalue_max_iter .......... 100\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   eigenvalue_stability ......... 1e-06\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   eigenvalue_tol ............... 0.01\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   eigenvalue_verbose ........... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   elasticity_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   fp16_auto_cast ............... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   fp16_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   fp16_master_weights_and_gradients  False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   global_rank .................. 0\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   grad_accum_dtype ............. None\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   gradient_accumulation_steps .. 1\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   gradient_clipping ............ 0.0\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   gradient_predivide_factor .... 1.0\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   initial_dynamic_scale ........ 65536\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   load_universal_checkpoint .... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   loss_scale ................... 0\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,923] [INFO] [config.py:1022:print]   memory_breakdown ............. False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,924] [INFO] [config.py:1022:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,924] [INFO] [config.py:1022:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,924] [INFO] [config.py:1022:print]   optimizer_legacy_fusion ...... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,924] [INFO] [config.py:1022:print]   optimizer_name ............... adamw\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,924] [INFO] [config.py:1022:print]   optimizer_params ............. {'lr': 0.0001, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,924] [INFO] [config.py:1022:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,924] [INFO] [config.py:1022:print]   pld_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,924] [INFO] [config.py:1022:print]   pld_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,924] [INFO] [config.py:1022:print]   prescale_gradients ........... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,924] [INFO] [config.py:1022:print]   scheduler_name ............... WarmupLR\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,924] [INFO] [config.py:1022:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 0}\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,924] [INFO] [config.py:1022:print]   sparse_attention ............. None\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,924] [INFO] [config.py:1022:print]   sparse_gradients_enabled ..... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,924] [INFO] [config.py:1022:print]   steps_per_print .............. 10\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,924] [INFO] [config.py:1022:print]   train_batch_size ............. 8\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,924] [INFO] [config.py:1022:print]   train_micro_batch_size_per_gpu  1\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,924] [INFO] [config.py:1022:print]   use_node_local_storage ....... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,924] [INFO] [config.py:1022:print]   wall_clock_breakdown ......... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,924] [INFO] [config.py:1022:print]   world_size ................... 8\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,924] [INFO] [config.py:1022:print]   zero_allow_untested_optimizer  True\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,924] [INFO] [config.py:1022:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,924] [INFO] [config.py:1022:print]   zero_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,924] [INFO] [config.py:1022:print]   zero_force_ds_cpu_optimizer .. True\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,924] [INFO] [config.py:1022:print]   zero_optimization_stage ...... 2\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:43,924] [INFO] [config.py:1007:print_user_config]   json = {\n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"zero_allow_untested_optimizer\": true, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"loss_scale\": 0, \n",
      "        \"initial_scale_power\": 16, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 0.0001, \n",
      "            \"betas\": [0.9, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 0.0\n",
      "        }\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupLR\", \n",
      "        \"params\": {\n",
      "            \"warmup_min_lr\": 0, \n",
      "            \"warmup_max_lr\": 0.0001, \n",
      "            \"warmup_num_steps\": 0\n",
      "        }\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 5.000000e+08, \n",
      "        \"overlap_comm\": false, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 5.000000e+08, \n",
      "        \"contiguous_gradients\": true\n",
      "    }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00034499168395996094 seconds\u001b[0m\n",
      "\u001b[34m0%|          | 0/20 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:44.033: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:44.060 algo-1:518 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m05/11/2023 10:11:44 - WARNING - transformers_modules.THUDM.chatglm-6b.4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:44.088 algo-1:518 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m05/11/2023 10:11:44 - WARNING - transformers_modules.THUDM.chatglm-6b.4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:44.089 algo-1:518 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:44.089 algo-1:518 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:44.090 algo-1:518 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:44.090 algo-1:518 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m05/11/2023 10:11:44 - WARNING - transformers_modules.THUDM.chatglm-6b.4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m05/11/2023 10:11:44 - WARNING - transformers_modules.THUDM.chatglm-6b.4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m05/11/2023 10:11:44 - WARNING - transformers_modules.THUDM.chatglm-6b.4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m05/11/2023 10:11:44 - WARNING - transformers_modules.THUDM.chatglm-6b.4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m05/11/2023 10:11:44 - WARNING - transformers_modules.THUDM.chatglm-6b.4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m05/11/2023 10:11:44 - WARNING - transformers_modules.THUDM.chatglm-6b.4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:11:54,167] [INFO] [loss_scaler.py:180:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\u001b[0m\n",
      "\u001b[34m5%|▌         | 1/20 [00:10<03:14, 10.24s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:12:03,001] [INFO] [loss_scaler.py:173:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\u001b[0m\n",
      "\u001b[34m10%|█         | 2/20 [00:19<02:49,  9.41s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:12:11,821] [INFO] [loss_scaler.py:173:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\u001b[0m\n",
      "\u001b[34m15%|█▌        | 3/20 [00:27<02:35,  9.14s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:12:20,608] [INFO] [loss_scaler.py:173:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\u001b[0m\n",
      "\u001b[34m20%|██        | 4/20 [00:36<02:24,  9.00s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:12:29,383] [INFO] [loss_scaler.py:173:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\u001b[0m\n",
      "\u001b[34m25%|██▌       | 5/20 [00:45<02:13,  8.92s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 6/20 [01:19<04:03, 17.37s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:13:12,407] [INFO] [loss_scaler.py:173:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\u001b[0m\n",
      "\u001b[34m35%|███▌      | 7/20 [01:28<03:11, 14.71s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 8/20 [02:02<04:10, 20.86s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 9/20 [02:36<04:36, 25.11s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:14:30,151] [INFO] [loss_scaler.py:173:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:14:30,152] [INFO] [logging.py:93:log_dist] [Rank 0] step=10, skipped=7, lr=[0.0001], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:14:30,152] [INFO] [timer.py:198:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=0.4350038209000939, CurrSamplesPerSec=0.862257510864166, MemAllocated=12.5GB, MaxMemAllocated=18.41GB\u001b[0m\n",
      "\u001b[34m50%|█████     | 10/20 [02:46<03:22, 20.22s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 6.7124, 'learning_rate': 0.0001, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m50%|█████     | 10/20 [02:46<03:22, 20.22s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:14:39,035] [INFO] [loss_scaler.py:173:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024, reducing to 512\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 11/20 [02:55<02:30, 16.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:14:47,915] [INFO] [loss_scaler.py:173:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 512, reducing to 256\u001b[0m\n",
      "\u001b[34m60%|██████    | 12/20 [03:03<01:54, 14.36s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 13/20 [03:37<02:21, 20.26s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 14/20 [04:12<02:26, 24.49s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 15/20 [04:46<02:17, 27.47s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 16/20 [05:20<01:58, 29.51s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 17/20 [05:55<01:32, 30.98s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 18/20 [06:29<01:04, 32.06s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 19/20 [07:04<00:32, 32.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:19:18,920] [INFO] [logging.py:93:log_dist] [Rank 0] step=20, skipped=9, lr=[0.0001], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:19:22,893] [INFO] [timer.py:198:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=0.3273997905764818, CurrSamplesPerSec=0.23080101323875837, MemAllocated=12.5GB, MaxMemAllocated=18.41GB\u001b[0m\n",
      "\u001b[34m100%|██████████| 20/20 [07:38<00:00, 33.38s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 9.8161, 'learning_rate': 0.0001, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 20/20 [07:38<00:00, 33.38s/it]\u001b[0m\n",
      "\u001b[34mSaving the whole model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-05-11 10:19:22,900 >> Configuration saved in /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-05-11 10:19:22,900 >> Configuration saved in /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-05-11 10:19:22,900 >> Configuration saved in /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-05-11 10:19:22,900 >> Configuration saved in /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1855] 2023-05-11 10:19:52,626 >> The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/pytorch_model.bin.index.json.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1855] 2023-05-11 10:19:52,626 >> The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/pytorch_model.bin.index.json.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-05-11 10:19:52,627 >> tokenizer config file saved in /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-05-11 10:19:52,627 >> tokenizer config file saved in /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-05-11 10:19:52,627 >> Special tokens file saved in /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-05-11 10:19:52,627 >> Special tokens file saved in /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:19:52,630] [INFO] [logging.py:93:log_dist] [Rank 0] [Torch] Checkpoint global_step20 is about to be saved!\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:19:52,634] [INFO] [logging.py:93:log_dist] [Rank 0] Saving model checkpoint: /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/global_step20/mp_rank_00_model_states.pt\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:19:52,634] [INFO] [torch_checkpoint_engine.py:17:save] [Torch] Saving /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/global_step20/mp_rank_00_model_states.pt...\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:20:07,353] [INFO] [torch_checkpoint_engine.py:19:save] [Torch] Saved /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/global_step20/mp_rank_00_model_states.pt.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:20:07,355] [INFO] [torch_checkpoint_engine.py:17:save] [Torch] Saving /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/global_step20/zero_pp_rank_0_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:20:23,297] [INFO] [torch_checkpoint_engine.py:19:save] [Torch] Saved /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/global_step20/zero_pp_rank_0_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:20:23,298] [INFO] [engine.py:3430:_save_zero_checkpoint] zero checkpoint saved /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/global_step20/zero_pp_rank_0_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:20:23,298] [INFO] [torch_checkpoint_engine.py:29:commit] [Torch] Checkpoint global_step20 is ready now!\u001b[0m\n",
      "\u001b[34m{'train_runtime': 519.3743, 'train_samples_per_second': 0.308, 'train_steps_per_second': 0.039, 'train_loss': 8.26422119140625, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 20/20 [08:39<00:00, 33.38s/it]\u001b[0m\n",
      "\u001b[34m------saving model!-----------saving model!-----\u001b[0m\n",
      "\u001b[34m------saving model!-----\u001b[0m\n",
      "\u001b[34m------saving model!-----\u001b[0m\n",
      "\u001b[34m100%|██████████| 20/20 [08:39<00:00, 25.97s/it]\u001b[0m\n",
      "\u001b[34m------saving model!-----\u001b[0m\n",
      "\u001b[34m------saving model!-----***** train metrics *****\u001b[0m\n",
      "\u001b[34mepoch                    =        0.0\n",
      "  train_loss               =     8.2642\n",
      "  train_runtime            = 0:08:39.37\n",
      "  train_samples            =     114599\n",
      "  train_samples_per_second =      0.308\n",
      "  train_steps_per_second   =      0.039\u001b[0m\n",
      "\u001b[34m------saving model!-----\u001b[0m\n",
      "\u001b[34m------saving model!-----\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-05-11 10:20:23,304 >> tokenizer config file saved in /tmp/model/adgen-chatglm-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-05-11 10:20:23,304 >> tokenizer config file saved in /tmp/model/adgen-chatglm-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-05-11 10:20:23,304 >> Special tokens file saved in /tmp/model/adgen-chatglm-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-05-11 10:20:23,304 >> Special tokens file saved in /tmp/model/adgen-chatglm-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m------model is saved!-----\u001b[0m\n",
      "\u001b[34mworld_rank===3\u001b[0m\n",
      "\u001b[34m------model is saved!-----\u001b[0m\n",
      "\u001b[34mworld_rank===6\u001b[0m\n",
      "\u001b[34m------model is saved!-----\u001b[0m\n",
      "\u001b[34mworld_rank===2\u001b[0m\n",
      "\u001b[34m------model is saved!-----\u001b[0m\n",
      "\u001b[34mworld_rank===1\u001b[0m\n",
      "\u001b[34m------model is saved!-----\u001b[0m\n",
      "\u001b[34mworld_rank===4\u001b[0m\n",
      "\u001b[34mSaving the whole model\u001b[0m\n",
      "\u001b[34m------model is saved!-----\u001b[0m\n",
      "\u001b[34mworld_rank===5\u001b[0m\n",
      "\u001b[34m------model is saved!-----\u001b[0m\n",
      "\u001b[34mworld_rank===7\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-05-11 10:20:23,313 >> Configuration saved in /tmp/model/adgen-chatglm-6b-ft/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-05-11 10:20:23,313 >> Configuration saved in /tmp/model/adgen-chatglm-6b-ft/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-05-11 10:20:23,313 >> Configuration saved in /tmp/model/adgen-chatglm-6b-ft/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-05-11 10:20:23,313 >> Configuration saved in /tmp/model/adgen-chatglm-6b-ft/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1855] 2023-05-11 10:20:52,991 >> The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/model/adgen-chatglm-6b-ft/pytorch_model.bin.index.json.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1855] 2023-05-11 10:20:52,991 >> The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/model/adgen-chatglm-6b-ft/pytorch_model.bin.index.json.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-05-11 10:20:52,994 >> tokenizer config file saved in /tmp/model/adgen-chatglm-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-05-11 10:20:52,994 >> tokenizer config file saved in /tmp/model/adgen-chatglm-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-05-11 10:20:52,995 >> Special tokens file saved in /tmp/model/adgen-chatglm-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-05-11 10:20:52,995 >> Special tokens file saved in /tmp/model/adgen-chatglm-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m------model is saved!-----\u001b[0m\n",
      "\u001b[34mworld_rank===0\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/generation_config.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/generation_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/config.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/special_tokens_map.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/rng_state_3.pth s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/rng_state_3.pth\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/configuration_chatglm.py s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/configuration_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/config.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/generation_config.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/generation_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/configuration_chatglm.py s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/configuration_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/training_args.bin s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/training_args.bin\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/trainer_state.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/trainer_state.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/rng_state_0.pth s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/rng_state_0.pth\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/special_tokens_map.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/training_args.bin s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/training_args.bin\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/quantization.py s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/quantization.py\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/latest s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/latest\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/train_results.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/train_results.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/tokenizer_config.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/rng_state_2.pth s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/rng_state_2.pth\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/all_results.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/all_results.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/tokenization_chatglm.py s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/tokenization_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/rng_state_5.pth s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/rng_state_5.pth\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/tokenization_chatglm.py s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/tokenization_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/rng_state_1.pth s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/rng_state_1.pth\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/modeling_chatglm.py s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/modeling_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/tokenizer_config.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/zero_to_fp32.py s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/zero_to_fp32.py\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/modeling_chatglm.py s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/modeling_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/rng_state_6.pth s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/rng_state_6.pth\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/rng_state_7.pth s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/rng_state_7.pth\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/pytorch_model.bin.index.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/pytorch_model.bin.index.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/trainer_state.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/trainer_state.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/rng_state_4.pth s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/rng_state_4.pth\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/ice_text.model s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/ice_text.model\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/ice_text.model s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/ice_text.model\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/quantization.py s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/quantization.py\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/global_step20/zero_pp_rank_7_mp_rank_00_optim_states.pt s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/global_step20/zero_pp_rank_7_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/global_step20/zero_pp_rank_0_mp_rank_00_optim_states.pt s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/global_step20/zero_pp_rank_0_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/global_step20/zero_pp_rank_2_mp_rank_00_optim_states.pt s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/global_step20/zero_pp_rank_2_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/global_step20/zero_pp_rank_5_mp_rank_00_optim_states.pt s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/global_step20/zero_pp_rank_5_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/global_step20/zero_pp_rank_6_mp_rank_00_optim_states.pt s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/global_step20/zero_pp_rank_6_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/global_step20/zero_pp_rank_1_mp_rank_00_optim_states.pt s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/global_step20/zero_pp_rank_1_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/global_step20/zero_pp_rank_4_mp_rank_00_optim_states.pt s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/global_step20/zero_pp_rank_4_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/global_step20/zero_pp_rank_3_mp_rank_00_optim_states.pt s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/global_step20/zero_pp_rank_3_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/global_step20/mp_rank_00_model_states.pt s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/global_step20/mp_rank_00_model_states.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/pytorch_model-00001-of-00002.bin s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/pytorch_model-00001-of-00002.bin\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/pytorch_model-00002-of-00002.bin s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/pytorch_model-00002-of-00002.bin\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-20/pytorch_model-00001-of-00002.bin s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/checkpoint-20/pytorch_model-00001-of-00002.bin\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/pytorch_model-00002-of-00002.bin s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/pytorch_model-00002-of-00002.bin\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:21:40,381] [INFO] [launch.py:350:main] Process 521 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:21:41,382] [INFO] [launch.py:350:main] Process 518 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:21:43,384] [INFO] [launch.py:350:main] Process 523 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:21:44,385] [INFO] [launch.py:350:main] Process 522 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:21:44,385] [INFO] [launch.py:350:main] Process 525 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:21:44,385] [INFO] [launch.py:350:main] Process 519 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:21:46,388] [INFO] [launch.py:350:main] Process 520 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:21:46,388] [INFO] [launch.py:350:main] Process 524 exits successfully.\u001b[0m\n",
      "\u001b[34m2023-05-11 10:21:47,867 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-05-11 10:21:47,867 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-05-11 10:21:47,868 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-05-11 10:21:59 Uploading - Uploading generated training model\n",
      "2023-05-11 10:21:59 Completed - Training job completed\n",
      "Training seconds: 1316\n",
      "Billable seconds: 1316\n"
     ]
    }
   ],
   "source": [
    "# define Training Job Name \n",
    "job_name = f'huggingface-chatglm-deepspeed-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "#define the model s3 path which will store your trained model asset\n",
    "#Note: you should use your real s3 path to configure model_s3_path\n",
    "model_s3_path='s3://{}/llm/models/chatglm/deepspeed-single-node/'.format(sagemaker_session.default_bucket())\n",
    "output_dir = '/tmp/model/adgen-chatglm-6b-ft'\n",
    "model_name_or_path = 'THUDM/chatglm-6b'\n",
    "#model_name_or_path = \"/tmp/chatglm/\"\n",
    "\n",
    "\n",
    "instance_count = 1\n",
    "#define the enviroment variables for your scripts.\n",
    "environment = {\n",
    "              \n",
    "              'MODEL_S3_PATH'          : model_uri,\n",
    "              'PYTORCH_CUDA_ALLOC_CONF': 'max_split_size_mb:32',\n",
    "              #'LD_LIBRARY_PATH'        : '${LD_LIBRARY_PATH}:/opt/conda/lib/',\n",
    "              'NUM_GPUS'               : str(processes_per_host),\n",
    "              'TRAIN_DATASET'          : '/opt/ml/input/data/AdvertiseGen/train.json',\n",
    "              'TEST_DATASET'           : '/opt/ml/input/data/AdvertiseGen/dev.json',\n",
    "              'PROMPT_COLUMN'          : 'content',\n",
    "              'RESPONSE_COLUMN'        : 'summary',\n",
    "              'MODEL_NAME_OR_PATH'     : model_name_or_path,\n",
    "              'OUTPUT_DIR'             : output_dir,\n",
    "              'MODEL_OUTPUT_S3_PATH'   : model_s3_path,\n",
    "              'TRAIN_STEPS'            :'20'\n",
    "}\n",
    "\n",
    "inputs={\n",
    "   'AdvertiseGen': f\"s3://{bucket}/llm/chatglm/datasets/\"\n",
    "}\n",
    "\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'start-single-node.py',          # user endpoint script\n",
    "    source_dir           = './ptuning',               # directory which includes all the files needed for training\n",
    "    instance_type        = instance_type, # instances type used for the training job\n",
    "    instance_count       = instance_count,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    script_mode          = True,\n",
    "    transformers_version = '4.26',            # the transformers version used in the training job\n",
    "    pytorch_version      = '1.13',            # the pytorch_version version used in the training job\n",
    "    py_version           = 'py39',            # the python version used in the training job\n",
    "    environment = environment,\n",
    ")\n",
    "huggingface_estimator.fit(inputs=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4ed43d3-3727-421d-8a5f-d289dcdd2f22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE checkpoint-20/\n",
      "2023-05-11 10:20:54        192 all_results.json\n",
      "2023-05-11 10:20:54        870 config.json\n",
      "2023-05-11 10:20:54       4276 configuration_chatglm.py\n",
      "2023-05-11 10:20:54        142 generation_config.json\n",
      "2023-05-11 10:20:54    2706249 ice_text.model\n",
      "2023-05-11 10:20:54      57620 modeling_chatglm.py\n",
      "2023-05-11 10:20:54 12346621179 pytorch_model-00001-of-00002.bin\n",
      "2023-05-11 10:20:54 12346585635 pytorch_model-00002-of-00002.bin\n",
      "2023-05-11 10:20:54      33416 pytorch_model.bin.index.json\n",
      "2023-05-11 10:20:54      15054 quantization.py\n",
      "2023-05-11 10:20:54        125 special_tokens_map.json\n",
      "2023-05-11 10:20:54      17047 tokenization_chatglm.py\n",
      "2023-05-11 10:20:54        495 tokenizer_config.json\n",
      "2023-05-11 10:20:54        192 train_results.json\n",
      "2023-05-11 10:20:54        808 trainer_state.json\n",
      "2023-05-11 10:20:54       4795 training_args.bin\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-single-node/adgen-chatglm-6b-ft/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf58dcd-d9f2-43b8-b9b7-f477985fb823",
   "metadata": {
    "tags": []
   },
   "source": [
    "### chatglm deepspeed 多机多卡改造\n",
    "1: 准备deepspeed lib，并修改deepspeed.json    \n",
    "2：数据集（以上一致）  \n",
    "3：entrypoint start.py,设置torch distribute launch configure  \n",
    "4：触发bash ds_train_finetune.sh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ea32b09-7afd-42f1-b144-721db4660e53",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-chatglm-deepspeed-2023-05-1-2023-05-11-10-22-23-381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-11 10:22:25 Starting - Starting the training job......\n",
      "2023-05-11 10:23:04 Starting - Preparing the instances for training......\n",
      "2023-05-11 10:24:19 Downloading - Downloading input data...\n",
      "2023-05-11 10:24:33 Training - Downloading the training image..................\n",
      "2023-05-11 10:27:50 Training - Training image download completed. Training in progress.......\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2023-05-11 10:28:41,303 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2023-05-11 10:28:41,364 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-05-11 10:28:41,375 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2023-05-11 10:28:41,377 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2023-05-11 10:28:41,818 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (3.20.2)\u001b[0m\n",
      "\u001b[35mCollecting transformers==4.28.0\u001b[0m\n",
      "\u001b[35mDownloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 70.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting cpm_kernels\u001b[0m\n",
      "\u001b[35mDownloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 416.6/416.6 kB 70.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch>=1.10 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[35mCollecting gradio\u001b[0m\n",
      "\u001b[35mDownloading gradio-3.29.0-py3-none-any.whl (17.3 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 88.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting mdtex2html\u001b[0m\n",
      "\u001b[35mDownloading mdtex2html-1.2.0-py3-none-any.whl (13 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (0.1.97)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (0.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: datasets in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (2.9.0)\u001b[0m\n",
      "\u001b[35mCollecting huggingface\u001b[0m\n",
      "\u001b[35mDownloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\u001b[0m\n",
      "\u001b[35mCollecting jieba\u001b[0m\n",
      "\u001b[35mDownloading jieba-0.42.1.tar.gz (19.2 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/19.2 MB 95.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-05-11 10:28:41,555 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-05-11 10:28:41,617 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-11 10:28:41,627 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-05-11 10:28:41,629 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-05-11 10:28:41,999 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (3.20.2)\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.28.0\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 76.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting cpm_kernels\u001b[0m\n",
      "\u001b[34mDownloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 416.6/416.6 kB 75.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mCollecting gradio\u001b[0m\n",
      "\u001b[34mDownloading gradio-3.29.0-py3-none-any.whl (17.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 96.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting mdtex2html\u001b[0m\n",
      "\u001b[34mDownloading mdtex2html-1.2.0-py3-none-any.whl (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (0.1.97)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (0.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (2.9.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface\u001b[0m\n",
      "\u001b[34mDownloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting jieba\u001b[0m\n",
      "\u001b[34mDownloading jieba-0.42.1.tar.gz (19.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/19.2 MB 92.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting rouge_chinese\u001b[0m\n",
      "\u001b[35mDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\u001b[0m\n",
      "\u001b[35mCollecting nltk\u001b[0m\n",
      "\u001b[35mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 88.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting deepspeed==0.8.3\u001b[0m\n",
      "\u001b[35mDownloading deepspeed-0.8.3.tar.gz (765 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 765.4/765.4 kB 81.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting rouge_chinese\u001b[0m\n",
      "\u001b[34mDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\u001b[0m\n",
      "\u001b[34mCollecting nltk\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 94.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting deepspeed==0.8.3\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.8.3.tar.gz (765 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 765.4/765.4 kB 85.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (1.23.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (0.13.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (2022.10.31)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (0.12.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (2.28.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (5.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (4.64.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (3.9.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (23.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: hjson in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 15)) (3.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 15)) (1.11.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 15)) (5.9.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 15)) (9.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pydantic in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 15)) (1.10.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.10->-r requirements.txt (line 5)) (4.4.0)\u001b[0m\n",
      "\u001b[35mCollecting websockets>=10.0\u001b[0m\n",
      "\u001b[35mDownloading websockets-11.0.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.7/129.7 kB 34.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (3.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (2022.10.31)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (0.13.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 3)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 15)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 15)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 15)) (5.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 15)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 15)) (1.10.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.10->-r requirements.txt (line 5)) (4.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (3.6.3)\u001b[0m\n",
      "\u001b[34mCollecting fastapi\u001b[0m\n",
      "\u001b[34mDownloading fastapi-0.95.1-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.0/57.0 kB 18.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pydub\u001b[0m\n",
      "\u001b[34mDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\u001b[0m\n",
      "\u001b[34mCollecting orjson\u001b[0m\n",
      "\u001b[34mDownloading orjson-3.8.12-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 137.2/137.2 kB 35.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting mdit-py-plugins<=0.3.3\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.5/50.5 kB 18.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting httpx\u001b[0m\n",
      "\u001b[35mDownloading httpx-0.24.0-py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.3/75.3 kB 21.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting pydub\u001b[0m\n",
      "\u001b[35mDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (3.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pillow in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (9.4.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: markupsafe in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (2.1.2)\u001b[0m\n",
      "\u001b[35mCollecting aiofiles\u001b[0m\n",
      "\u001b[35mDownloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (3.8.4)\u001b[0m\n",
      "\u001b[35mCollecting orjson\u001b[0m\n",
      "\u001b[35mDownloading orjson-3.8.12-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 137.2/137.2 kB 33.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (1.5.3)\u001b[0m\n",
      "\u001b[35mCollecting altair>=4.2.0\u001b[0m\n",
      "\u001b[35mDownloading altair-5.0.0-py3-none-any.whl (477 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 477.4/477.4 kB 67.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting gradio-client>=0.2.1\u001b[0m\n",
      "\u001b[35mDownloading gradio_client-0.2.3-py3-none-any.whl (287 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 287.9/287.9 kB 53.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting python-multipart\u001b[0m\n",
      "\u001b[35mDownloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.7/45.7 kB 12.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting huggingface-hub<1.0,>=0.11.0\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 53.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting uvicorn>=0.14.0\u001b[0m\n",
      "\u001b[35mDownloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 14.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting ffmpy\u001b[0m\n",
      "\u001b[35mDownloading ffmpy-0.3.0.tar.gz (4.8 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markupsafe in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (2.1.2)\u001b[0m\n",
      "\u001b[34mCollecting httpx\u001b[0m\n",
      "\u001b[34mDownloading httpx-0.24.0-py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.3/75.3 kB 24.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting markdown-it-py[linkify]>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.5/84.5 kB 22.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (1.5.3)\u001b[0m\n",
      "\u001b[34mCollecting ffmpy\u001b[0m\n",
      "\u001b[34mDownloading ffmpy-0.3.0.tar.gz (4.8 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting gradio-client>=0.2.1\u001b[0m\n",
      "\u001b[34mDownloading gradio_client-0.2.3-py3-none-any.whl (287 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 287.9/287.9 kB 50.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting altair>=4.2.0\u001b[0m\n",
      "\u001b[34mDownloading altair-5.0.0-py3-none-any.whl (477 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 477.4/477.4 kB 81.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (3.1.2)\u001b[0m\n",
      "\u001b[34mCollecting semantic-version\u001b[0m\n",
      "\u001b[34mDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiofiles\u001b[0m\n",
      "\u001b[34mDownloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mCollecting python-multipart\u001b[0m\n",
      "\u001b[34mDownloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.7/45.7 kB 14.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (3.8.4)\u001b[0m\n",
      "\u001b[34mCollecting websockets>=10.0\u001b[0m\n",
      "\u001b[34mDownloading websockets-11.0.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.7/129.7 kB 35.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (9.4.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.11.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 44.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments>=2.12.0 in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (2.14.0)\u001b[0m\n",
      "\u001b[34mCollecting uvicorn>=0.14.0\u001b[0m\n",
      "\u001b[34mDownloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 19.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (3.6.3)\u001b[0m\n",
      "\u001b[35mCollecting mdit-py-plugins<=0.3.3\u001b[0m\n",
      "\u001b[35mDownloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.5/50.5 kB 16.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting semantic-version\u001b[0m\n",
      "\u001b[35mDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[35mCollecting markdown-it-py[linkify]>=2.0.0\u001b[0m\n",
      "\u001b[35mDownloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.5/84.5 kB 22.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting fastapi\u001b[0m\n",
      "\u001b[35mDownloading fastapi-0.95.1-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.0/57.0 kB 17.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pygments>=2.12.0 in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 6)) (2.14.0)\u001b[0m\n",
      "\u001b[35mCollecting latex2mathml\u001b[0m\n",
      "\u001b[35mDownloading latex2mathml-3.75.5-py3-none-any.whl (73 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.3/73.3 kB 22.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: markdown in /opt/conda/lib/python3.9/site-packages (from mdtex2html->-r requirements.txt (line 7)) (3.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 10)) (0.3.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 10)) (0.70.14)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 10)) (0.18.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 10)) (3.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 10)) (2023.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 10)) (11.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from rouge_chinese->-r requirements.txt (line 13)) (1.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 14)) (8.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 14)) (1.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: toolz in /opt/conda/lib/python3.9/site-packages (from altair>=4.2.0->gradio->-r requirements.txt (line 6)) (0.12.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.9/site-packages (from altair>=4.2.0->gradio->-r requirements.txt (line 6)) (4.17.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (22.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (4.0.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (1.3.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (2.1.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (6.0.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (1.3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (1.8.2)\u001b[0m\n",
      "\u001b[35mCollecting mdurl~=0.1\u001b[0m\n",
      "\u001b[35mDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\u001b[0m\n",
      "\u001b[35mCollecting linkify-it-py<3,>=1\u001b[0m\n",
      "\u001b[35mDownloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->gradio->-r requirements.txt (line 6)) (2.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->gradio->-r requirements.txt (line 6)) (2022.7.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.0->-r requirements.txt (line 3)) (1.26.14)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.0->-r requirements.txt (line 3)) (3.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.0->-r requirements.txt (line 3)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mCollecting latex2mathml\u001b[0m\n",
      "\u001b[34mDownloading latex2mathml-3.75.5-py3-none-any.whl (73 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.3/73.3 kB 24.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown in /opt/conda/lib/python3.9/site-packages (from mdtex2html->-r requirements.txt (line 7)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 10)) (2023.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 10)) (11.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 10)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 10)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 10)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 10)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from rouge_chinese->-r requirements.txt (line 13)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 14)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 14)) (8.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.9/site-packages (from altair>=4.2.0->gradio->-r requirements.txt (line 6)) (4.17.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: toolz in /opt/conda/lib/python3.9/site-packages (from altair>=4.2.0->gradio->-r requirements.txt (line 6)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (1.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->-r requirements.txt (line 6)) (1.3.1)\u001b[0m\n",
      "\u001b[34mCollecting mdurl~=0.1\u001b[0m\n",
      "\u001b[34mDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting linkify-it-py<3,>=1\u001b[0m\n",
      "\u001b[34mDownloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->gradio->-r requirements.txt (line 6)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->gradio->-r requirements.txt (line 6)) (2022.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.0->-r requirements.txt (line 3)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.0->-r requirements.txt (line 3)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.0->-r requirements.txt (line 3)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mCollecting h11>=0.8\u001b[0m\n",
      "\u001b[34mDownloading h11-0.14.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 19.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting starlette<0.27.0,>=0.26.1\u001b[0m\n",
      "\u001b[34mDownloading starlette-0.26.1-py3-none-any.whl (66 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.9/66.9 kB 21.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting httpcore<0.18.0,>=0.15.0\u001b[0m\n",
      "\u001b[34mDownloading httpcore-0.17.0-py3-none-any.whl (70 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 70.6/70.6 kB 23.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting sniffio\u001b[0m\n",
      "\u001b[34mDownloading sniffio-1.3.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown->mdtex2html->-r requirements.txt (line 7)) (4.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 6)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 6)) (1.0.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 6)) (1.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 6)) (0.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 6)) (4.38.0)\u001b[0m\n",
      "\u001b[35mCollecting h11>=0.8\u001b[0m\n",
      "\u001b[35mDownloading h11-0.14.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 17.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting starlette<0.27.0,>=0.26.1\u001b[0m\n",
      "\u001b[35mDownloading starlette-0.26.1-py3-none-any.whl (66 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.9/66.9 kB 19.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting sniffio\u001b[0m\n",
      "\u001b[35mDownloading sniffio-1.3.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[35mCollecting httpcore<0.18.0,>=0.15.0\u001b[0m\n",
      "\u001b[35mDownloading httpcore-0.17.0-py3-none-any.whl (70 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 70.6/70.6 kB 20.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown->mdtex2html->-r requirements.txt (line 7)) (4.13.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 6)) (1.0.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 6)) (3.0.9)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 6)) (0.11.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 6)) (1.4.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio->-r requirements.txt (line 6)) (4.38.0)\u001b[0m\n",
      "\u001b[35mCollecting anyio<5.0,>=3.0\u001b[0m\n",
      "\u001b[35mDownloading anyio-3.6.2-py3-none-any.whl (80 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.6/80.6 kB 24.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown->mdtex2html->-r requirements.txt (line 7)) (3.13.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio->-r requirements.txt (line 6)) (0.19.3)\u001b[0m\n",
      "\u001b[35mCollecting uc-micro-py\u001b[0m\n",
      "\u001b[35mDownloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: deepspeed, jieba, ffmpy\u001b[0m\n",
      "\u001b[35mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mCollecting anyio<5.0,>=3.0\u001b[0m\n",
      "\u001b[34mDownloading anyio-3.6.2-py3-none-any.whl (80 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.6/80.6 kB 28.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown->mdtex2html->-r requirements.txt (line 7)) (3.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio->-r requirements.txt (line 6)) (0.19.3)\u001b[0m\n",
      "\u001b[34mCollecting uc-micro-py\u001b[0m\n",
      "\u001b[34mDownloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed, jieba, ffmpy\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for deepspeed: filename=deepspeed-0.8.3-py3-none-any.whl size=776406 sha256=d8e484d4f88cb6dbbb912b5260b66af946ad9399dfb9c5785339920f61548fc0\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/f8/ea/8f/0768328ba436ed66f602d8d3b809624448c9eb627434176d04\u001b[0m\n",
      "\u001b[35mBuilding wheel for jieba (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.8.3-py3-none-any.whl size=776408 sha256=82cac181ed954dc054f3f9a14d1a7ccd2ee6872938ea1827b8366d568da9e602\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/f8/ea/8f/0768328ba436ed66f602d8d3b809624448c9eb627434176d04\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=bcf1cca7b470991113f084438f10e2899853a499a6bcb25e33374b76abd006a8\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/7d/74/cf/08c94db4b784e2c1ef675a600b7b5b281fd25240dcb954ee7e\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for jieba (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=bcf1cca7b470991113f084438f10e2899853a499a6bcb25e33374b76abd006a8\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/7d/74/cf/08c94db4b784e2c1ef675a600b7b5b281fd25240dcb954ee7e\u001b[0m\n",
      "\u001b[35mBuilding wheel for ffmpy (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for ffmpy (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4693 sha256=6fbc6020fa962ffcac1f32d4686a717cba81ef6e5b60271265c033eaa28a71c8\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/91/e2/96/f676aa08bfd789328c6576cd0f1fde4a3d686703bb0c247697\u001b[0m\n",
      "\u001b[35mSuccessfully built deepspeed jieba ffmpy\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4693 sha256=6fbc6020fa962ffcac1f32d4686a717cba81ef6e5b60271265c033eaa28a71c8\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/91/e2/96/f676aa08bfd789328c6576cd0f1fde4a3d686703bb0c247697\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed jieba ffmpy\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pydub, jieba, huggingface, ffmpy, cpm_kernels, websockets, uc-micro-py, sniffio, semantic-version, rouge_chinese, python-multipart, orjson, nltk, mdurl, latex2mathml, h11, aiofiles, uvicorn, markdown-it-py, linkify-it-py, huggingface-hub, deepspeed, anyio, transformers, starlette, mdtex2html, mdit-py-plugins, httpcore, altair, httpx, fastapi, gradio-client, gradio\u001b[0m\n",
      "\u001b[35mInstalling collected packages: pydub, jieba, huggingface, ffmpy, cpm_kernels, websockets, uc-micro-py, sniffio, semantic-version, rouge_chinese, python-multipart, orjson, nltk, mdurl, latex2mathml, h11, aiofiles, uvicorn, markdown-it-py, linkify-it-py, huggingface-hub, deepspeed, anyio, transformers, starlette, mdtex2html, mdit-py-plugins, httpcore, altair, httpx, fastapi, gradio-client, gradio\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.12.0\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.12.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.12.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+06f2048\u001b[0m\n",
      "\u001b[34mUninstalling deepspeed-0.6.1+06f2048:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled deepspeed-0.6.1+06f2048\u001b[0m\n",
      "\u001b[35mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[35mFound existing installation: huggingface-hub 0.12.0\u001b[0m\n",
      "\u001b[35mUninstalling huggingface-hub-0.12.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled huggingface-hub-0.12.0\u001b[0m\n",
      "\u001b[35mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[35mFound existing installation: deepspeed 0.6.1+06f2048\u001b[0m\n",
      "\u001b[35mUninstalling deepspeed-0.6.1+06f2048:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled deepspeed-0.6.1+06f2048\u001b[0m\n",
      "\u001b[35mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[35mFound existing installation: transformers 4.26.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.26.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.26.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.26.0\u001b[0m\n",
      "\u001b[35mUninstalling transformers-4.26.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled transformers-4.26.0\u001b[0m\n",
      "\u001b[35mSuccessfully installed aiofiles-23.1.0 altair-5.0.0 anyio-3.6.2 cpm_kernels-1.0.11 deepspeed-0.8.3 fastapi-0.95.1 ffmpy-0.3.0 gradio-3.29.0 gradio-client-0.2.3 h11-0.14.0 httpcore-0.17.0 httpx-0.24.0 huggingface-0.0.1 huggingface-hub-0.14.1 jieba-0.42.1 latex2mathml-3.75.5 linkify-it-py-2.0.2 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 mdtex2html-1.2.0 mdurl-0.1.2 nltk-3.8.1 orjson-3.8.12 pydub-0.25.1 python-multipart-0.0.6 rouge_chinese-1.0.3 semantic-version-2.10.0 sniffio-1.3.0 starlette-0.26.1 transformers-4.28.0 uc-micro-py-1.0.2 uvicorn-0.22.0 websockets-11.0.3\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m[notice] A new release of pip is available: 23.0 -> 23.1.2\u001b[0m\n",
      "\u001b[35m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiofiles-23.1.0 altair-5.0.0 anyio-3.6.2 cpm_kernels-1.0.11 deepspeed-0.8.3 fastapi-0.95.1 ffmpy-0.3.0 gradio-3.29.0 gradio-client-0.2.3 h11-0.14.0 httpcore-0.17.0 httpx-0.24.0 huggingface-0.0.1 huggingface-hub-0.14.1 jieba-0.42.1 latex2mathml-3.75.5 linkify-it-py-2.0.2 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 mdtex2html-1.2.0 mdurl-0.1.2 nltk-3.8.1 orjson-3.8.12 pydub-0.25.1 python-multipart-0.0.6 rouge_chinese-1.0.3 semantic-version-2.10.0 sniffio-1.3.0 starlette-0.26.1 transformers-4.28.0 uc-micro-py-1.0.2 uvicorn-0.22.0 websockets-11.0.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.1.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-05-11 10:29:08,454 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-05-11 10:29:08,454 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-05-11 10:29:08,527 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-11 10:29:08,605 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-11 10:29:08,682 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-11 10:29:08,695 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"AdvertiseGen\": \"/opt/ml/input/data/AdvertiseGen\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.48xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"AdvertiseGen\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.48xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-chatglm-deepspeed-2023-05-1-2023-05-11-10-22-23-381\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-deepspeed-2023-05-1-2023-05-11-10-22-23-381/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"start\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 192,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.48xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.48xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"start.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=start.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"AdvertiseGen\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"AdvertiseGen\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.48xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=start\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=192\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-deepspeed-2023-05-1-2023-05-11-10-22-23-381/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"AdvertiseGen\":\"/opt/ml/input/data/AdvertiseGen\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-1\"],\"current_instance_type\":\"ml.g5.48xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"AdvertiseGen\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-chatglm-deepspeed-2023-05-1-2023-05-11-10-22-23-381\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-deepspeed-2023-05-1-2023-05-11-10-22-23-381/source/sourcedir.tar.gz\",\"module_name\":\"start\",\"network_interface_name\":\"eth0\",\"num_cpus\":192,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"start.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_ADVERTISEGEN=/opt/ml/input/data/AdvertiseGen\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 start.py\u001b[0m\n",
      "\u001b[35m2023-05-11 10:29:08,779 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2023-05-11 10:29:08,779 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2023-05-11 10:29:08,844 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-05-11 10:29:08,919 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-05-11 10:29:08,994 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-05-11 10:29:09,006 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"AdvertiseGen\": \"/opt/ml/input/data/AdvertiseGen\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.48xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"AdvertiseGen\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.48xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-chatglm-deepspeed-2023-05-1-2023-05-11-10-22-23-381\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-deepspeed-2023-05-1-2023-05-11-10-22-23-381/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"start\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 192,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.g5.48xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.48xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"start.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=start.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"AdvertiseGen\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"AdvertiseGen\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.g5.48xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-1\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=start\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=192\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-deepspeed-2023-05-1-2023-05-11-10-22-23-381/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"AdvertiseGen\":\"/opt/ml/input/data/AdvertiseGen\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-1\"],\"current_instance_type\":\"ml.g5.48xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"AdvertiseGen\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-chatglm-deepspeed-2023-05-1-2023-05-11-10-22-23-381\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-687912291502/huggingface-chatglm-deepspeed-2023-05-1-2023-05-11-10-22-23-381/source/sourcedir.tar.gz\",\"module_name\":\"start\",\"network_interface_name\":\"eth0\",\"num_cpus\":192,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"start.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_ADVERTISEGEN=/opt/ml/input/data/AdvertiseGen\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.9 start.py\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:29:13.250: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m2023-05-11 10:29:13,254 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[35m2023-05-11 10:29:13,274 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[35m10.0.136.223 slots=8\u001b[0m\n",
      "\u001b[35m10.0.141.69 slots=8\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/.gitattributes /tmp/chatglm/.gitattributes\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/LICENSE /tmp/chatglm/LICENSE\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/modeling_chatglm.py /tmp/chatglm/modeling_chatglm.py\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/config.json /tmp/chatglm/config.json\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/MODEL_LICENSE /tmp/chatglm/MODEL_LICENSE\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/tokenization_chatglm.py /tmp/chatglm/tokenization_chatglm.py\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/README.md /tmp/chatglm/README.md\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/configuration_chatglm.py /tmp/chatglm/configuration_chatglm.py\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/pytorch_model.bin.index.json /tmp/chatglm/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/tokenizer_config.json /tmp/chatglm/tokenizer_config.json\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/test_modeling_chatglm.py /tmp/chatglm/test_modeling_chatglm.py\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/ice_text.model /tmp/chatglm/ice_text.model\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/quantization.py /tmp/chatglm/quantization.py\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:29:12.996: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-05-11 10:29:13,000 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-05-11 10:29:13,020 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m10.0.136.223 slots=8\u001b[0m\n",
      "\u001b[34m10.0.141.69 slots=8\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/LICENSE /tmp/chatglm/LICENSE\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/.gitattributes /tmp/chatglm/.gitattributes\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/pytorch_model.bin.index.json /tmp/chatglm/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/tokenization_chatglm.py /tmp/chatglm/tokenization_chatglm.py\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/config.json /tmp/chatglm/config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/configuration_chatglm.py /tmp/chatglm/configuration_chatglm.py\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/quantization.py /tmp/chatglm/quantization.py\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/test_modeling_chatglm.py /tmp/chatglm/test_modeling_chatglm.py\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/modeling_chatglm.py /tmp/chatglm/modeling_chatglm.py\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/README.md /tmp/chatglm/README.md\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/MODEL_LICENSE /tmp/chatglm/MODEL_LICENSE\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/tokenizer_config.json /tmp/chatglm/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/ice_text.model /tmp/chatglm/ice_text.model\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/pytorch_model-00007-of-00008.bin /tmp/chatglm/pytorch_model-00007-of-00008.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/pytorch_model-00007-of-00008.bin /tmp/chatglm/pytorch_model-00007-of-00008.bin\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/pytorch_model-00008-of-00008.bin /tmp/chatglm/pytorch_model-00008-of-00008.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/pytorch_model-00008-of-00008.bin /tmp/chatglm/pytorch_model-00008-of-00008.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/pytorch_model-00004-of-00008.bin /tmp/chatglm/pytorch_model-00004-of-00008.bin\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/pytorch_model-00004-of-00008.bin /tmp/chatglm/pytorch_model-00004-of-00008.bin\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/pytorch_model-00001-of-00008.bin /tmp/chatglm/pytorch_model-00001-of-00008.bin\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/pytorch_model-00006-of-00008.bin /tmp/chatglm/pytorch_model-00006-of-00008.bin\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/pytorch_model-00003-of-00008.bin /tmp/chatglm/pytorch_model-00003-of-00008.bin\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/pytorch_model-00005-of-00008.bin /tmp/chatglm/pytorch_model-00005-of-00008.bin\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/pytorch_model-00002-of-00008.bin /tmp/chatglm/pytorch_model-00002-of-00008.bin\u001b[0m\n",
      "\u001b[35mpython -m torch.distributed.launch --nproc_per_node 8 --nnodes 2 --node_rank 1 --master_addr algo-1 --master_port 23456 /opt/ml/code//main_tuning.py --deepspeed /opt/ml/code//deepspeed.json --do_train --train_mutipl --train_file /opt/ml/input/data/AdvertiseGen/train.json --test_file /opt/ml/input/data/AdvertiseGen/dev.json --prompt_column content --response_column summary --overwrite_cache --model_name_or_path THUDM/chatglm-6b --output_dir /tmp/model/adgen-chatglm-6b-ft --model_output_s3_path s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/ --overwrite_output_dir --max_source_length 64 --max_target_length 64 --per_device_train_batch_size 1 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --predict_with_generate --max_steps 50 --logging_steps 10 --save_steps 50 --learning_rate 1e-4 --fp16\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/pytorch_model-00001-of-00008.bin /tmp/chatglm/pytorch_model-00001-of-00008.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/pytorch_model-00006-of-00008.bin /tmp/chatglm/pytorch_model-00006-of-00008.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/pytorch_model-00003-of-00008.bin /tmp/chatglm/pytorch_model-00003-of-00008.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/pytorch_model-00005-of-00008.bin /tmp/chatglm/pytorch_model-00005-of-00008.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/model/chatglm/orignal/pytorch_model-00002-of-00008.bin /tmp/chatglm/pytorch_model-00002-of-00008.bin\u001b[0m\n",
      "\u001b[34mpython -m torch.distributed.launch --nproc_per_node 8 --nnodes 2 --node_rank 0 --master_addr algo-1 --master_port 23456 /opt/ml/code//main_tuning.py --deepspeed /opt/ml/code//deepspeed.json --do_train --train_mutipl --train_file /opt/ml/input/data/AdvertiseGen/train.json --test_file /opt/ml/input/data/AdvertiseGen/dev.json --prompt_column content --response_column summary --overwrite_cache --model_name_or_path THUDM/chatglm-6b --output_dir /tmp/model/adgen-chatglm-6b-ft --model_output_s3_path s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/ --overwrite_output_dir --max_source_length 64 --max_target_length 64 --per_device_train_batch_size 1 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --predict_with_generate --max_steps 50 --logging_steps 10 --save_steps 50 --learning_rate 1e-4 --fp16\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated\u001b[0m\n",
      "\u001b[34mand will be removed in future. Use torchrun.\u001b[0m\n",
      "\u001b[34mNote that --use_env is set by default in torchrun.\u001b[0m\n",
      "\u001b[34mIf your script expects `--local_rank` argument to be set, please\u001b[0m\n",
      "\u001b[34mchange it to read from `os.environ['LOCAL_RANK']` instead. See \u001b[0m\n",
      "\u001b[34mhttps://pytorch.org/docs/stable/distributed.html#launch-utility for \u001b[0m\n",
      "\u001b[34mfurther instructions\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.9/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated\u001b[0m\n",
      "\u001b[35mand will be removed in future. Use torchrun.\u001b[0m\n",
      "\u001b[35mNote that --use_env is set by default in torchrun.\u001b[0m\n",
      "\u001b[35mIf your script expects `--local_rank` argument to be set, please\u001b[0m\n",
      "\u001b[35mchange it to read from `os.environ['LOCAL_RANK']` instead. See \u001b[0m\n",
      "\u001b[35mhttps://pytorch.org/docs/stable/distributed.html#launch-utility for \u001b[0m\n",
      "\u001b[35mfurther instructions\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[35m*****************************************\u001b[0m\n",
      "\u001b[35mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[35m*****************************************\u001b[0m\n",
      "\u001b[35m05/11/2023 10:29:25 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[35m05/11/2023 10:29:25 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[35m05/11/2023 10:29:25 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[35m05/11/2023 10:29:25 - WARNING - __main__ - Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[35m05/11/2023 10:29:25 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[35m05/11/2023 10:29:25 - WARNING - __main__ - Process rank: 7, device: cuda:7, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[35m05/11/2023 10:29:25 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\u001b[0m\n",
      "\u001b[35m_n_gpu=1,\u001b[0m\n",
      "\u001b[35madafactor=False,\u001b[0m\n",
      "\u001b[35madam_beta1=0.9,\u001b[0m\n",
      "\u001b[35madam_beta2=0.999,\u001b[0m\n",
      "\u001b[35madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[35mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[35mbf16=False,\u001b[0m\n",
      "\u001b[35mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[35mdata_seed=None,\u001b[0m\n",
      "\u001b[35mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[35mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[35mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[35mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[35mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[35mddp_timeout=1800,\u001b[0m\n",
      "\u001b[35mdebug=[],\u001b[0m\n",
      "\u001b[35mdeepspeed=/opt/ml/code//deepspeed.json,\u001b[0m\n",
      "\u001b[35mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[35mdo_eval=False,\u001b[0m\n",
      "\u001b[35mdo_predict=False,\u001b[0m\n",
      "\u001b[35mdo_train=True,\u001b[0m\n",
      "\u001b[35meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[35meval_delay=0,\u001b[0m\n",
      "\u001b[35meval_steps=None,\u001b[0m\n",
      "\u001b[35mevaluation_strategy=no,\u001b[0m\n",
      "\u001b[35mfp16=True,\u001b[0m\n",
      "\u001b[35mfp16_backend=auto,\u001b[0m\n",
      "\u001b[35mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[35mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[35mfsdp=[],\u001b[0m\n",
      "\u001b[35mfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[35mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[35mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[35mfull_determinism=False,\u001b[0m\n",
      "\u001b[35mgeneration_config=None,\u001b[0m\n",
      "\u001b[35mgeneration_max_length=None,\u001b[0m\n",
      "\u001b[35mgeneration_num_beams=None,\u001b[0m\n",
      "\u001b[35mgradient_accumulation_steps=1,\u001b[0m\n",
      "\u001b[35mgradient_checkpointing=False,\u001b[0m\n",
      "\u001b[35mgreater_is_better=None,\u001b[0m\n",
      "\u001b[35mgroup_by_length=False,\u001b[0m\n",
      "\u001b[35mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[35mhub_model_id=None,\u001b[0m\n",
      "\u001b[35mhub_private_repo=False,\u001b[0m\n",
      "\u001b[35mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[35mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[35mignore_data_skip=False,\u001b[0m\n",
      "\u001b[35minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[35mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[35mlabel_names=None,\u001b[0m\n",
      "\u001b[35mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[35mlearning_rate=0.0001,\u001b[0m\n",
      "\u001b[35mlength_column_name=length,\u001b[0m\n",
      "\u001b[35mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[35mlocal_rank=0,\u001b[0m\n",
      "\u001b[35mlog_level=passive,\u001b[0m\n",
      "\u001b[35mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[35mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[35mlogging_dir=/tmp/model/adgen-chatglm-6b-ft/runs/May11_10-29-25_algo-2,\u001b[0m\n",
      "\u001b[35mlogging_first_step=False,\u001b[0m\n",
      "\u001b[35mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[35mlogging_steps=10,\u001b[0m\n",
      "\u001b[35mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[35mlr_scheduler_type=linear,\u001b[0m\n",
      "\u001b[35mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[35mmax_steps=50,\u001b[0m\n",
      "\u001b[35mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[35mmp_parameters=,\u001b[0m\n",
      "\u001b[35mno_cuda=False,\u001b[0m\n",
      "\u001b[35mnum_train_epochs=3.0,\u001b[0m\n",
      "\u001b[35moptim=adamw_hf,\u001b[0m\n",
      "\u001b[35moptim_args=None,\u001b[0m\n",
      "\u001b[35moutput_dir=/tmp/model/adgen-chatglm-6b-ft,\u001b[0m\n",
      "\u001b[35moverwrite_output_dir=True,\u001b[0m\n",
      "\u001b[35mpast_index=-1,\u001b[0m\n",
      "\u001b[35mper_device_eval_batch_size=4,\u001b[0m\n",
      "\u001b[35mper_device_train_batch_size=1,\u001b[0m\n",
      "\u001b[35mpredict_with_generate=True,\u001b[0m\n",
      "\u001b[35mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[35mpush_to_hub=False,\u001b[0m\n",
      "\u001b[35mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[35mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[35mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[35mray_scope=last,\u001b[0m\n",
      "\u001b[35mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[35mreport_to=[],\u001b[0m\n",
      "\u001b[35mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[35mrun_name=/tmp/model/adgen-chatglm-6b-ft,\u001b[0m\n",
      "\u001b[35msave_on_each_node=False,\u001b[0m\n",
      "\u001b[35msave_safetensors=False,\u001b[0m\n",
      "\u001b[35msave_steps=50,\u001b[0m\n",
      "\u001b[35msave_strategy=steps,\u001b[0m\n",
      "\u001b[35msave_total_limit=None,\u001b[0m\n",
      "\u001b[35mseed=42,\u001b[0m\n",
      "\u001b[35msharded_ddp=[],\u001b[0m\n",
      "\u001b[35mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[35msortish_sampler=False,\u001b[0m\n",
      "\u001b[35mtf32=None,\u001b[0m\n",
      "\u001b[35mtorch_compile=False,\u001b[0m\n",
      "\u001b[35mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[35mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[35mtorchdynamo=None,\u001b[0m\n",
      "\u001b[35mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[35mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[35muse_ipex=False,\u001b[0m\n",
      "\u001b[35muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[35muse_mps_device=False,\u001b[0m\n",
      "\u001b[35mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[35mwarmup_steps=0,\u001b[0m\n",
      "\u001b[35mweight_decay=0.0,\u001b[0m\n",
      "\u001b[35mxpu_backend=None,\u001b[0m\n",
      "\u001b[35m)\u001b[0m\n",
      "\u001b[35m05/11/2023 10:29:25 - WARNING - __main__ - Process rank: 6, device: cuda:6, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[35m05/11/2023 10:29:25 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:29:25,076] [INFO] [comm.py:652:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m05/11/2023 10:29:25 - WARNING - __main__ - Process rank: 6, device: cuda:6, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/11/2023 10:29:25 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/11/2023 10:29:25 - WARNING - __main__ - Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/11/2023 10:29:25 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/11/2023 10:29:25 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=False,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=/opt/ml/code//deepspeed.json,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdo_eval=False,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=no,\u001b[0m\n",
      "\u001b[34mfp16=True,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgeneration_config=None,\u001b[0m\n",
      "\u001b[34mgeneration_max_length=None,\u001b[0m\n",
      "\u001b[34mgeneration_num_beams=None,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=1,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=False,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=0.0001,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=0,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/tmp/model/adgen-chatglm-6b-ft/runs/May11_10-29-25_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=10,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=linear,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=50,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=3.0,\u001b[0m\n",
      "\u001b[34moptim=adamw_hf,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moutput_dir=/tmp/model/adgen-chatglm-6b-ft,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=True,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=4,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=1,\u001b[0m\n",
      "\u001b[34mpredict_with_generate=True,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/tmp/model/adgen-chatglm-6b-ft,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_safetensors=False,\u001b[0m\n",
      "\u001b[34msave_steps=50,\u001b[0m\n",
      "\u001b[34msave_strategy=steps,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=42,\u001b[0m\n",
      "\u001b[34msharded_ddp=[],\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34msortish_sampler=False,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,\u001b[0m\n",
      "\u001b[34mxpu_backend=None,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m05/11/2023 10:29:25 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/11/2023 10:29:25 - WARNING - __main__ - Process rank: 7, device: cuda:7, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/11/2023 10:29:25 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/11/2023 10:29:25 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/11/2023 10:29:25 - WARNING - datasets.builder - Using custom data configuration default-c73a742fdca54a5f\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-c73a742fdca54a5f/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 2/2 [00:00<00:00, 13842.59it/s]\u001b[0m\n",
      "\u001b[34m05/11/2023 10:29:25 - WARNING - datasets.builder - Using custom data configuration default-c73a742fdca54a5f\u001b[0m\n",
      "\u001b[34m05/11/2023 10:29:25 - WARNING - datasets.builder - Using custom data configuration default-c73a742fdca54a5f\u001b[0m\n",
      "\u001b[34m05/11/2023 10:29:25 - WARNING - datasets.builder - Using custom data configuration default-c73a742fdca54a5f\u001b[0m\n",
      "\u001b[34m05/11/2023 10:29:25 - WARNING - datasets.builder - Using custom data configuration default-c73a742fdca54a5f\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m05/11/2023 10:29:25 - WARNING - datasets.builder - Using custom data configuration default-c73a742fdca54a5f\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 2/2 [00:00<00:00, 2191.95it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34m05/11/2023 10:29:25 - WARNING - datasets.builder - Using custom data configuration default-c73a742fdca54a5f\u001b[0m\n",
      "\u001b[34m05/11/2023 10:29:25 - WARNING - datasets.builder - Using custom data configuration default-c73a742fdca54a5f\u001b[0m\n",
      "\u001b[35m05/11/2023 10:29:25 - WARNING - datasets.builder - Using custom data configuration default-e273886434774961\u001b[0m\n",
      "\u001b[35mDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-e273886434774961/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\u001b[0m\n",
      "\u001b[35mDownloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mDownloading data files: 100%|██████████| 2/2 [00:00<00:00, 13273.11it/s]\u001b[0m\n",
      "\u001b[35m05/11/2023 10:29:25 - WARNING - datasets.builder - Using custom data configuration default-e273886434774961\u001b[0m\n",
      "\u001b[35m05/11/2023 10:29:25 - WARNING - datasets.builder - Using custom data configuration default-e273886434774961\u001b[0m\n",
      "\u001b[35m05/11/2023 10:29:25 - WARNING - datasets.builder - Using custom data configuration default-e273886434774961\u001b[0m\n",
      "\u001b[35m05/11/2023 10:29:25 - WARNING - datasets.builder - Using custom data configuration default-e273886434774961\u001b[0m\n",
      "\u001b[35m05/11/2023 10:29:25 - WARNING - datasets.builder - Using custom data configuration default-e273886434774961\u001b[0m\n",
      "\u001b[35m05/11/2023 10:29:25 - WARNING - datasets.builder - Using custom data configuration default-e273886434774961\u001b[0m\n",
      "\u001b[35m05/11/2023 10:29:25 - WARNING - datasets.builder - Using custom data configuration default-e273886434774961\u001b[0m\n",
      "\u001b[35mExtracting data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mExtracting data files: 100%|██████████| 2/2 [00:00<00:00, 1955.84it/s]\u001b[0m\n",
      "\u001b[35mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[35mGenerating train split: 44644 examples [00:00, 261789.96 examples/s]\u001b[0m\n",
      "\u001b[35mGenerating train split: 89430 examples [00:00, 283354.26 examples/s]\u001b[0m\n",
      "\u001b[35mGenerating test split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[35mDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-e273886434774961/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[35m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:00<00:00, 609.50it/s]\u001b[0m\n",
      "\u001b[35m05/11/2023 10:29:26 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-e273886434774961/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[35m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m05/11/2023 10:29:26 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-e273886434774961/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:00<00:00, 559.91it/s]\u001b[0m\n",
      "\u001b[35m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m05/11/2023 10:29:26 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-e273886434774961/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[35m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:00<00:00, 550.47it/s]\u001b[0m\n",
      "\u001b[35m05/11/2023 10:29:26 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-e273886434774961/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:00<00:00, 590.79it/s]\u001b[0m\n",
      "\u001b[35m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:00<00:00, 632.48it/s]\u001b[0m\n",
      "\u001b[35m05/11/2023 10:29:26 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-e273886434774961/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[35m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:00<00:00, 601.03it/s]\u001b[0m\n",
      "\u001b[35m05/11/2023 10:29:26 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-e273886434774961/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[35m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:00<00:00, 553.74it/s]\u001b[0m\n",
      "\u001b[35m05/11/2023 10:29:26 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-e273886434774961/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[35m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:00<00:00, 561.04it/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)lve/main/config.json:   0%|          | 0.00/773 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)lve/main/config.json: 100%|██████████| 773/773 [00:00<00:00, 238kB/s]\u001b[0m\n",
      "\u001b[35m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,582 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,582 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,606 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,606 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:668] 2023-05-11 10:29:26,606 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/config.json\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:668] 2023-05-11 10:29:26,606 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/config.json\u001b[0m\n",
      "\u001b[35m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,607 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,607 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,610 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,610 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,613 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,613 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,614 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,614 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mGenerating train split: 44644 examples [00:00, 262121.25 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 89430 examples [00:00, 283425.47 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating test split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-c73a742fdca54a5f/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 651.44it/s]\u001b[0m\n",
      "\u001b[34m05/11/2023 10:29:26 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-c73a742fdca54a5f/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 505.73it/s]\u001b[0m\n",
      "\u001b[34m05/11/2023 10:29:26 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-c73a742fdca54a5f/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m05/11/2023 10:29:26 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-c73a742fdca54a5f/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 532.10it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 506.56it/s]\u001b[0m\n",
      "\u001b[34m05/11/2023 10:29:26 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-c73a742fdca54a5f/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 640.84it/s]\u001b[0m\n",
      "\u001b[34m05/11/2023 10:29:26 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-c73a742fdca54a5f/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 618.86it/s]\u001b[0m\n",
      "\u001b[34m05/11/2023 10:29:26 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-c73a742fdca54a5f/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m05/11/2023 10:29:26 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-c73a742fdca54a5f/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 622.44it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 513.88it/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/773 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 773/773 [00:00<00:00, 239kB/s]\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,780 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,780 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,789 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,789 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,800 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,800 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,807 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,807 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,814 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,814 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:668] 2023-05-11 10:29:26,820 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/config.json\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,820 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,820 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:668] 2023-05-11 10:29:26,820 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/config.json\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,820 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,820 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,824 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,824 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mDownloading (…)iguration_chatglm.py:   0%|          | 0.00/4.28k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)iguration_chatglm.py: 100%|██████████| 4.28k/4.28k [00:00<00:00, 1.78MB/s]\u001b[0m\n",
      "\u001b[35m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,624 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,624 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,680 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|configuration_auto.py:925] 2023-05-11 10:29:26,680 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35mDownloading (…)iguration_chatglm.py:   0%|          | 0.00/4.28k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)iguration_chatglm.py: 100%|██████████| 4.28k/4.28k [00:00<00:00, 5.50MB/s]\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:668] 2023-05-11 10:29:26,889 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/config.json\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:668] 2023-05-11 10:29:26,889 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/config.json\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:720] 2023-05-11 10:29:26,890 >> Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm-6b\",\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\"\n",
      "  },\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"gmask_token_id\": 130001,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"inner_hidden_size\": 16384,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"mask_token_id\": 130000,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_encoding_2d\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.28.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 130528\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:720] 2023-05-11 10:29:26,890 >> Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm-6b\",\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\"\n",
      "  },\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"gmask_token_id\": 130001,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"inner_hidden_size\": 16384,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"mask_token_id\": 130000,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_encoding_2d\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.28.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 130528\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mDownloading (…)okenizer_config.json:   0%|          | 0.00/441 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)okenizer_config.json: 100%|██████████| 441/441 [00:00<00:00, 148kB/s]\u001b[0m\n",
      "\u001b[35m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,046 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,046 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,046 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,046 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,062 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,062 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,076 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,076 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,077 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,077 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,082 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,082 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,087 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,087 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,089 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,089 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35mDownloading (…)enization_chatglm.py:   0%|          | 0.00/17.0k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)enization_chatglm.py: 100%|██████████| 17.0k/17.0k [00:00<00:00, 7.39MB/s]\u001b[0m\n",
      "\u001b[35mDownloading ice_text.model:   0%|          | 0.00/2.71M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading ice_text.model: 100%|██████████| 2.71M/2.71M [00:00<00:00, 69.2MB/s]\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:1809] 2023-05-11 10:29:27,606 >> loading file ice_text.model from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/ice_text.model\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:1809] 2023-05-11 10:29:27,606 >> loading file ice_text.model from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/ice_text.model\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:1809] 2023-05-11 10:29:27,606 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:1809] 2023-05-11 10:29:27,606 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:1809] 2023-05-11 10:29:27,607 >> loading file special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:1809] 2023-05-11 10:29:27,607 >> loading file special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:1809] 2023-05-11 10:29:27,607 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/tokenizer_config.json\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:1809] 2023-05-11 10:29:27,607 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:668] 2023-05-11 10:29:27,084 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:668] 2023-05-11 10:29:27,084 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:720] 2023-05-11 10:29:27,085 >> Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm-6b\",\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\"\n",
      "  },\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"gmask_token_id\": 130001,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"inner_hidden_size\": 16384,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"mask_token_id\": 130000,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_encoding_2d\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.28.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 130528\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:720] 2023-05-11 10:29:27,085 >> Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm-6b\",\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\"\n",
      "  },\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"gmask_token_id\": 130001,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"inner_hidden_size\": 16384,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"mask_token_id\": 130000,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_encoding_2d\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.28.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 130528\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json:   0%|          | 0.00/441 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json: 100%|██████████| 441/441 [00:00<00:00, 663kB/s]\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,225 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,225 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,229 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,229 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,232 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,232 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,234 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,234 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,266 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,266 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,268 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,268 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,269 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,269 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,272 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-05-11 10:29:27,272 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mDownloading (…)enization_chatglm.py:   0%|          | 0.00/17.0k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)enization_chatglm.py: 100%|██████████| 17.0k/17.0k [00:00<00:00, 27.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ice_text.model:   0%|          | 0.00/2.71M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ice_text.model: 100%|██████████| 2.71M/2.71M [00:00<00:00, 61.5MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1809] 2023-05-11 10:29:27,823 >> loading file ice_text.model from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/ice_text.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1809] 2023-05-11 10:29:27,823 >> loading file ice_text.model from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/ice_text.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1809] 2023-05-11 10:29:27,823 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1809] 2023-05-11 10:29:27,823 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1809] 2023-05-11 10:29:27,823 >> loading file special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1809] 2023-05-11 10:29:27,823 >> loading file special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1809] 2023-05-11 10:29:27,823 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1809] 2023-05-11 10:29:27,823 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/tokenizer_config.json\u001b[0m\n",
      "\u001b[35m[WARNING|auto_factory.py:456] 2023-05-11 10:29:27,809 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|auto_factory.py:456] 2023-05-11 10:29:27,809 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|auto_factory.py:456] 2023-05-11 10:29:27,818 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|auto_factory.py:456] 2023-05-11 10:29:27,818 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|auto_factory.py:456] 2023-05-11 10:29:27,821 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|auto_factory.py:456] 2023-05-11 10:29:27,821 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|auto_factory.py:456] 2023-05-11 10:29:27,823 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|auto_factory.py:456] 2023-05-11 10:29:27,823 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|auto_factory.py:456] 2023-05-11 10:29:27,832 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|auto_factory.py:456] 2023-05-11 10:29:27,832 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|auto_factory.py:456] 2023-05-11 10:29:27,834 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|auto_factory.py:456] 2023-05-11 10:29:27,834 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|auto_factory.py:456] 2023-05-11 10:29:27,840 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|auto_factory.py:456] 2023-05-11 10:29:27,840 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|auto_factory.py:456] 2023-05-11 10:29:27,859 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[WARNING|auto_factory.py:456] 2023-05-11 10:29:27,859 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35mDownloading (…)/modeling_chatglm.py:   0%|          | 0.00/57.6k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)/modeling_chatglm.py: 100%|██████████| 57.6k/57.6k [00:00<00:00, 776kB/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)main/quantization.py:   0%|          | 0.00/15.1k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)main/quantization.py: 100%|██████████| 15.1k/15.1k [00:00<00:00, 19.9MB/s]\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:29:27,993 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:29:27,993 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:29:27,999 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:29:27,999 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:29:28,002 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:29:28,002 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:29:28,003 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:29:28,003 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:29:28,032 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:29:28,032 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:29:28,034 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:29:28,034 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:29:28,040 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:29:28,040 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:29:28,044 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-05-11 10:29:28,044 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mDownloading (…)/modeling_chatglm.py:   0%|          | 0.00/57.6k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/modeling_chatglm.py: 100%|██████████| 57.6k/57.6k [00:00<00:00, 763kB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)main/quantization.py:   0%|          | 0.00/15.1k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)main/quantization.py: 100%|██████████| 15.1k/15.1k [00:00<00:00, 5.87MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)model.bin.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)model.bin.index.json: 100%|██████████| 33.4k/33.4k [00:00<00:00, 12.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2534] 2023-05-11 10:29:28,903 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2534] 2023-05-11 10:29:28,903 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   0%|          | 0.00/1.74G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)model.bin.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)model.bin.index.json: 100%|██████████| 33.4k/33.4k [00:00<00:00, 10.9MB/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:2534] 2023-05-11 10:29:28,772 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:2534] 2023-05-11 10:29:28,772 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:   0%|          | 0.00/1.74G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:   1%|          | 21.0M/1.74G [00:00<00:09, 189MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:   3%|▎         | 52.4M/1.74G [00:00<00:06, 251MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:   5%|▍         | 83.9M/1.74G [00:00<00:06, 268MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:   7%|▋         | 115M/1.74G [00:00<00:05, 275MB/s] #033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:   8%|▊         | 147M/1.74G [00:00<00:05, 277MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  10%|█         | 178M/1.74G [00:00<00:05, 283MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   1%|          | 21.0M/1.74G [00:00<00:09, 180MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   2%|▏         | 41.9M/1.74G [00:00<00:08, 195MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   4%|▍         | 73.4M/1.74G [00:00<00:06, 247MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   6%|▌         | 105M/1.74G [00:00<00:06, 259MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:   8%|▊         | 136M/1.74G [00:00<00:05, 271MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  10%|▉         | 168M/1.74G [00:00<00:05, 280MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  11%|█▏        | 199M/1.74G [00:00<00:05, 286MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  14%|█▍        | 241M/1.74G [00:00<00:04, 301MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  16%|█▋        | 283M/1.74G [00:01<00:04, 311MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  12%|█▏        | 210M/1.74G [00:00<00:05, 282MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  14%|█▍        | 252M/1.74G [00:00<00:05, 293MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  17%|█▋        | 294M/1.74G [00:01<00:04, 307MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  19%|█▉        | 336M/1.74G [00:01<00:04, 315MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  22%|██▏       | 377M/1.74G [00:01<00:04, 318MB/s]\u001b[0m\n",
      "\u001b[35m#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  24%|██▍       | 419M/1.74G [00:01<00:04, 318MB/s]\u001b[0m\n",
      "\u001b[35m#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  27%|██▋       | 461M/1.74G [00:01<00:04, 317MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  29%|██▉       | 503M/1.74G [00:01<00:03, 314MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  19%|█▊        | 325M/1.74G [00:01<00:04, 314MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  21%|██        | 367M/1.74G [00:01<00:04, 321MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  23%|██▎       | 409M/1.74G [00:01<00:04, 321MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  26%|██▌       | 451M/1.74G [00:01<00:03, 324MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  28%|██▊       | 493M/1.74G [00:01<00:03, 323MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  31%|███       | 535M/1.74G [00:01<00:03, 311MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  33%|███▎      | 566M/1.74G [00:01<00:03, 308MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  34%|███▍      | 598M/1.74G [00:02<00:03, 302MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  31%|███       | 535M/1.74G [00:01<00:03, 305MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  33%|███▎      | 566M/1.74G [00:01<00:03, 300MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  34%|███▍      | 598M/1.74G [00:02<00:03, 298MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  36%|███▌      | 629M/1.74G [00:02<00:03, 294MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  38%|███▊      | 661M/1.74G [00:02<00:03, 289MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  40%|███▉      | 692M/1.74G [00:02<00:03, 288MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  42%|████▏     | 724M/1.74G [00:02<00:03, 280MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  43%|████▎     | 755M/1.74G [00:02<00:03, 281MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  45%|████▌     | 786M/1.74G [00:02<00:03, 283MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  36%|███▌      | 629M/1.74G [00:02<00:03, 298MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  38%|███▊      | 661M/1.74G [00:02<00:03, 294MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  40%|███▉      | 692M/1.74G [00:02<00:03, 293MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  42%|████▏     | 724M/1.74G [00:02<00:03, 282MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  43%|████▎     | 755M/1.74G [00:02<00:03, 283MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  45%|████▌     | 786M/1.74G [00:02<00:03, 281MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  47%|████▋     | 818M/1.74G [00:02<00:03, 282MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  49%|████▉     | 849M/1.74G [00:02<00:03, 280MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  47%|████▋     | 818M/1.74G [00:02<00:03, 282MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  49%|████▉     | 849M/1.74G [00:02<00:03, 281MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  51%|█████     | 881M/1.74G [00:03<00:03, 282MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  52%|█████▏    | 912M/1.74G [00:03<00:02, 279MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  54%|█████▍    | 944M/1.74G [00:03<00:02, 279MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  56%|█████▌    | 975M/1.74G [00:03<00:02, 278MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  58%|█████▊    | 1.01G/1.74G [00:03<00:02, 280MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  60%|█████▉    | 1.04G/1.74G [00:03<00:02, 280MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  61%|██████▏   | 1.07G/1.74G [00:03<00:02, 284MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  51%|█████     | 881M/1.74G [00:03<00:03, 280MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  52%|█████▏    | 912M/1.74G [00:03<00:02, 279MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  54%|█████▍    | 944M/1.74G [00:03<00:02, 277MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  56%|█████▌    | 975M/1.74G [00:03<00:02, 281MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  58%|█████▊    | 1.01G/1.74G [00:03<00:02, 284MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  60%|█████▉    | 1.04G/1.74G [00:03<00:02, 287MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  61%|██████▏   | 1.07G/1.74G [00:03<00:02, 287MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  63%|██████▎   | 1.10G/1.74G [00:03<00:02, 288MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  65%|██████▌   | 1.13G/1.74G [00:03<00:02, 288MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  63%|██████▎   | 1.10G/1.74G [00:03<00:02, 284MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  65%|██████▌   | 1.13G/1.74G [00:03<00:02, 284MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  67%|██████▋   | 1.16G/1.74G [00:04<00:02, 281MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  69%|██████▊   | 1.20G/1.74G [00:04<00:01, 281MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  70%|███████   | 1.23G/1.74G [00:04<00:01, 280MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  72%|███████▏  | 1.26G/1.74G [00:04<00:01, 281MB/s]\u001b[0m\n",
      "\u001b[35m#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  74%|███████▍  | 1.29G/1.74G [00:04<00:01, 282MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  76%|███████▌  | 1.32G/1.74G [00:04<00:01, 282MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  78%|███████▊  | 1.35G/1.74G [00:04<00:01, 280MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  67%|██████▋   | 1.16G/1.74G [00:04<00:02, 287MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  69%|██████▊   | 1.20G/1.74G [00:04<00:01, 290MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  70%|███████   | 1.23G/1.74G [00:04<00:01, 289MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  72%|███████▏  | 1.26G/1.74G [00:04<00:01, 289MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  74%|███████▍  | 1.29G/1.74G [00:04<00:01, 290MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  76%|███████▌  | 1.32G/1.74G [00:04<00:01, 291MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  78%|███████▊  | 1.35G/1.74G [00:04<00:01, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  80%|███████▉  | 1.38G/1.74G [00:04<00:01, 299MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  82%|████████▏ | 1.43G/1.74G [00:04<00:01, 301MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  80%|███████▉  | 1.38G/1.74G [00:04<00:01, 281MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  81%|████████▏ | 1.42G/1.74G [00:04<00:01, 282MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  83%|████████▎ | 1.45G/1.74G [00:05<00:01, 284MB/s]\u001b[0m\n",
      "\u001b[35m#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  85%|████████▍ | 1.48G/1.74G [00:05<00:00, 284MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  87%|████████▋ | 1.51G/1.74G [00:05<00:00, 285MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  89%|████████▊ | 1.54G/1.74G [00:05<00:00, 286MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  90%|█████████ | 1.57G/1.74G [00:05<00:00, 285MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  92%|█████████▏| 1.60G/1.74G [00:05<00:00, 285MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  94%|█████████▍| 1.64G/1.74G [00:05<00:00, 285MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  84%|████████▍ | 1.47G/1.74G [00:05<00:00, 304MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  86%|████████▌ | 1.50G/1.74G [00:05<00:00, 304MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  88%|████████▊ | 1.53G/1.74G [00:05<00:00, 301MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  90%|████████▉ | 1.56G/1.74G [00:05<00:00, 299MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  92%|█████████▏| 1.59G/1.74G [00:05<00:00, 302MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  93%|█████████▎| 1.63G/1.74G [00:05<00:00, 301MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  95%|█████████▌| 1.66G/1.74G [00:05<00:00, 296MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  97%|█████████▋| 1.69G/1.74G [00:05<00:00, 298MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin:  99%|█████████▉| 1.72G/1.74G [00:05<00:00, 296MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00008.bin: 100%|██████████| 1.74G/1.74G [00:05<00:00, 292MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  12%|█▎        | 1/8 [00:06<00:42,  6.07s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  12%|█▎        | 1/8 [00:06<00:42,  6.05s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  12%|█▎        | 1/8 [00:06<00:42,  6.05s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  12%|█▎        | 1/8 [00:06<00:42,  6.05s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  12%|█▎        | 1/8 [00:06<00:42,  6.05s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  12%|█▎        | 1/8 [00:06<00:42,  6.05s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  12%|█▎        | 1/8 [00:06<00:42,  6.05s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  12%|█▎        | 1/8 [00:06<00:42,  6.10s/it]\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  96%|█████████▌| 1.67G/1.74G [00:05<00:00, 283MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  98%|█████████▊| 1.70G/1.74G [00:05<00:00, 285MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin:  99%|█████████▉| 1.73G/1.74G [00:06<00:00, 283MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00001-of-00008.bin: 100%|██████████| 1.74G/1.74G [00:06<00:00, 287MB/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:  12%|█▎        | 1/8 [00:06<00:43,  6.21s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  12%|█▎        | 1/8 [00:06<00:43,  6.21s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  12%|█▎        | 1/8 [00:06<00:43,  6.20s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  12%|█▎        | 1/8 [00:06<00:43,  6.26s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  12%|█▎        | 1/8 [00:06<00:43,  6.25s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  12%|█▎        | 1/8 [00:06<00:43,  6.25s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  12%|█▎        | 1/8 [00:06<00:43,  6.26s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  12%|█▎        | 1/8 [00:06<00:43,  6.25s/it]\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:   2%|▏         | 31.5M/1.88G [00:00<00:06, 297MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:   3%|▎         | 62.9M/1.88G [00:00<00:05, 303MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:   5%|▌         | 94.4M/1.88G [00:00<00:05, 308MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:   7%|▋         | 136M/1.88G [00:00<00:05, 316MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   2%|▏         | 41.9M/1.88G [00:00<00:05, 334MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   4%|▍         | 83.9M/1.88G [00:00<00:05, 331MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   7%|▋         | 126M/1.88G [00:00<00:05, 331MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:   9%|▉         | 168M/1.88G [00:00<00:05, 337MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  11%|█         | 210M/1.88G [00:00<00:04, 341MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  13%|█▎        | 252M/1.88G [00:00<00:04, 338MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  16%|█▌        | 294M/1.88G [00:00<00:04, 334MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:   9%|▉         | 178M/1.88G [00:00<00:05, 317MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  12%|█▏        | 220M/1.88G [00:00<00:05, 320MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  14%|█▍        | 262M/1.88G [00:00<00:05, 320MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  16%|█▌        | 304M/1.88G [00:00<00:04, 320MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  18%|█▊        | 346M/1.88G [00:01<00:04, 321MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  21%|██        | 388M/1.88G [00:01<00:04, 320MB/s]\u001b[0m\n",
      "\u001b[35m#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  23%|██▎       | 430M/1.88G [00:01<00:04, 322MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  25%|██▌       | 472M/1.88G [00:01<00:04, 317MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  18%|█▊        | 336M/1.88G [00:01<00:04, 335MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  20%|██        | 377M/1.88G [00:01<00:04, 335MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  22%|██▏       | 419M/1.88G [00:01<00:04, 329MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  25%|██▍       | 461M/1.88G [00:01<00:04, 330MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  27%|██▋       | 503M/1.88G [00:01<00:04, 324MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  29%|██▉       | 545M/1.88G [00:01<00:04, 314MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  31%|███       | 587M/1.88G [00:01<00:04, 310MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  33%|███▎      | 619M/1.88G [00:01<00:04, 310MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  27%|██▋       | 514M/1.88G [00:01<00:04, 312MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  29%|██▉       | 545M/1.88G [00:01<00:04, 311MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  31%|███       | 577M/1.88G [00:01<00:04, 309MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  33%|███▎      | 619M/1.88G [00:01<00:04, 313MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  35%|███▍      | 650M/1.88G [00:02<00:03, 312MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  37%|███▋      | 692M/1.88G [00:02<00:03, 314MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  38%|███▊      | 724M/1.88G [00:02<00:03, 313MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  40%|████      | 755M/1.88G [00:02<00:03, 312MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  42%|████▏     | 797M/1.88G [00:02<00:03, 317MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  35%|███▌      | 661M/1.88G [00:02<00:03, 315MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  37%|███▋      | 703M/1.88G [00:02<00:03, 310MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  39%|███▉      | 734M/1.88G [00:02<00:03, 307MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  41%|████      | 765M/1.88G [00:02<00:03, 304MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  42%|████▏     | 797M/1.88G [00:02<00:03, 293MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  44%|████▍     | 828M/1.88G [00:02<00:03, 298MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  46%|████▌     | 860M/1.88G [00:02<00:03, 297MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  48%|████▊     | 902M/1.88G [00:02<00:03, 305MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  45%|████▍     | 839M/1.88G [00:02<00:03, 315MB/s]\u001b[0m\n",
      "\u001b[35m#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  47%|████▋     | 881M/1.88G [00:02<00:03, 314MB/s]\u001b[0m\n",
      "\u001b[35m#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  49%|████▊     | 912M/1.88G [00:02<00:03, 314MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  51%|█████     | 954M/1.88G [00:03<00:02, 317MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  53%|█████▎    | 996M/1.88G [00:03<00:02, 318MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  55%|█████▌    | 1.04G/1.88G [00:03<00:02, 312MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  57%|█████▋    | 1.07G/1.88G [00:03<00:02, 308MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  59%|█████▉    | 1.11G/1.88G [00:03<00:02, 313MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  50%|█████     | 944M/1.88G [00:02<00:03, 311MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  52%|█████▏    | 986M/1.88G [00:03<00:02, 314MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  54%|█████▍    | 1.02G/1.88G [00:03<00:02, 313MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  56%|█████▌    | 1.05G/1.88G [00:03<00:02, 310MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  57%|█████▋    | 1.08G/1.88G [00:03<00:02, 301MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  59%|█████▉    | 1.11G/1.88G [00:03<00:02, 305MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  61%|██████    | 1.14G/1.88G [00:03<00:02, 307MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  63%|██████▎   | 1.18G/1.88G [00:03<00:02, 312MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  65%|██████▍   | 1.22G/1.88G [00:03<00:02, 305MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  61%|██████    | 1.14G/1.88G [00:03<00:02, 313MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  62%|██████▏   | 1.17G/1.88G [00:03<00:02, 308MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  64%|██████▍   | 1.21G/1.88G [00:03<00:02, 302MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  66%|██████▌   | 1.24G/1.88G [00:04<00:03, 193MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  67%|██████▋   | 1.27G/1.88G [00:04<00:02, 217MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  69%|██████▉   | 1.30G/1.88G [00:04<00:02, 238MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  71%|███████   | 1.33G/1.88G [00:04<00:02, 254MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  67%|██████▋   | 1.26G/1.88G [00:03<00:02, 310MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  69%|██████▉   | 1.30G/1.88G [00:04<00:01, 327MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  71%|███████▏  | 1.34G/1.88G [00:04<00:01, 330MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  74%|███████▎  | 1.38G/1.88G [00:04<00:01, 334MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  76%|███████▌  | 1.43G/1.88G [00:04<00:01, 333MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  78%|███████▊  | 1.47G/1.88G [00:04<00:01, 330MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  80%|████████  | 1.51G/1.88G [00:04<00:01, 332MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  83%|████████▎ | 1.55G/1.88G [00:04<00:00, 335MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  73%|███████▎  | 1.36G/1.88G [00:04<00:01, 269MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  74%|███████▍  | 1.39G/1.88G [00:04<00:01, 279MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  76%|███████▌  | 1.43G/1.88G [00:04<00:01, 289MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  78%|███████▊  | 1.46G/1.88G [00:04<00:01, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  79%|███████▉  | 1.49G/1.88G [00:04<00:01, 299MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  81%|████████  | 1.52G/1.88G [00:05<00:01, 301MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  83%|████████▎ | 1.56G/1.88G [00:05<00:01, 309MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  85%|████████▌ | 1.60G/1.88G [00:05<00:00, 310MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  88%|████████▊ | 1.65G/1.88G [00:05<00:00, 311MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  85%|████████▍ | 1.59G/1.88G [00:04<00:00, 331MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  87%|████████▋ | 1.64G/1.88G [00:05<00:00, 329MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  89%|████████▉ | 1.68G/1.88G [00:05<00:00, 325MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  91%|█████████▏| 1.72G/1.88G [00:05<00:00, 320MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  94%|█████████▎| 1.76G/1.88G [00:05<00:00, 318MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  96%|█████████▌| 1.80G/1.88G [00:05<00:00, 318MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin:  98%|█████████▊| 1.85G/1.88G [00:05<00:00, 314MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin: 100%|█████████▉| 1.88G/1.88G [00:05<00:00, 311MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00008.bin: 100%|██████████| 1.88G/1.88G [00:05<00:00, 319MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 2/8 [00:12<00:36,  6.03s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 2/8 [00:12<00:36,  6.01s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 2/8 [00:12<00:36,  6.03s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 2/8 [00:12<00:36,  6.02s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 2/8 [00:12<00:36,  6.02s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 2/8 [00:12<00:36,  6.03s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 2/8 [00:12<00:36,  6.04s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 2/8 [00:12<00:36,  6.05s/it]\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  89%|████████▉ | 1.68G/1.88G [00:05<00:00, 312MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  91%|█████████▏| 1.72G/1.88G [00:05<00:00, 316MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  94%|█████████▎| 1.76G/1.88G [00:05<00:00, 312MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  95%|█████████▌| 1.79G/1.88G [00:05<00:00, 310MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin:  98%|█████████▊| 1.84G/1.88G [00:06<00:00, 313MB/s]\u001b[0m\n",
      "\u001b[35m#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin: 100%|█████████▉| 1.88G/1.88G [00:06<00:00, 317MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00002-of-00008.bin: 100%|██████████| 1.88G/1.88G [00:06<00:00, 302MB/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:  25%|██▌       | 2/8 [00:12<00:37,  6.28s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  25%|██▌       | 2/8 [00:12<00:37,  6.27s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  25%|██▌       | 2/8 [00:12<00:37,  6.26s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  25%|██▌       | 2/8 [00:12<00:37,  6.28s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  25%|██▌       | 2/8 [00:12<00:37,  6.29s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  25%|██▌       | 2/8 [00:12<00:37,  6.28s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  25%|██▌       | 2/8 [00:12<00:37,  6.28s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  25%|██▌       | 2/8 [00:12<00:37,  6.31s/it]\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:   2%|▏         | 31.5M/1.98G [00:00<00:06, 308MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   2%|▏         | 31.5M/1.98G [00:00<00:06, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   3%|▎         | 62.9M/1.98G [00:00<00:06, 301MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   5%|▍         | 94.4M/1.98G [00:00<00:06, 302MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   6%|▋         | 126M/1.98G [00:00<00:06, 305MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:   8%|▊         | 157M/1.98G [00:00<00:05, 305MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  10%|▉         | 189M/1.98G [00:00<00:05, 306MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  12%|█▏        | 231M/1.98G [00:00<00:05, 308MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  13%|█▎        | 262M/1.98G [00:00<00:05, 309MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:   4%|▎         | 73.4M/1.98G [00:00<00:06, 311MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:   6%|▌         | 115M/1.98G [00:00<00:05, 319MB/s] #033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:   8%|▊         | 157M/1.98G [00:00<00:05, 320MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  10%|█         | 199M/1.98G [00:00<00:05, 308MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  12%|█▏        | 241M/1.98G [00:00<00:05, 312MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  14%|█▍        | 273M/1.98G [00:00<00:05, 310MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  15%|█▌        | 304M/1.98G [00:00<00:05, 311MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  17%|█▋        | 346M/1.98G [00:01<00:05, 312MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  15%|█▍        | 294M/1.98G [00:00<00:05, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  16%|█▋        | 325M/1.98G [00:01<00:05, 293MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  18%|█▊        | 357M/1.98G [00:01<00:05, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  20%|█▉        | 388M/1.98G [00:01<00:05, 297MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  21%|██        | 419M/1.98G [00:01<00:05, 301MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  23%|██▎       | 451M/1.98G [00:01<00:05, 302MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  24%|██▍       | 482M/1.98G [00:01<00:04, 303MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  26%|██▌       | 514M/1.98G [00:01<00:04, 299MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  28%|██▊       | 545M/1.98G [00:01<00:04, 302MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  29%|██▉       | 577M/1.98G [00:01<00:04, 299MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  20%|█▉        | 388M/1.98G [00:01<00:05, 314MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  21%|██        | 419M/1.98G [00:01<00:04, 313MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  23%|██▎       | 461M/1.98G [00:01<00:04, 316MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  25%|██▌       | 503M/1.98G [00:01<00:04, 313MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  28%|██▊       | 545M/1.98G [00:01<00:04, 314MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  30%|██▉       | 587M/1.98G [00:01<00:04, 317MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  32%|███▏      | 629M/1.98G [00:01<00:04, 321MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  34%|███▍      | 671M/1.98G [00:02<00:04, 324MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  31%|███       | 608M/1.98G [00:02<00:04, 299MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  32%|███▏      | 640M/1.98G [00:02<00:04, 297MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  34%|███▍      | 671M/1.98G [00:02<00:04, 297MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  35%|███▌      | 703M/1.98G [00:02<00:04, 296MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  37%|███▋      | 734M/1.98G [00:02<00:04, 294MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  39%|███▊      | 765M/1.98G [00:02<00:04, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  41%|████      | 807M/1.98G [00:02<00:03, 305MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  43%|████▎     | 849M/1.98G [00:02<00:03, 316MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  44%|████▍     | 881M/1.98G [00:02<00:03, 308MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  36%|███▌      | 713M/1.98G [00:02<00:03, 327MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  38%|███▊      | 755M/1.98G [00:02<00:03, 329MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  40%|████      | 797M/1.98G [00:02<00:03, 322MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  42%|████▏     | 839M/1.98G [00:02<00:03, 312MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  44%|████▍     | 870M/1.98G [00:02<00:03, 311MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  46%|████▌     | 902M/1.98G [00:02<00:03, 308MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  47%|████▋     | 933M/1.98G [00:02<00:03, 307MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  49%|████▊     | 965M/1.98G [00:03<00:03, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  50%|█████     | 996M/1.98G [00:03<00:03, 282MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  46%|████▌     | 912M/1.98G [00:03<00:03, 306MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  48%|████▊     | 954M/1.98G [00:03<00:03, 309MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  50%|████▉     | 986M/1.98G [00:03<00:03, 303MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  51%|█████▏    | 1.02G/1.98G [00:03<00:03, 298MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  53%|█████▎    | 1.05G/1.98G [00:03<00:03, 281MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  55%|█████▍    | 1.08G/1.98G [00:03<00:03, 277MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  56%|█████▌    | 1.11G/1.98G [00:03<00:03, 278MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  58%|█████▊    | 1.15G/1.98G [00:03<00:02, 292MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  52%|█████▏    | 1.03G/1.98G [00:03<00:03, 280MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  54%|█████▍    | 1.07G/1.98G [00:03<00:03, 294MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  56%|█████▌    | 1.11G/1.98G [00:03<00:02, 301MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  58%|█████▊    | 1.15G/1.98G [00:03<00:02, 307MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  60%|██████    | 1.20G/1.98G [00:03<00:02, 308MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  62%|██████▏   | 1.23G/1.98G [00:03<00:02, 305MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  64%|██████▎   | 1.26G/1.98G [00:04<00:02, 306MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  65%|██████▌   | 1.29G/1.98G [00:04<00:02, 303MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  60%|██████    | 1.20G/1.98G [00:03<00:02, 300MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  62%|██████▏   | 1.24G/1.98G [00:04<00:02, 307MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  64%|██████▍   | 1.27G/1.98G [00:04<00:02, 308MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  66%|██████▌   | 1.30G/1.98G [00:04<00:02, 302MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  67%|██████▋   | 1.33G/1.98G [00:04<00:02, 304MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  69%|██████▉   | 1.36G/1.98G [00:04<00:02, 299MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  70%|███████   | 1.39G/1.98G [00:04<00:01, 302MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  72%|███████▏  | 1.43G/1.98G [00:04<00:01, 296MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  74%|███████▎  | 1.46G/1.98G [00:04<00:01, 298MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  67%|██████▋   | 1.33G/1.98G [00:04<00:02, 309MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  69%|██████▉   | 1.36G/1.98G [00:04<00:01, 310MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  71%|███████   | 1.41G/1.98G [00:04<00:01, 313MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  73%|███████▎  | 1.45G/1.98G [00:04<00:01, 316MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  75%|███████▌  | 1.49G/1.98G [00:04<00:01, 321MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  77%|███████▋  | 1.53G/1.98G [00:04<00:01, 323MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  79%|███████▉  | 1.57G/1.98G [00:05<00:01, 329MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  82%|████████▏ | 1.61G/1.98G [00:05<00:01, 331MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  75%|███████▌  | 1.49G/1.98G [00:04<00:01, 301MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  77%|███████▋  | 1.52G/1.98G [00:05<00:01, 304MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  79%|███████▉  | 1.56G/1.98G [00:05<00:01, 309MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  80%|████████  | 1.59G/1.98G [00:05<00:01, 194MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  82%|████████▏ | 1.63G/1.98G [00:05<00:01, 216MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  84%|████████▎ | 1.66G/1.98G [00:05<00:01, 220MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  85%|████████▌ | 1.69G/1.98G [00:05<00:01, 236MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  84%|████████▎ | 1.66G/1.98G [00:05<00:00, 328MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  86%|████████▌ | 1.70G/1.98G [00:05<00:00, 331MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  88%|████████▊ | 1.74G/1.98G [00:05<00:00, 330MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  90%|█████████ | 1.78G/1.98G [00:05<00:00, 328MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  92%|█████████▏| 1.82G/1.98G [00:05<00:00, 331MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  94%|█████████▍| 1.87G/1.98G [00:05<00:00, 331MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  96%|█████████▋| 1.91G/1.98G [00:06<00:00, 334MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin:  98%|█████████▊| 1.95G/1.98G [00:06<00:00, 335MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  87%|████████▋ | 1.72G/1.98G [00:05<00:01, 252MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  89%|████████▉ | 1.76G/1.98G [00:06<00:00, 274MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  91%|█████████ | 1.80G/1.98G [00:06<00:00, 286MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  93%|█████████▎| 1.85G/1.98G [00:06<00:00, 294MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  95%|█████████▍| 1.88G/1.98G [00:06<00:00, 294MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  96%|█████████▋| 1.91G/1.98G [00:06<00:00, 299MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin:  98%|█████████▊| 1.95G/1.98G [00:06<00:00, 310MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00003-of-00008.bin: 100%|██████████| 1.98G/1.98G [00:06<00:00, 291MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  38%|███▊      | 3/8 [00:18<00:32,  6.43s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  38%|███▊      | 3/8 [00:18<00:32,  6.42s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  38%|███▊      | 3/8 [00:18<00:32,  6.43s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  38%|███▊      | 3/8 [00:18<00:32,  6.42s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  38%|███▊      | 3/8 [00:18<00:32,  6.43s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  38%|███▊      | 3/8 [00:18<00:32,  6.43s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  38%|███▊      | 3/8 [00:19<00:32,  6.43s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  38%|███▊      | 3/8 [00:19<00:32,  6.44s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   0%|          | 0.00/1.91G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00003-of-00008.bin: 100%|██████████| 1.98G/1.98G [00:06<00:00, 316MB/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:  38%|███▊      | 3/8 [00:18<00:31,  6.32s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  38%|███▊      | 3/8 [00:18<00:31,  6.30s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  38%|███▊      | 3/8 [00:18<00:31,  6.31s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  38%|███▊      | 3/8 [00:18<00:31,  6.31s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  38%|███▊      | 3/8 [00:18<00:31,  6.32s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  38%|███▊      | 3/8 [00:18<00:31,  6.32s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  38%|███▊      | 3/8 [00:18<00:31,  6.31s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  38%|███▊      | 3/8 [00:18<00:31,  6.33s/it]\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:   0%|          | 0.00/1.91G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:   2%|▏         | 31.5M/1.91G [00:00<00:07, 252MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:   4%|▍         | 73.4M/1.91G [00:00<00:05, 316MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:   6%|▌         | 115M/1.91G [00:00<00:05, 335MB/s] #033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:   8%|▊         | 157M/1.91G [00:00<00:05, 351MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  10%|█         | 199M/1.91G [00:00<00:04, 355MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  13%|█▎        | 241M/1.91G [00:00<00:04, 360MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  15%|█▍        | 283M/1.91G [00:00<00:04, 364MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   2%|▏         | 31.5M/1.91G [00:00<00:06, 307MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   3%|▎         | 62.9M/1.91G [00:00<00:06, 308MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   5%|▌         | 105M/1.91G [00:00<00:05, 319MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:   8%|▊         | 147M/1.91G [00:00<00:05, 323MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  10%|▉         | 189M/1.91G [00:00<00:05, 323MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  12%|█▏        | 231M/1.91G [00:00<00:05, 318MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  14%|█▍        | 273M/1.91G [00:00<00:05, 324MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  16%|█▋        | 315M/1.91G [00:00<00:04, 327MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  17%|█▋        | 325M/1.91G [00:00<00:04, 364MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  19%|█▉        | 367M/1.91G [00:01<00:04, 367MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  21%|██▏       | 409M/1.91G [00:01<00:04, 368MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  24%|██▎       | 451M/1.91G [00:01<00:03, 368MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  26%|██▌       | 493M/1.91G [00:01<00:03, 370MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  28%|██▊       | 535M/1.91G [00:01<00:03, 367MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  30%|███       | 577M/1.91G [00:01<00:03, 367MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  32%|███▏      | 619M/1.91G [00:01<00:03, 364MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  19%|█▊        | 357M/1.91G [00:01<00:04, 327MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  21%|██        | 398M/1.91G [00:01<00:04, 335MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  23%|██▎       | 440M/1.91G [00:01<00:04, 344MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  25%|██▌       | 482M/1.91G [00:01<00:04, 348MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  27%|██▋       | 524M/1.91G [00:01<00:04, 341MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  30%|██▉       | 566M/1.91G [00:01<00:04, 331MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  32%|███▏      | 608M/1.91G [00:01<00:03, 328MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  34%|███▍      | 650M/1.91G [00:01<00:04, 315MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  35%|███▍      | 661M/1.91G [00:01<00:03, 365MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  37%|███▋      | 703M/1.91G [00:01<00:03, 365MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  39%|███▉      | 744M/1.91G [00:02<00:03, 365MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  41%|████      | 786M/1.91G [00:02<00:03, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  43%|████▎     | 828M/1.91G [00:02<00:02, 364MB/s]\u001b[0m\n",
      "\u001b[35m#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  45%|████▌     | 870M/1.91G [00:02<00:02, 365MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  48%|████▊     | 912M/1.91G [00:02<00:02, 367MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  50%|████▉     | 954M/1.91G [00:02<00:02, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  52%|█████▏    | 996M/1.91G [00:02<00:02, 368MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  36%|███▌      | 692M/1.91G [00:02<00:03, 315MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  38%|███▊      | 734M/1.91G [00:02<00:03, 323MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  41%|████      | 776M/1.91G [00:02<00:03, 331MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  43%|████▎     | 818M/1.91G [00:02<00:03, 337MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  45%|████▍     | 860M/1.91G [00:02<00:03, 340MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  47%|████▋     | 902M/1.91G [00:02<00:02, 345MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  49%|████▉     | 944M/1.91G [00:02<00:02, 347MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  52%|█████▏    | 986M/1.91G [00:02<00:02, 346MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  54%|█████▍    | 1.04G/1.91G [00:02<00:02, 367MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  56%|█████▋    | 1.08G/1.91G [00:02<00:02, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  59%|█████▊    | 1.12G/1.91G [00:03<00:02, 369MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  61%|██████    | 1.16G/1.91G [00:03<00:02, 370MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  63%|██████▎   | 1.21G/1.91G [00:03<00:01, 369MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  65%|██████▌   | 1.25G/1.91G [00:03<00:01, 369MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  67%|██████▋   | 1.29G/1.91G [00:03<00:01, 369MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  70%|██████▉   | 1.33G/1.91G [00:03<00:01, 368MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  72%|███████▏  | 1.37G/1.91G [00:03<00:01, 370MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  54%|█████▎    | 1.03G/1.91G [00:03<00:02, 332MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  56%|█████▌    | 1.07G/1.91G [00:03<00:02, 317MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  58%|█████▊    | 1.11G/1.91G [00:03<00:02, 312MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  60%|█████▉    | 1.14G/1.91G [00:03<00:02, 308MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  61%|██████▏   | 1.17G/1.91G [00:03<00:02, 301MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  63%|██████▎   | 1.21G/1.91G [00:03<00:02, 301MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  65%|██████▍   | 1.24G/1.91G [00:03<00:02, 296MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  66%|██████▋   | 1.27G/1.91G [00:03<00:02, 293MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  68%|██████▊   | 1.30G/1.91G [00:04<00:02, 292MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  74%|███████▍  | 1.42G/1.91G [00:03<00:01, 371MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  76%|███████▌  | 1.46G/1.91G [00:04<00:01, 372MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  78%|███████▊  | 1.50G/1.91G [00:04<00:01, 373MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  81%|████████  | 1.54G/1.91G [00:04<00:00, 373MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  83%|████████▎ | 1.58G/1.91G [00:04<00:00, 373MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  85%|████████▍ | 1.63G/1.91G [00:04<00:00, 373MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  87%|████████▋ | 1.67G/1.91G [00:04<00:00, 372MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  89%|████████▉ | 1.71G/1.91G [00:04<00:00, 372MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  92%|█████████▏| 1.75G/1.91G [00:04<00:00, 372MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  70%|██████▉   | 1.33G/1.91G [00:04<00:01, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  71%|███████   | 1.36G/1.91G [00:04<00:01, 299MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  73%|███████▎  | 1.39G/1.91G [00:04<00:01, 296MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  75%|███████▍  | 1.43G/1.91G [00:04<00:01, 300MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  76%|███████▌  | 1.46G/1.91G [00:04<00:01, 301MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  78%|███████▊  | 1.49G/1.91G [00:04<00:01, 302MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  79%|███████▉  | 1.52G/1.91G [00:04<00:01, 299MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  81%|████████  | 1.55G/1.91G [00:04<00:01, 301MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  83%|████████▎ | 1.58G/1.91G [00:04<00:01, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  94%|█████████▎| 1.79G/1.91G [00:04<00:00, 371MB/s]\u001b[0m\n",
      "\u001b[35m#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  96%|█████████▌| 1.84G/1.91G [00:05<00:00, 368MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin:  98%|█████████▊| 1.88G/1.91G [00:05<00:00, 369MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00004-of-00008.bin: 100%|██████████| 1.91G/1.91G [00:05<00:00, 365MB/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:  50%|█████     | 4/8 [00:24<00:23,  5.94s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  50%|█████     | 4/8 [00:24<00:23,  5.95s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  50%|█████     | 4/8 [00:24<00:23,  5.94s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  50%|█████     | 4/8 [00:24<00:23,  5.95s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  50%|█████     | 4/8 [00:24<00:23,  5.95s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  50%|█████     | 4/8 [00:24<00:23,  5.96s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  50%|█████     | 4/8 [00:24<00:23,  5.95s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  50%|█████     | 4/8 [00:24<00:23,  5.96s/it]\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:   2%|▏         | 41.9M/1.88G [00:00<00:05, 344MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:   4%|▍         | 83.9M/1.88G [00:00<00:05, 353MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:   7%|▋         | 126M/1.88G [00:00<00:04, 363MB/s] #033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:   9%|▉         | 168M/1.88G [00:00<00:04, 367MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  84%|████████▍ | 1.61G/1.91G [00:05<00:01, 296MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  86%|████████▌ | 1.65G/1.91G [00:05<00:00, 300MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  88%|████████▊ | 1.68G/1.91G [00:05<00:00, 302MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  89%|████████▉ | 1.71G/1.91G [00:05<00:00, 300MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  91%|█████████ | 1.74G/1.91G [00:05<00:00, 302MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  93%|█████████▎| 1.77G/1.91G [00:05<00:00, 305MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  94%|█████████▍| 1.80G/1.91G [00:05<00:00, 300MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  96%|█████████▌| 1.84G/1.91G [00:05<00:00, 302MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin:  98%|█████████▊| 1.88G/1.91G [00:05<00:00, 307MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin: 100%|█████████▉| 1.91G/1.91G [00:06<00:00, 309MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  11%|█         | 210M/1.88G [00:00<00:04, 364MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  13%|█▎        | 252M/1.88G [00:00<00:04, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  16%|█▌        | 294M/1.88G [00:00<00:04, 368MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  18%|█▊        | 336M/1.88G [00:00<00:04, 368MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  20%|██        | 377M/1.88G [00:01<00:04, 369MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  22%|██▏       | 419M/1.88G [00:01<00:03, 367MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  25%|██▍       | 461M/1.88G [00:01<00:03, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  27%|██▋       | 503M/1.88G [00:01<00:03, 365MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00004-of-00008.bin: 100%|██████████| 1.91G/1.91G [00:06<00:00, 315MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 4/8 [00:25<00:25,  6.33s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 4/8 [00:25<00:25,  6.32s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 4/8 [00:25<00:25,  6.32s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 4/8 [00:25<00:25,  6.32s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 4/8 [00:25<00:25,  6.32s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 4/8 [00:25<00:25,  6.33s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 4/8 [00:25<00:25,  6.33s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 4/8 [00:25<00:25,  6.33s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   2%|▏         | 31.5M/1.88G [00:00<00:06, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   4%|▍         | 73.4M/1.88G [00:00<00:05, 315MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   6%|▌         | 115M/1.88G [00:00<00:05, 316MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:   8%|▊         | 157M/1.88G [00:00<00:05, 316MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  11%|█         | 199M/1.88G [00:00<00:05, 323MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  13%|█▎        | 241M/1.88G [00:00<00:04, 329MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  15%|█▌        | 283M/1.88G [00:00<00:04, 327MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  29%|██▉       | 545M/1.88G [00:01<00:03, 368MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  31%|███       | 587M/1.88G [00:01<00:03, 370MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  33%|███▎      | 629M/1.88G [00:01<00:03, 370MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  36%|███▌      | 671M/1.88G [00:01<00:03, 370MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  38%|███▊      | 713M/1.88G [00:01<00:03, 368MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  40%|████      | 755M/1.88G [00:02<00:03, 371MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  42%|████▏     | 797M/1.88G [00:02<00:02, 368MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  45%|████▍     | 839M/1.88G [00:02<00:02, 370MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  47%|████▋     | 881M/1.88G [00:02<00:02, 372MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  17%|█▋        | 325M/1.88G [00:01<00:04, 331MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  20%|█▉        | 367M/1.88G [00:01<00:04, 332MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  22%|██▏       | 409M/1.88G [00:01<00:04, 331MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  24%|██▍       | 451M/1.88G [00:01<00:04, 330MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  26%|██▌       | 493M/1.88G [00:01<00:04, 329MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  28%|██▊       | 535M/1.88G [00:01<00:04, 330MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  31%|███       | 577M/1.88G [00:01<00:03, 333MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  49%|████▉     | 923M/1.88G [00:02<00:02, 373MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  51%|█████▏    | 965M/1.88G [00:02<00:02, 371MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  54%|█████▎    | 1.01G/1.88G [00:02<00:02, 372MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  56%|█████▌    | 1.05G/1.88G [00:02<00:02, 371MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  58%|█████▊    | 1.09G/1.88G [00:02<00:02, 371MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  60%|██████    | 1.13G/1.88G [00:03<00:02, 372MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  62%|██████▏   | 1.17G/1.88G [00:03<00:01, 370MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  65%|██████▍   | 1.22G/1.88G [00:03<00:01, 372MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  67%|██████▋   | 1.26G/1.88G [00:03<00:01, 372MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  33%|███▎      | 619M/1.88G [00:01<00:03, 326MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  35%|███▌      | 661M/1.88G [00:02<00:03, 315MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  37%|███▋      | 703M/1.88G [00:02<00:03, 304MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  39%|███▉      | 734M/1.88G [00:02<00:03, 298MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  41%|████      | 765M/1.88G [00:02<00:03, 294MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  42%|████▏     | 797M/1.88G [00:02<00:03, 294MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  44%|████▍     | 828M/1.88G [00:02<00:05, 190MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  69%|██████▉   | 1.30G/1.88G [00:03<00:01, 370MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  71%|███████▏  | 1.34G/1.88G [00:03<00:01, 370MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  74%|███████▎  | 1.38G/1.88G [00:03<00:01, 361MB/s]\u001b[0m\n",
      "\u001b[35m#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  76%|███████▌  | 1.43G/1.88G [00:03<00:01, 360MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  78%|███████▊  | 1.47G/1.88G [00:04<00:01, 351MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  80%|████████  | 1.51G/1.88G [00:04<00:01, 353MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  83%|████████▎ | 1.55G/1.88G [00:04<00:00, 355MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  85%|████████▍ | 1.59G/1.88G [00:04<00:00, 356MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  87%|████████▋ | 1.64G/1.88G [00:04<00:00, 353MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  46%|████▌     | 860M/1.88G [00:02<00:04, 208MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  47%|████▋     | 891M/1.88G [00:03<00:04, 225MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  49%|████▉     | 923M/1.88G [00:03<00:04, 238MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  51%|█████     | 954M/1.88G [00:03<00:03, 249MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  52%|█████▏    | 986M/1.88G [00:03<00:03, 260MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  54%|█████▍    | 1.02G/1.88G [00:03<00:03, 268MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  56%|█████▌    | 1.05G/1.88G [00:03<00:03, 273MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  57%|█████▋    | 1.08G/1.88G [00:03<00:02, 276MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  59%|█████▉    | 1.11G/1.88G [00:03<00:02, 281MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  89%|████████▉ | 1.68G/1.88G [00:04<00:00, 352MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  91%|█████████▏| 1.72G/1.88G [00:04<00:00, 352MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  94%|█████████▎| 1.76G/1.88G [00:04<00:00, 352MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  96%|█████████▌| 1.80G/1.88G [00:04<00:00, 354MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin:  98%|█████████▊| 1.85G/1.88G [00:05<00:00, 351MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00005-of-00008.bin: 100%|██████████| 1.88G/1.88G [00:05<00:00, 363MB/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:  62%|██████▎   | 5/8 [00:29<00:17,  5.70s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  62%|██████▎   | 5/8 [00:29<00:17,  5.70s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  62%|██████▎   | 5/8 [00:29<00:17,  5.70s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  62%|██████▎   | 5/8 [00:29<00:17,  5.71s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  62%|██████▎   | 5/8 [00:29<00:17,  5.72s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  62%|██████▎   | 5/8 [00:29<00:17,  5.71s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  62%|██████▎   | 5/8 [00:29<00:17,  5.72s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  62%|██████▎   | 5/8 [00:29<00:17,  5.72s/it]\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:   2%|▏         | 31.5M/1.88G [00:00<00:06, 297MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  61%|██████    | 1.14G/1.88G [00:03<00:02, 284MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  62%|██████▏   | 1.17G/1.88G [00:04<00:02, 285MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  64%|██████▍   | 1.21G/1.88G [00:04<00:02, 286MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  66%|██████▌   | 1.24G/1.88G [00:04<00:02, 288MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  67%|██████▋   | 1.27G/1.88G [00:04<00:02, 290MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  69%|██████▉   | 1.30G/1.88G [00:04<00:01, 296MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  71%|███████▏  | 1.34G/1.88G [00:04<00:01, 308MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  73%|███████▎  | 1.37G/1.88G [00:04<00:01, 309MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  75%|███████▌  | 1.42G/1.88G [00:04<00:01, 317MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:   4%|▍         | 73.4M/1.88G [00:00<00:05, 320MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:   6%|▌         | 115M/1.88G [00:00<00:05, 320MB/s] #033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:   8%|▊         | 157M/1.88G [00:00<00:05, 306MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  11%|█         | 199M/1.88G [00:00<00:05, 313MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  13%|█▎        | 241M/1.88G [00:00<00:05, 319MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  15%|█▌        | 283M/1.88G [00:00<00:04, 323MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  17%|█▋        | 325M/1.88G [00:01<00:04, 331MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  20%|█▉        | 367M/1.88G [00:01<00:04, 338MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  78%|███████▊  | 1.46G/1.88G [00:04<00:01, 316MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  80%|███████▉  | 1.50G/1.88G [00:05<00:01, 310MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  82%|████████▏ | 1.54G/1.88G [00:05<00:01, 313MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  84%|████████▍ | 1.58G/1.88G [00:05<00:00, 314MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  86%|████████▋ | 1.63G/1.88G [00:05<00:00, 321MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  89%|████████▊ | 1.67G/1.88G [00:05<00:00, 324MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  91%|█████████ | 1.71G/1.88G [00:05<00:00, 320MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  93%|█████████▎| 1.75G/1.88G [00:05<00:00, 320MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  22%|██▏       | 409M/1.88G [00:01<00:04, 342MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  24%|██▍       | 451M/1.88G [00:01<00:04, 345MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  26%|██▌       | 493M/1.88G [00:01<00:03, 347MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  28%|██▊       | 535M/1.88G [00:01<00:03, 349MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  31%|███       | 577M/1.88G [00:01<00:03, 351MB/s]\u001b[0m\n",
      "\u001b[35m#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  33%|███▎      | 619M/1.88G [00:01<00:03, 350MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  35%|███▌      | 661M/1.88G [00:01<00:03, 346MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  37%|███▋      | 703M/1.88G [00:02<00:03, 351MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  40%|███▉      | 744M/1.88G [00:02<00:03, 354MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  95%|█████████▌| 1.79G/1.88G [00:06<00:00, 315MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin:  98%|█████████▊| 1.84G/1.88G [00:06<00:00, 316MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin: 100%|█████████▉| 1.88G/1.88G [00:06<00:00, 320MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00005-of-00008.bin: 100%|██████████| 1.88G/1.88G [00:06<00:00, 299MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  62%|██████▎   | 5/8 [00:31<00:19,  6.35s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  62%|██████▎   | 5/8 [00:31<00:19,  6.34s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  62%|██████▎   | 5/8 [00:31<00:19,  6.34s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  62%|██████▎   | 5/8 [00:31<00:19,  6.35s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  62%|██████▎   | 5/8 [00:31<00:19,  6.35s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  62%|██████▎   | 5/8 [00:31<00:19,  6.35s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  62%|██████▎   | 5/8 [00:31<00:19,  6.35s/it]#015Downloading shards:  62%|██████▎   | 5/8 [00:31<00:19,  6.35s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   2%|▏         | 41.9M/1.88G [00:00<00:05, 318MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   4%|▍         | 83.9M/1.88G [00:00<00:05, 325MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   7%|▋         | 126M/1.88G [00:00<00:05, 325MB/s] #033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  42%|████▏     | 786M/1.88G [00:02<00:03, 352MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  44%|████▍     | 828M/1.88G [00:02<00:02, 352MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  46%|████▋     | 870M/1.88G [00:02<00:02, 354MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  49%|████▊     | 912M/1.88G [00:02<00:02, 352MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  51%|█████     | 954M/1.88G [00:02<00:02, 350MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  53%|█████▎    | 996M/1.88G [00:02<00:02, 351MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  55%|█████▌    | 1.04G/1.88G [00:03<00:02, 350MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  57%|█████▋    | 1.08G/1.88G [00:03<00:02, 351MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:   9%|▉         | 168M/1.88G [00:00<00:05, 324MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  11%|█         | 210M/1.88G [00:00<00:05, 317MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  13%|█▎        | 252M/1.88G [00:00<00:05, 309MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  15%|█▌        | 283M/1.88G [00:00<00:05, 305MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  17%|█▋        | 315M/1.88G [00:01<00:05, 301MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  18%|█▊        | 346M/1.88G [00:01<00:05, 300MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  20%|██        | 377M/1.88G [00:01<00:05, 296MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  22%|██▏       | 409M/1.88G [00:01<00:04, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  23%|██▎       | 440M/1.88G [00:01<00:04, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  60%|█████▉    | 1.12G/1.88G [00:03<00:02, 350MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  62%|██████▏   | 1.16G/1.88G [00:03<00:02, 352MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  64%|██████▍   | 1.21G/1.88G [00:03<00:01, 353MB/s]\u001b[0m\n",
      "\u001b[35m#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  66%|██████▋   | 1.25G/1.88G [00:03<00:01, 352MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  69%|██████▊   | 1.29G/1.88G [00:03<00:01, 357MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  71%|███████   | 1.33G/1.88G [00:03<00:01, 361MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  73%|███████▎  | 1.37G/1.88G [00:03<00:01, 363MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  75%|███████▌  | 1.42G/1.88G [00:04<00:01, 365MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  78%|███████▊  | 1.46G/1.88G [00:04<00:01, 364MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  25%|██▌       | 472M/1.88G [00:01<00:04, 293MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  27%|██▋       | 503M/1.88G [00:01<00:04, 293MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  28%|██▊       | 535M/1.88G [00:01<00:04, 292MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  30%|███       | 566M/1.88G [00:01<00:04, 290MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  32%|███▏      | 598M/1.88G [00:01<00:04, 287MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  33%|███▎      | 629M/1.88G [00:02<00:04, 290MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  35%|███▌      | 661M/1.88G [00:02<00:04, 290MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  37%|███▋      | 692M/1.88G [00:02<00:04, 290MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  38%|███▊      | 724M/1.88G [00:02<00:04, 289MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  80%|███████▉  | 1.50G/1.88G [00:04<00:01, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  82%|████████▏ | 1.54G/1.88G [00:04<00:00, 365MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  84%|████████▍ | 1.58G/1.88G [00:04<00:00, 364MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  86%|████████▋ | 1.63G/1.88G [00:04<00:00, 364MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  89%|████████▊ | 1.67G/1.88G [00:04<00:00, 364MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  91%|█████████ | 1.71G/1.88G [00:04<00:00, 364MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  93%|█████████▎| 1.75G/1.88G [00:05<00:00, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  95%|█████████▌| 1.79G/1.88G [00:05<00:00, 367MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  40%|████      | 755M/1.88G [00:02<00:03, 290MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  42%|████▏     | 786M/1.88G [00:02<00:03, 292MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  44%|████▎     | 818M/1.88G [00:02<00:03, 293MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  45%|████▌     | 849M/1.88G [00:02<00:03, 294MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  47%|████▋     | 881M/1.88G [00:02<00:03, 294MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  49%|████▊     | 912M/1.88G [00:03<00:03, 296MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  50%|█████     | 944M/1.88G [00:03<00:03, 296MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  52%|█████▏    | 975M/1.88G [00:03<00:03, 296MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  54%|█████▍    | 1.02G/1.88G [00:03<00:02, 311MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin:  98%|█████████▊| 1.84G/1.88G [00:05<00:00, 358MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin: 100%|█████████▉| 1.88G/1.88G [00:05<00:00, 351MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00006-of-00008.bin: 100%|██████████| 1.88G/1.88G [00:05<00:00, 350MB/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:  75%|███████▌  | 6/8 [00:35<00:11,  5.63s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  75%|███████▌  | 6/8 [00:35<00:11,  5.63s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  75%|███████▌  | 6/8 [00:35<00:11,  5.63s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  75%|███████▌  | 6/8 [00:35<00:11,  5.63s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  75%|███████▌  | 6/8 [00:35<00:11,  5.62s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  75%|███████▌  | 6/8 [00:35<00:11,  5.63s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  75%|███████▌  | 6/8 [00:35<00:11,  5.64s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  75%|███████▌  | 6/8 [00:35<00:11,  5.63s/it]\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin:   0%|          | 0.00/1.07G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin:   3%|▎         | 31.5M/1.07G [00:00<00:03, 309MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin:   7%|▋         | 73.4M/1.07G [00:00<00:03, 317MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin:  11%|█         | 115M/1.07G [00:00<00:05, 181MB/s] #033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin:  15%|█▍        | 157M/1.07G [00:00<00:04, 221MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  56%|█████▋    | 1.06G/1.88G [00:03<00:02, 321MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  59%|█████▊    | 1.10G/1.88G [00:03<00:02, 329MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  61%|██████    | 1.14G/1.88G [00:03<00:02, 335MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  63%|██████▎   | 1.18G/1.88G [00:03<00:02, 338MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  65%|██████▌   | 1.23G/1.88G [00:04<00:01, 340MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  67%|██████▋   | 1.27G/1.88G [00:04<00:01, 337MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  70%|██████▉   | 1.31G/1.88G [00:04<00:01, 330MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  72%|███████▏  | 1.35G/1.88G [00:04<00:01, 321MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin:  19%|█▊        | 199M/1.07G [00:00<00:03, 248MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin:  22%|██▏       | 241M/1.07G [00:00<00:03, 270MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin:  26%|██▋       | 283M/1.07G [00:01<00:02, 286MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin:  30%|███       | 325M/1.07G [00:01<00:02, 299MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin:  34%|███▍      | 367M/1.07G [00:01<00:02, 308MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin:  38%|███▊      | 409M/1.07G [00:01<00:02, 313MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin:  42%|████▏     | 451M/1.07G [00:01<00:01, 317MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin:  46%|████▌     | 493M/1.07G [00:01<00:01, 322MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  74%|███████▍  | 1.39G/1.88G [00:04<00:01, 321MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  76%|███████▋  | 1.44G/1.88G [00:04<00:01, 323MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  79%|███████▊  | 1.48G/1.88G [00:04<00:01, 327MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  81%|████████  | 1.52G/1.88G [00:04<00:01, 325MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  83%|████████▎ | 1.56G/1.88G [00:05<00:00, 322MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  85%|████████▌ | 1.60G/1.88G [00:05<00:00, 321MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  88%|████████▊ | 1.65G/1.88G [00:05<00:00, 319MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  90%|████████▉ | 1.69G/1.88G [00:05<00:00, 320MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin:  50%|████▉     | 535M/1.07G [00:01<00:01, 325MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin:  54%|█████▎    | 577M/1.07G [00:01<00:01, 327MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin:  58%|█████▊    | 619M/1.07G [00:02<00:01, 327MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin:  62%|██████▏   | 661M/1.07G [00:02<00:01, 329MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin:  65%|██████▌   | 703M/1.07G [00:02<00:01, 330MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin:  69%|██████▉   | 744M/1.07G [00:02<00:01, 329MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin:  73%|███████▎  | 786M/1.07G [00:02<00:00, 330MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  92%|█████████▏| 1.73G/1.88G [00:05<00:00, 321MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  94%|█████████▍| 1.77G/1.88G [00:05<00:00, 323MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  97%|█████████▋| 1.81G/1.88G [00:05<00:00, 324MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin:  99%|█████████▊| 1.86G/1.88G [00:05<00:00, 319MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00006-of-00008.bin: 100%|██████████| 1.88G/1.88G [00:06<00:00, 311MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 6/8 [00:37<00:12,  6.28s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 6/8 [00:37<00:12,  6.27s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 6/8 [00:37<00:12,  6.28s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 6/8 [00:37<00:12,  6.28s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 6/8 [00:37<00:12,  6.28s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 6/8 [00:37<00:12,  6.29s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 6/8 [00:37<00:12,  6.28s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 6/8 [00:37<00:12,  6.28s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   0%|          | 0.00/1.07G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   2%|▏         | 21.0M/1.07G [00:00<00:05, 198MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   4%|▍         | 41.9M/1.07G [00:00<00:05, 199MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:   8%|▊         | 83.9M/1.07G [00:00<00:03, 264MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin:  77%|███████▋  | 828M/1.07G [00:02<00:00, 330MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin:  81%|████████  | 870M/1.07G [00:02<00:00, 328MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin:  85%|████████▍ | 912M/1.07G [00:03<00:00, 328MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin:  89%|████████▉ | 954M/1.07G [00:03<00:00, 329MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin:  93%|█████████▎| 996M/1.07G [00:03<00:00, 329MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin:  97%|█████████▋| 1.04G/1.07G [00:03<00:00, 329MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin: 100%|██████████| 1.07G/1.07G [00:03<00:00, 334MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00007-of-00008.bin: 100%|██████████| 1.07G/1.07G [00:03<00:00, 308MB/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:  88%|████████▊ | 7/8 [00:38<00:04,  4.96s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  88%|████████▊ | 7/8 [00:38<00:04,  4.95s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  88%|████████▊ | 7/8 [00:38<00:04,  4.95s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  88%|████████▊ | 7/8 [00:38<00:04,  4.96s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  88%|████████▊ | 7/8 [00:38<00:04,  4.96s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  88%|████████▊ | 7/8 [00:38<00:04,  4.96s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  88%|████████▊ | 7/8 [00:38<00:04,  4.96s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  88%|████████▊ | 7/8 [00:38<00:04,  4.97s/it]\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00008-of-00008.bin:   0%|          | 0.00/1.07G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00008-of-00008.bin:   4%|▍         | 41.9M/1.07G [00:00<00:02, 350MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  12%|█▏        | 126M/1.07G [00:00<00:03, 293MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  16%|█▌        | 168M/1.07G [00:00<00:02, 311MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  20%|█▉        | 210M/1.07G [00:00<00:02, 321MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  23%|██▎       | 252M/1.07G [00:00<00:02, 327MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  27%|██▋       | 294M/1.07G [00:00<00:02, 333MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  31%|███       | 336M/1.07G [00:01<00:02, 335MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  35%|███▌      | 377M/1.07G [00:01<00:02, 332MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  39%|███▉      | 419M/1.07G [00:01<00:02, 317MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00008-of-00008.bin:   8%|▊         | 83.9M/1.07G [00:00<00:02, 357MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00008-of-00008.bin:  12%|█▏        | 126M/1.07G [00:00<00:02, 357MB/s] #033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00008-of-00008.bin:  16%|█▌        | 168M/1.07G [00:00<00:02, 358MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00008-of-00008.bin:  20%|█▉        | 210M/1.07G [00:00<00:02, 362MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00008-of-00008.bin:  24%|██▎       | 252M/1.07G [00:00<00:02, 364MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00008-of-00008.bin:  27%|██▋       | 294M/1.07G [00:00<00:02, 365MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00008-of-00008.bin:  31%|███▏      | 336M/1.07G [00:00<00:01, 368MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00008-of-00008.bin:  35%|███▌      | 377M/1.07G [00:01<00:01, 368MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  43%|████▎     | 461M/1.07G [00:01<00:01, 311MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  46%|████▌     | 493M/1.07G [00:01<00:01, 306MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  49%|████▉     | 524M/1.07G [00:01<00:01, 303MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  52%|█████▏    | 556M/1.07G [00:01<00:01, 302MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  55%|█████▍    | 587M/1.07G [00:01<00:01, 301MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  58%|█████▊    | 619M/1.07G [00:02<00:01, 298MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  61%|██████    | 650M/1.07G [00:02<00:01, 296MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  63%|██████▎   | 682M/1.07G [00:02<00:01, 296MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  66%|██████▋   | 713M/1.07G [00:02<00:01, 297MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00008-of-00008.bin:  39%|███▉      | 419M/1.07G [00:01<00:01, 369MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00008-of-00008.bin:  43%|████▎     | 461M/1.07G [00:01<00:01, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00008-of-00008.bin:  47%|████▋     | 503M/1.07G [00:01<00:01, 368MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00008-of-00008.bin:  51%|█████     | 545M/1.07G [00:01<00:01, 369MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00008-of-00008.bin:  55%|█████▍    | 587M/1.07G [00:01<00:01, 369MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00008-of-00008.bin:  59%|█████▉    | 629M/1.07G [00:01<00:01, 367MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00008-of-00008.bin:  63%|██████▎   | 671M/1.07G [00:01<00:01, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00008-of-00008.bin:  67%|██████▋   | 713M/1.07G [00:01<00:00, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00008-of-00008.bin:  71%|███████   | 755M/1.07G [00:02<00:00, 368MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  69%|██████▉   | 744M/1.07G [00:02<00:01, 294MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  72%|███████▏  | 776M/1.07G [00:02<00:01, 296MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  75%|███████▌  | 807M/1.07G [00:02<00:00, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  78%|███████▊  | 839M/1.07G [00:02<00:00, 293MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  81%|████████  | 870M/1.07G [00:02<00:00, 294MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  84%|████████▍ | 902M/1.07G [00:02<00:00, 293MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  87%|████████▋ | 933M/1.07G [00:03<00:00, 293MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  90%|████████▉ | 965M/1.07G [00:03<00:00, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  93%|█████████▎| 996M/1.07G [00:03<00:00, 294MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00008-of-00008.bin:  75%|███████▍  | 797M/1.07G [00:02<00:00, 365MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00008-of-00008.bin:  78%|███████▊  | 839M/1.07G [00:02<00:00, 363MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00008-of-00008.bin:  82%|████████▏ | 881M/1.07G [00:02<00:00, 364MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00008-of-00008.bin:  86%|████████▋ | 923M/1.07G [00:02<00:00, 365MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00008-of-00008.bin:  90%|█████████ | 965M/1.07G [00:02<00:00, 367MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00008-of-00008.bin:  94%|█████████▍| 1.01G/1.07G [00:02<00:00, 365MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00008-of-00008.bin:  98%|█████████▊| 1.05G/1.07G [00:02<00:00, 361MB/s]#033[A\u001b[0m\n",
      "\u001b[35mDownloading (…)l-00008-of-00008.bin: 100%|██████████| 1.07G/1.07G [00:02<00:00, 364MB/s]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 8/8 [00:41<00:00,  4.35s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 8/8 [00:41<00:00,  5.20s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 8/8 [00:41<00:00,  4.35s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 8/8 [00:41<00:00,  5.21s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 8/8 [00:41<00:00,  4.35s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 8/8 [00:41<00:00,  5.21s/it]\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:575] 2023-05-11 10:30:10,455 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"transformers_version\": \"4.28.0\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:575] 2023-05-11 10:30:10,455 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"transformers_version\": \"4.28.0\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 8/8 [00:41<00:00,  4.35s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 8/8 [00:41<00:00,  5.21s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 8/8 [00:41<00:00,  4.35s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 8/8 [00:41<00:00,  5.21s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 8/8 [00:41<00:00,  4.36s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 8/8 [00:41<00:00,  5.21s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 8/8 [00:41<00:00,  4.35s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 8/8 [00:41<00:00,  5.21s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 8/8 [00:41<00:00,  4.36s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 8/8 [00:41<00:00,  5.22s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  96%|█████████▌| 1.03G/1.07G [00:03<00:00, 292MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin:  99%|█████████▊| 1.06G/1.07G [00:03<00:00, 293MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00007-of-00008.bin: 100%|██████████| 1.07G/1.07G [00:03<00:00, 300MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  88%|████████▊ | 7/8 [00:41<00:05,  5.43s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  88%|████████▊ | 7/8 [00:41<00:05,  5.42s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  88%|████████▊ | 7/8 [00:41<00:05,  5.42s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  88%|████████▊ | 7/8 [00:41<00:05,  5.42s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  88%|████████▊ | 7/8 [00:41<00:05,  5.42s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  88%|████████▊ | 7/8 [00:41<00:05,  5.43s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  88%|████████▊ | 7/8 [00:41<00:05,  5.43s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  88%|████████▊ | 7/8 [00:41<00:05,  5.44s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:   0%|          | 0.00/1.07G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:   3%|▎         | 31.5M/1.07G [00:00<00:04, 251MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:   7%|▋         | 73.4M/1.07G [00:00<00:02, 337MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  11%|█         | 115M/1.07G [00:00<00:02, 369MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  15%|█▍        | 157M/1.07G [00:00<00:02, 380MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  19%|█▊        | 199M/1.07G [00:00<00:02, 391MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  23%|██▎       | 241M/1.07G [00:00<00:02, 396MB/s]#033[A\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  26%|██▋       | 283M/1.07G [00:00<00:01, 401MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  30%|███       | 325M/1.07G [00:00<00:01, 404MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  34%|███▍      | 367M/1.07G [00:00<00:01, 407MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  38%|███▊      | 409M/1.07G [00:01<00:01, 409MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  42%|████▏     | 451M/1.07G [00:01<00:01, 409MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  46%|████▌     | 493M/1.07G [00:01<00:01, 410MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  50%|█████     | 535M/1.07G [00:01<00:01, 411MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  54%|█████▍    | 577M/1.07G [00:01<00:01, 410MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  58%|█████▊    | 619M/1.07G [00:01<00:01, 410MB/s]#033[A\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:10,  1.45s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:10,  1.51s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:10,  1.54s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:10,  1.49s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:10,  1.50s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:10,  1.53s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:10,  1.52s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:10,  1.57s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  62%|██████▏   | 661M/1.07G [00:01<00:00, 410MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  66%|██████▌   | 703M/1.07G [00:01<00:00, 396MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  70%|██████▉   | 744M/1.07G [00:01<00:00, 387MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  74%|███████▎  | 786M/1.07G [00:01<00:00, 386MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  77%|███████▋  | 828M/1.07G [00:02<00:00, 384MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  81%|████████▏ | 870M/1.07G [00:02<00:00, 380MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  85%|████████▌ | 912M/1.07G [00:02<00:00, 383MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  89%|████████▉ | 954M/1.07G [00:02<00:00, 387MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  93%|█████████▎| 996M/1.07G [00:02<00:00, 384MB/s]#033[A\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  25%|██▌       | 2/8 [00:03<00:09,  1.51s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin:  97%|█████████▋| 1.04G/1.07G [00:02<00:00, 383MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00008-of-00008.bin: 100%|██████████| 1.07G/1.07G [00:02<00:00, 390MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [00:44<00:00,  4.61s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [00:44<00:00,  5.52s/it]\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:575] 2023-05-11 10:30:13,097 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"transformers_version\": \"4.28.0\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:575] 2023-05-11 10:30:13,097 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 130004,\n",
      "  \"eos_token_id\": 130005,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"transformers_version\": \"4.28.0\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [00:44<00:00,  4.61s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [00:44<00:00,  5.53s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [00:44<00:00,  4.62s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [00:44<00:00,  5.53s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [00:44<00:00,  4.62s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [00:44<00:00,  5.53s/it]#015Downloading shards: 100%|██████████| 8/8 [00:44<00:00,  4.62s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [00:44<00:00,  5.53s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [00:44<00:00,  4.62s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [00:44<00:00,  5.53s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [00:44<00:00,  4.62s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [00:44<00:00,  5.53s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [00:44<00:00,  4.62s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [00:44<00:00,  5.53s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  25%|██▌       | 2/8 [00:03<00:09,  1.55s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  25%|██▌       | 2/8 [00:03<00:09,  1.59s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  25%|██▌       | 2/8 [00:03<00:09,  1.59s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  25%|██▌       | 2/8 [00:03<00:09,  1.58s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  25%|██▌       | 2/8 [00:03<00:09,  1.58s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  25%|██▌       | 2/8 [00:03<00:09,  1.60s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  25%|██▌       | 2/8 [00:03<00:09,  1.62s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:09,  1.40s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:11,  1.59s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:10,  1.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:11,  1.64s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:11,  1.59s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:11,  1.59s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:11,  1.63s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:11,  1.65s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  38%|███▊      | 3/8 [00:04<00:07,  1.60s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  38%|███▊      | 3/8 [00:04<00:08,  1.60s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  38%|███▊      | 3/8 [00:04<00:08,  1.62s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  38%|███▊      | 3/8 [00:04<00:08,  1.64s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  38%|███▊      | 3/8 [00:04<00:08,  1.62s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  38%|███▊      | 3/8 [00:04<00:08,  1.64s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  38%|███▊      | 3/8 [00:04<00:08,  1.66s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  38%|███▊      | 3/8 [00:04<00:08,  1.67s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 2/8 [00:02<00:09,  1.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 2/8 [00:03<00:10,  1.67s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 2/8 [00:03<00:10,  1.68s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 2/8 [00:03<00:10,  1.69s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 2/8 [00:03<00:10,  1.71s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 2/8 [00:03<00:10,  1.68s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 2/8 [00:03<00:10,  1.71s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 2/8 [00:03<00:10,  1.71s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 4/8 [00:06<00:06,  1.61s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 4/8 [00:06<00:06,  1.67s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 4/8 [00:06<00:06,  1.69s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 4/8 [00:06<00:06,  1.69s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 4/8 [00:06<00:06,  1.67s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 4/8 [00:06<00:06,  1.68s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 4/8 [00:06<00:06,  1.70s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 4/8 [00:06<00:06,  1.68s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  38%|███▊      | 3/8 [00:04<00:07,  1.58s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:07<00:04,  1.62s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  38%|███▊      | 3/8 [00:05<00:08,  1.76s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  38%|███▊      | 3/8 [00:05<00:08,  1.77s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  38%|███▊      | 3/8 [00:05<00:08,  1.78s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  38%|███▊      | 3/8 [00:05<00:08,  1.78s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  38%|███▊      | 3/8 [00:05<00:08,  1.80s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  38%|███▊      | 3/8 [00:05<00:08,  1.80s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  38%|███▊      | 3/8 [00:05<00:08,  1.79s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:08<00:05,  1.73s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:08<00:05,  1.73s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:08<00:05,  1.74s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:08<00:05,  1.73s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:08<00:05,  1.74s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:08<00:05,  1.74s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:08<00:05,  1.73s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 4/8 [00:06<00:06,  1.60s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:09<00:03,  1.62s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 4/8 [00:07<00:07,  1.79s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 4/8 [00:07<00:07,  1.80s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 4/8 [00:07<00:07,  1.79s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 4/8 [00:07<00:07,  1.80s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 4/8 [00:07<00:07,  1.80s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 4/8 [00:07<00:07,  1.80s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 4/8 [00:07<00:07,  1.80s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:10<00:03,  1.76s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:10<00:03,  1.76s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:10<00:03,  1.77s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:10<00:03,  1.77s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:10<00:03,  1.77s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:10<00:03,  1.76s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:10<00:03,  1.76s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:10<00:01,  1.42s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:07<00:04,  1.60s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:11<00:01,  1.54s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:11<00:01,  1.53s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:11<00:01,  1.54s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:11<00:01,  1.53s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:11<00:01,  1.55s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:11<00:01,  1.55s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:11<00:01,  1.55s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 8/8 [00:11<00:00,  1.27s/it]#015Loading checkpoint shards: 100%|██████████| 8/8 [00:11<00:00,  1.44s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:08<00:05,  1.80s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:08<00:05,  1.79s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:08<00:05,  1.80s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:08<00:05,  1.80s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:08<00:05,  1.80s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:08<00:05,  1.81s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:08<00:05,  1.81s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:09<00:03,  1.60s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.39s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.55s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.39s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.55s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.39s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.55s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.39s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.55s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.39s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.55s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.39s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.55s/it]\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:3190] 2023-05-11 10:30:23,072 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:3190] 2023-05-11 10:30:23,072 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:3198] 2023-05-11 10:30:23,073 >> All the weights of ChatGLMForConditionalGeneration were initialized from the model checkpoint at THUDM/chatglm-6b.\u001b[0m\n",
      "\u001b[35mIf your task is similar to the task the model of the checkpoint was trained on, you can already use ChatGLMForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:3198] 2023-05-11 10:30:23,073 >> All the weights of ChatGLMForConditionalGeneration were initialized from the model checkpoint at THUDM/chatglm-6b.\u001b[0m\n",
      "\u001b[35mIf your task is similar to the task the model of the checkpoint was trained on, you can already use ChatGLMForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.40s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.55s/it]\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:2839] 2023-05-11 10:30:23,237 >> Generation config file not found, using a generation config created from the model config.\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:2839] 2023-05-11 10:30:23,237 >> Generation config file not found, using a generation config created from the model config.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:10<00:01,  1.37s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:10<00:03,  1.78s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:10<00:03,  1.79s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:10<00:03,  1.79s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:10<00:03,  1.79s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:10<00:03,  1.79s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:10<00:03,  1.79s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:10<00:03,  1.79s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:11<00:00,  1.26s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:11<00:00,  1.43s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:11<00:01,  1.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:11<00:01,  1.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:11<00:01,  1.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:11<00:01,  1.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:11<00:01,  1.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:11<00:01,  1.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:11<00:01,  1.56s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.37s/it]#015Loading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.58s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.37s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3190] 2023-05-11 10:30:25,985 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3190] 2023-05-11 10:30:25,985 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3198] 2023-05-11 10:30:25,985 >> All the weights of ChatGLMForConditionalGeneration were initialized from the model checkpoint at THUDM/chatglm-6b.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use ChatGLMForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3198] 2023-05-11 10:30:25,985 >> All the weights of ChatGLMForConditionalGeneration were initialized from the model checkpoint at THUDM/chatglm-6b.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use ChatGLMForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.37s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.59s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.37s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.58s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.37s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.59s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.37s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.59s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.38s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.59s/it]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2839] 2023-05-11 10:30:26,342 >> Generation config file not found, using a generation config created from the model config.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2839] 2023-05-11 10:30:26,342 >> Generation config file not found, using a generation config created from the model config.\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   0%|          | 0/115 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   1%|          | 1/115 [00:00<01:41,  1.12ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   2%|▏         | 2/115 [00:01<01:37,  1.16ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   3%|▎         | 3/115 [00:02<01:34,  1.18ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   0%|          | 0/115 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   3%|▎         | 4/115 [00:03<01:33,  1.19ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   1%|          | 1/115 [00:00<01:40,  1.14ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   4%|▍         | 5/115 [00:04<01:32,  1.19ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   2%|▏         | 2/115 [00:01<01:35,  1.18ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 3/115 [00:02<01:33,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 4/115 [00:03<01:31,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   4%|▍         | 5/115 [00:04<01:30,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   5%|▌         | 6/115 [00:05<01:31,  1.19ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   6%|▌         | 7/115 [00:05<01:30,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   7%|▋         | 8/115 [00:06<01:29,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   8%|▊         | 9/115 [00:07<01:28,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   5%|▌         | 6/115 [00:04<01:29,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   9%|▊         | 10/115 [00:08<01:27,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   6%|▌         | 7/115 [00:05<01:28,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  10%|▉         | 11/115 [00:09<01:26,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   7%|▋         | 8/115 [00:06<01:27,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  10%|█         | 12/115 [00:10<01:25,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   8%|▊         | 9/115 [00:07<01:26,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  11%|█▏        | 13/115 [00:10<01:24,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  12%|█▏        | 14/115 [00:11<01:23,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   9%|▊         | 10/115 [00:08<01:25,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|▉         | 11/115 [00:09<01:24,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  13%|█▎        | 15/115 [00:12<01:23,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|█         | 12/115 [00:09<01:23,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  14%|█▍        | 16/115 [00:13<01:22,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  11%|█▏        | 13/115 [00:10<01:22,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  15%|█▍        | 17/115 [00:14<01:21,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  12%|█▏        | 14/115 [00:11<01:22,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  13%|█▎        | 15/115 [00:12<01:21,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  14%|█▍        | 16/115 [00:13<01:20,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  15%|█▍        | 17/115 [00:13<01:19,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  16%|█▌        | 18/115 [00:15<01:20,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  17%|█▋        | 19/115 [00:15<01:19,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  17%|█▋        | 20/115 [00:16<01:18,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  18%|█▊        | 21/115 [00:17<01:17,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  16%|█▌        | 18/115 [00:14<01:19,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  19%|█▉        | 22/115 [00:18<01:17,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 19/115 [00:15<01:18,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  20%|██        | 23/115 [00:19<01:16,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 20/115 [00:16<01:17,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  21%|██        | 24/115 [00:20<01:15,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  18%|█▊        | 21/115 [00:17<01:16,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  19%|█▉        | 22/115 [00:18<01:15,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  22%|██▏       | 25/115 [00:20<01:14,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  23%|██▎       | 26/115 [00:21<01:14,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  20%|██        | 23/115 [00:18<01:15,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  23%|██▎       | 27/115 [00:22<01:13,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  21%|██        | 24/115 [00:19<01:14,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  24%|██▍       | 28/115 [00:23<01:12,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  22%|██▏       | 25/115 [00:20<01:13,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  25%|██▌       | 29/115 [00:24<01:11,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 26/115 [00:21<01:12,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 27/115 [00:22<01:12,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  26%|██▌       | 30/115 [00:25<01:10,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  24%|██▍       | 28/115 [00:22<01:11,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  27%|██▋       | 31/115 [00:25<01:10,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  28%|██▊       | 32/115 [00:26<01:09,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  25%|██▌       | 29/115 [00:23<01:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  29%|██▊       | 33/115 [00:27<01:08,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  30%|██▉       | 34/115 [00:28<01:07,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  30%|███       | 35/115 [00:29<01:06,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  26%|██▌       | 30/115 [00:24<01:09,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  27%|██▋       | 31/115 [00:25<01:09,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  28%|██▊       | 32/115 [00:26<01:08,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  29%|██▊       | 33/115 [00:27<01:07,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  31%|███▏      | 36/115 [00:30<01:05,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|██▉       | 34/115 [00:27<01:06,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  32%|███▏      | 37/115 [00:30<01:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  33%|███▎      | 38/115 [00:31<01:04,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|███       | 35/115 [00:28<01:05,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  34%|███▍      | 39/115 [00:32<01:03,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  31%|███▏      | 36/115 [00:29<01:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  35%|███▍      | 40/115 [00:33<01:02,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  32%|███▏      | 37/115 [00:30<01:03,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  33%|███▎      | 38/115 [00:31<01:02,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  36%|███▌      | 41/115 [00:34<01:01,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  34%|███▍      | 39/115 [00:31<01:01,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  37%|███▋      | 42/115 [00:35<01:00,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  35%|███▍      | 40/115 [00:32<01:01,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  37%|███▋      | 43/115 [00:35<00:59,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  38%|███▊      | 44/115 [00:36<00:59,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  36%|███▌      | 41/115 [00:33<01:00,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  39%|███▉      | 45/115 [00:37<00:58,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 42/115 [00:34<00:59,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  40%|████      | 46/115 [00:38<00:57,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 43/115 [00:35<00:58,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  38%|███▊      | 44/115 [00:36<00:57,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  41%|████      | 47/115 [00:39<00:56,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  39%|███▉      | 45/115 [00:36<00:56,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  40%|████      | 46/115 [00:37<00:56,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  41%|████      | 47/115 [00:38<00:55,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  42%|████▏     | 48/115 [00:40<00:55,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  43%|████▎     | 49/115 [00:40<00:54,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  43%|████▎     | 50/115 [00:41<00:54,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  44%|████▍     | 51/115 [00:42<00:53,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  42%|████▏     | 48/115 [00:39<00:54,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 49/115 [00:40<00:53,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  45%|████▌     | 52/115 [00:43<00:52,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 50/115 [00:40<00:52,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  46%|████▌     | 53/115 [00:44<00:51,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  44%|████▍     | 51/115 [00:41<00:52,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  47%|████▋     | 54/115 [00:44<00:50,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  45%|████▌     | 52/115 [00:42<00:51,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  48%|████▊     | 55/115 [00:45<00:49,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  49%|████▊     | 56/115 [00:46<00:49,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  46%|████▌     | 53/115 [00:43<00:50,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  47%|████▋     | 54/115 [00:44<00:49,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  50%|████▉     | 57/115 [00:47<00:48,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  48%|████▊     | 55/115 [00:44<00:48,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  50%|█████     | 58/115 [00:48<00:47,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  49%|████▊     | 56/115 [00:45<00:48,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  51%|█████▏    | 59/115 [00:49<00:46,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|████▉     | 57/115 [00:46<00:47,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|█████     | 58/115 [00:47<00:46,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  51%|█████▏    | 59/115 [00:48<00:45,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  52%|█████▏    | 60/115 [00:49<00:44,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  52%|█████▏    | 60/115 [00:50<00:45,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  53%|█████▎    | 61/115 [00:50<00:45,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  54%|█████▍    | 62/115 [00:51<00:44,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  55%|█████▍    | 63/115 [00:52<00:43,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  53%|█████▎    | 61/115 [00:49<00:43,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  56%|█████▌    | 64/115 [00:53<00:42,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  54%|█████▍    | 62/115 [00:50<00:43,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  57%|█████▋    | 65/115 [00:54<00:41,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  55%|█████▍    | 63/115 [00:51<00:42,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  57%|█████▋    | 66/115 [00:54<00:40,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  56%|█████▌    | 64/115 [00:52<00:41,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 65/115 [00:53<00:40,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  58%|█████▊    | 67/115 [00:55<00:39,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  59%|█████▉    | 68/115 [00:56<00:39,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 66/115 [00:53<00:39,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  60%|██████    | 69/115 [00:57<00:38,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  58%|█████▊    | 67/115 [00:54<00:39,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  61%|██████    | 70/115 [00:58<00:37,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  59%|█████▉    | 68/115 [00:55<00:38,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  62%|██████▏   | 71/115 [00:59<00:36,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  60%|██████    | 69/115 [00:56<00:37,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  63%|██████▎   | 72/115 [00:59<00:35,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  61%|██████    | 70/115 [00:57<00:36,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  62%|██████▏   | 71/115 [00:58<00:35,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  63%|██████▎   | 73/115 [01:00<00:34,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  64%|██████▍   | 74/115 [01:01<00:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 72/115 [00:58<00:35,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  65%|██████▌   | 75/115 [01:02<00:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  66%|██████▌   | 76/115 [01:03<00:32,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  67%|██████▋   | 77/115 [01:04<00:31,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 73/115 [00:59<00:34,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  64%|██████▍   | 74/115 [01:00<00:33,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  65%|██████▌   | 75/115 [01:01<00:32,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  66%|██████▌   | 76/115 [01:02<00:31,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  68%|██████▊   | 78/115 [01:04<00:30,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  67%|██████▋   | 77/115 [01:02<00:30,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  69%|██████▊   | 79/115 [01:05<00:29,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  70%|██████▉   | 80/115 [01:06<00:29,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  68%|██████▊   | 78/115 [01:03<00:30,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  70%|███████   | 81/115 [01:07<00:28,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  69%|██████▊   | 79/115 [01:04<00:29,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  71%|███████▏  | 82/115 [01:08<00:27,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|██████▉   | 80/115 [01:05<00:28,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|███████   | 81/115 [01:06<00:27,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  72%|███████▏  | 83/115 [01:09<00:26,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  71%|███████▏  | 82/115 [01:06<00:26,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  73%|███████▎  | 84/115 [01:09<00:25,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  72%|███████▏  | 83/115 [01:07<00:26,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  74%|███████▍  | 85/115 [01:10<00:25,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  75%|███████▍  | 86/115 [01:11<00:24,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  73%|███████▎  | 84/115 [01:08<00:25,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  76%|███████▌  | 87/115 [01:12<00:23,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  74%|███████▍  | 85/115 [01:09<00:24,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  77%|███████▋  | 88/115 [01:13<00:22,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  75%|███████▍  | 86/115 [01:10<00:23,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  76%|███████▌  | 87/115 [01:11<00:22,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  77%|███████▋  | 89/115 [01:14<00:21,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 88/115 [01:11<00:22,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 89/115 [01:12<00:21,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  78%|███████▊  | 90/115 [01:13<00:20,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  78%|███████▊  | 90/115 [01:14<00:20,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  79%|███████▉  | 91/115 [01:15<00:20,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  80%|████████  | 92/115 [01:16<00:19,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  81%|████████  | 93/115 [01:17<00:18,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  79%|███████▉  | 91/115 [01:14<00:19,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  80%|████████  | 92/115 [01:15<00:18,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  82%|████████▏ | 94/115 [01:18<00:17,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  81%|████████  | 93/115 [01:15<00:18,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  83%|████████▎ | 95/115 [01:19<00:16,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  82%|████████▏ | 94/115 [01:16<00:17,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  83%|████████▎ | 96/115 [01:19<00:15,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 95/115 [01:17<00:16,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  84%|████████▍ | 97/115 [01:20<00:15,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  85%|████████▌ | 98/115 [01:21<00:14,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 96/115 [01:18<00:15,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  86%|████████▌ | 99/115 [01:22<00:13,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  84%|████████▍ | 97/115 [01:19<00:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  85%|████████▌ | 98/115 [01:20<00:13,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  87%|████████▋ | 100/115 [01:23<00:12,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  86%|████████▌ | 99/115 [01:20<00:13,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  88%|████████▊ | 101/115 [01:24<00:11,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  87%|████████▋ | 100/115 [01:21<00:12,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  89%|████████▊ | 102/115 [01:24<00:10,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  88%|████████▊ | 101/115 [01:22<00:11,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  90%|████████▉ | 103/115 [01:25<00:10,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  90%|█████████ | 104/115 [01:26<00:09,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  89%|████████▊ | 102/115 [01:23<00:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  91%|█████████▏| 105/115 [01:27<00:08,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  92%|█████████▏| 106/115 [01:28<00:07,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  93%|█████████▎| 107/115 [01:29<00:06,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|████████▉ | 103/115 [01:24<00:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|█████████ | 104/115 [01:25<00:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  91%|█████████▏| 105/115 [01:25<00:08,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  92%|█████████▏| 106/115 [01:26<00:07,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  94%|█████████▍| 108/115 [01:29<00:05,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  93%|█████████▎| 107/115 [01:27<00:06,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  95%|█████████▍| 109/115 [01:30<00:04,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  96%|█████████▌| 110/115 [01:31<00:04,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  94%|█████████▍| 108/115 [01:28<00:05,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  95%|█████████▍| 109/115 [01:29<00:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  97%|█████████▋| 111/115 [01:32<00:03,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  96%|█████████▌| 110/115 [01:29<00:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  97%|█████████▋| 112/115 [01:33<00:02,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 111/115 [01:30<00:03,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  98%|█████████▊| 113/115 [01:34<00:01,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 112/115 [01:31<00:02,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  99%|█████████▉| 114/115 [01:34<00:00,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:35<00:00,  1.37ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:35<00:00,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  98%|█████████▊| 113/115 [01:32<00:01,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  99%|█████████▉| 114/115 [01:33<00:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:33<00:00,  1.39ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:33<00:00,  1.23ba/s]\u001b[0m\n",
      "\u001b[35minput_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\u001b[0m\n",
      "\u001b[35minputs 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[35mlabel_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[35mlabels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   0%|          | 0/115 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   0%|          | 0/115 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   0%|          | 0/115 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   0%|          | 0/115 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   0%|          | 0/115 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   0%|          | 0/115 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   0%|          | 0/115 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   1%|          | 1/115 [00:00<01:34,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   1%|          | 1/115 [00:00<01:34,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   1%|          | 1/115 [00:00<01:35,  1.19ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   1%|          | 1/115 [00:00<01:36,  1.19ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   1%|          | 1/115 [00:00<01:35,  1.19ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   1%|          | 1/115 [00:00<01:36,  1.18ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   1%|          | 1/115 [00:00<01:36,  1.18ba/s]\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34minput_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\u001b[0m\n",
      "\u001b[34minputs 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[34mlabel_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[34mlabels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:32:10,312] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed info: version=0.8.3, git-hash=unknown, git-branch=unknown\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   0%|          | 0/115 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   0%|          | 0/115 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   0%|          | 0/115 [00:00<?, ?ba/s]#015Running tokenizer on train dataset:   0%|          | 0/115 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   0%|          | 0/115 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   0%|          | 0/115 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   0%|          | 0/115 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   1%|          | 1/115 [00:00<01:35,  1.19ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   1%|          | 1/115 [00:00<01:35,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   1%|          | 1/115 [00:00<01:35,  1.19ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   1%|          | 1/115 [00:00<01:36,  1.18ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   1%|          | 1/115 [00:00<01:35,  1.19ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   1%|          | 1/115 [00:00<01:36,  1.18ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   1%|          | 1/115 [00:00<01:36,  1.18ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   2%|▏         | 2/115 [00:01<01:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   2%|▏         | 2/115 [00:01<01:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   2%|▏         | 2/115 [00:01<01:34,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   2%|▏         | 2/115 [00:01<01:33,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   2%|▏         | 2/115 [00:01<01:34,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   2%|▏         | 2/115 [00:01<01:34,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   2%|▏         | 2/115 [00:01<01:34,  1.19ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   2%|▏         | 2/115 [00:01<01:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   2%|▏         | 2/115 [00:01<01:34,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   2%|▏         | 2/115 [00:01<01:34,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   2%|▏         | 2/115 [00:01<01:34,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   2%|▏         | 2/115 [00:01<01:34,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   2%|▏         | 2/115 [00:01<01:34,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   2%|▏         | 2/115 [00:01<01:34,  1.19ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 3/115 [00:02<01:32,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 3/115 [00:02<01:32,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 3/115 [00:02<01:32,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 3/115 [00:02<01:32,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 3/115 [00:02<01:33,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 3/115 [00:02<01:33,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 3/115 [00:02<01:33,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   3%|▎         | 3/115 [00:02<01:32,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   3%|▎         | 3/115 [00:02<01:32,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   3%|▎         | 3/115 [00:02<01:33,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   3%|▎         | 3/115 [00:02<01:33,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   3%|▎         | 3/115 [00:02<01:33,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   3%|▎         | 3/115 [00:02<01:33,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   3%|▎         | 3/115 [00:02<01:33,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   3%|▎         | 4/115 [00:03<01:31,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   3%|▎         | 4/115 [00:03<01:31,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   3%|▎         | 4/115 [00:03<01:32,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   3%|▎         | 4/115 [00:03<01:32,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   3%|▎         | 4/115 [00:03<01:32,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   3%|▎         | 4/115 [00:03<01:32,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   3%|▎         | 4/115 [00:03<01:32,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 4/115 [00:03<01:31,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 4/115 [00:03<01:31,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 4/115 [00:03<01:31,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 4/115 [00:03<01:32,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 4/115 [00:03<01:31,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 4/115 [00:03<01:32,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 4/115 [00:03<01:32,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   4%|▍         | 5/115 [00:04<01:30,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   4%|▍         | 5/115 [00:04<01:30,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   4%|▍         | 5/115 [00:04<01:31,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   4%|▍         | 5/115 [00:04<01:31,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   4%|▍         | 5/115 [00:04<01:31,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   4%|▍         | 5/115 [00:04<01:31,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   4%|▍         | 5/115 [00:04<01:31,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   4%|▍         | 5/115 [00:04<01:30,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   4%|▍         | 5/115 [00:04<01:30,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   4%|▍         | 5/115 [00:04<01:30,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   4%|▍         | 5/115 [00:04<01:31,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   4%|▍         | 5/115 [00:04<01:31,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   4%|▍         | 5/115 [00:04<01:31,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   4%|▍         | 5/115 [00:04<01:31,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   5%|▌         | 6/115 [00:04<01:29,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   5%|▌         | 6/115 [00:04<01:30,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   5%|▌         | 6/115 [00:04<01:30,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   5%|▌         | 6/115 [00:04<01:30,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   5%|▌         | 6/115 [00:04<01:30,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   5%|▌         | 6/115 [00:04<01:30,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   5%|▌         | 6/115 [00:05<01:30,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   5%|▌         | 6/115 [00:04<01:29,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   5%|▌         | 6/115 [00:04<01:29,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   5%|▌         | 6/115 [00:04<01:30,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   5%|▌         | 6/115 [00:04<01:30,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   5%|▌         | 6/115 [00:04<01:30,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   5%|▌         | 6/115 [00:04<01:30,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   5%|▌         | 6/115 [00:04<01:30,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   6%|▌         | 7/115 [00:05<01:28,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   6%|▌         | 7/115 [00:05<01:29,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   6%|▌         | 7/115 [00:05<01:29,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   6%|▌         | 7/115 [00:05<01:29,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   6%|▌         | 7/115 [00:05<01:29,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   6%|▌         | 7/115 [00:05<01:29,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   6%|▌         | 7/115 [00:05<01:29,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   6%|▌         | 7/115 [00:05<01:28,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   6%|▌         | 7/115 [00:05<01:28,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   6%|▌         | 7/115 [00:05<01:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   6%|▌         | 7/115 [00:05<01:29,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   6%|▌         | 7/115 [00:05<01:29,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   6%|▌         | 7/115 [00:05<01:29,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   6%|▌         | 7/115 [00:05<01:29,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   7%|▋         | 8/115 [00:06<01:27,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   7%|▋         | 8/115 [00:06<01:27,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   7%|▋         | 8/115 [00:06<01:27,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   7%|▋         | 8/115 [00:06<01:27,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   7%|▋         | 8/115 [00:06<01:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   7%|▋         | 8/115 [00:06<01:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   7%|▋         | 8/115 [00:06<01:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   7%|▋         | 8/115 [00:06<01:27,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   7%|▋         | 8/115 [00:06<01:28,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   7%|▋         | 8/115 [00:06<01:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   7%|▋         | 8/115 [00:06<01:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   7%|▋         | 8/115 [00:06<01:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   7%|▋         | 8/115 [00:06<01:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   7%|▋         | 8/115 [00:06<01:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   8%|▊         | 9/115 [00:07<01:26,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   8%|▊         | 9/115 [00:07<01:26,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   8%|▊         | 9/115 [00:07<01:27,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   8%|▊         | 9/115 [00:07<01:26,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   8%|▊         | 9/115 [00:07<01:27,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   8%|▊         | 9/115 [00:07<01:27,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   8%|▊         | 9/115 [00:07<01:27,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   8%|▊         | 9/115 [00:07<01:26,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   8%|▊         | 9/115 [00:07<01:27,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   8%|▊         | 9/115 [00:07<01:27,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   8%|▊         | 9/115 [00:07<01:27,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   8%|▊         | 9/115 [00:07<01:27,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   8%|▊         | 9/115 [00:07<01:27,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   8%|▊         | 9/115 [00:07<01:28,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   9%|▊         | 10/115 [00:08<01:25,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   9%|▊         | 10/115 [00:08<01:26,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   9%|▊         | 10/115 [00:08<01:26,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   9%|▊         | 10/115 [00:08<01:26,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   9%|▊         | 10/115 [00:08<01:26,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   9%|▊         | 10/115 [00:08<01:26,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:   9%|▊         | 10/115 [00:08<01:27,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   9%|▊         | 10/115 [00:08<01:25,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   9%|▊         | 10/115 [00:08<01:25,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   9%|▊         | 10/115 [00:08<01:26,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   9%|▊         | 10/115 [00:08<01:25,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   9%|▊         | 10/115 [00:08<01:26,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   9%|▊         | 10/115 [00:08<01:26,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   9%|▊         | 10/115 [00:08<01:26,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  10%|▉         | 11/115 [00:09<01:25,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  10%|▉         | 11/115 [00:09<01:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  10%|▉         | 11/115 [00:09<01:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  10%|▉         | 11/115 [00:09<01:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  10%|▉         | 11/115 [00:09<01:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  10%|▉         | 11/115 [00:09<01:26,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  10%|▉         | 11/115 [00:09<01:26,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|▉         | 11/115 [00:09<01:24,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|▉         | 11/115 [00:09<01:25,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|▉         | 11/115 [00:09<01:24,  1.22ba/s]#015Running tokenizer on train dataset:  10%|▉         | 11/115 [00:09<01:25,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|▉         | 11/115 [00:09<01:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|▉         | 11/115 [00:09<01:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|▉         | 11/115 [00:09<01:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  10%|█         | 12/115 [00:09<01:24,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  10%|█         | 12/115 [00:09<01:24,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  10%|█         | 12/115 [00:09<01:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  10%|█         | 12/115 [00:09<01:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  10%|█         | 12/115 [00:09<01:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  10%|█         | 12/115 [00:09<01:25,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  10%|█         | 12/115 [00:09<01:25,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|█         | 12/115 [00:09<01:24,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|█         | 12/115 [00:09<01:24,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|█         | 12/115 [00:09<01:24,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|█         | 12/115 [00:09<01:24,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|█         | 12/115 [00:09<01:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|█         | 12/115 [00:09<01:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|█         | 12/115 [00:09<01:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  11%|█▏        | 13/115 [00:10<01:23,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  11%|█▏        | 13/115 [00:10<01:23,  1.22ba/s]#015Running tokenizer on train dataset:  11%|█▏        | 13/115 [00:10<01:23,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  11%|█▏        | 13/115 [00:10<01:23,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  11%|█▏        | 13/115 [00:10<01:23,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  11%|█▏        | 13/115 [00:10<01:24,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  11%|█▏        | 13/115 [00:10<01:24,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  11%|█▏        | 13/115 [00:10<01:24,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  11%|█▏        | 13/115 [00:10<01:24,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  11%|█▏        | 13/115 [00:10<01:24,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  11%|█▏        | 13/115 [00:10<01:24,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  11%|█▏        | 13/115 [00:10<01:24,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  11%|█▏        | 13/115 [00:10<01:24,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  11%|█▏        | 13/115 [00:10<01:24,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  12%|█▏        | 14/115 [00:11<01:22,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  12%|█▏        | 14/115 [00:11<01:23,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  12%|█▏        | 14/115 [00:11<01:22,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  12%|█▏        | 14/115 [00:11<01:23,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  12%|█▏        | 14/115 [00:11<01:23,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  12%|█▏        | 14/115 [00:11<01:23,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  12%|█▏        | 14/115 [00:11<01:23,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  13%|█▎        | 15/115 [00:12<01:22,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  13%|█▎        | 15/115 [00:12<01:22,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  13%|█▎        | 15/115 [00:12<01:22,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  13%|█▎        | 15/115 [00:12<01:22,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  13%|█▎        | 15/115 [00:12<01:22,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  13%|█▎        | 15/115 [00:12<01:22,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  13%|█▎        | 15/115 [00:12<01:23,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  14%|█▍        | 16/115 [00:13<01:21,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  14%|█▍        | 16/115 [00:13<01:20,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  14%|█▍        | 16/115 [00:13<01:21,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  14%|█▍        | 16/115 [00:13<01:21,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  14%|█▍        | 16/115 [00:13<01:21,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  14%|█▍        | 16/115 [00:13<01:21,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  14%|█▍        | 16/115 [00:13<01:22,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  12%|█▏        | 14/115 [00:11<01:22,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  12%|█▏        | 14/115 [00:11<01:23,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  12%|█▏        | 14/115 [00:11<01:23,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  12%|█▏        | 14/115 [00:11<01:23,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  12%|█▏        | 14/115 [00:11<01:23,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  12%|█▏        | 14/115 [00:11<01:24,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  12%|█▏        | 14/115 [00:11<01:24,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  13%|█▎        | 15/115 [00:12<01:21,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  13%|█▎        | 15/115 [00:12<01:22,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  13%|█▎        | 15/115 [00:12<01:22,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  13%|█▎        | 15/115 [00:12<01:22,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  13%|█▎        | 15/115 [00:12<01:22,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  13%|█▎        | 15/115 [00:12<01:23,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  13%|█▎        | 15/115 [00:12<01:23,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  14%|█▍        | 16/115 [00:13<01:20,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  14%|█▍        | 16/115 [00:13<01:21,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  14%|█▍        | 16/115 [00:13<01:21,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  14%|█▍        | 16/115 [00:13<01:21,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  14%|█▍        | 16/115 [00:13<01:21,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  14%|█▍        | 16/115 [00:13<01:22,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  14%|█▍        | 16/115 [00:13<01:22,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  15%|█▍        | 17/115 [00:13<01:20,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  15%|█▍        | 17/115 [00:14<01:20,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  15%|█▍        | 17/115 [00:14<01:20,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  15%|█▍        | 17/115 [00:14<01:20,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  15%|█▍        | 17/115 [00:14<01:20,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  15%|█▍        | 17/115 [00:14<01:21,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  15%|█▍        | 17/115 [00:14<01:21,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  15%|█▍        | 17/115 [00:13<01:20,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  15%|█▍        | 17/115 [00:13<01:20,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  15%|█▍        | 17/115 [00:13<01:20,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  15%|█▍        | 17/115 [00:13<01:20,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  15%|█▍        | 17/115 [00:14<01:20,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  15%|█▍        | 17/115 [00:14<01:20,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  15%|█▍        | 17/115 [00:14<01:21,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  16%|█▌        | 18/115 [00:14<01:19,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  16%|█▌        | 18/115 [00:14<01:19,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  16%|█▌        | 18/115 [00:14<01:20,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  16%|█▌        | 18/115 [00:14<01:20,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  16%|█▌        | 18/115 [00:14<01:20,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  16%|█▌        | 18/115 [00:14<01:20,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  16%|█▌        | 18/115 [00:14<01:20,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  16%|█▌        | 18/115 [00:14<01:19,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  16%|█▌        | 18/115 [00:14<01:19,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  16%|█▌        | 18/115 [00:14<01:19,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  16%|█▌        | 18/115 [00:14<01:19,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  16%|█▌        | 18/115 [00:14<01:19,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  16%|█▌        | 18/115 [00:14<01:19,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  16%|█▌        | 18/115 [00:14<01:20,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 19/115 [00:15<01:18,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 19/115 [00:15<01:18,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 19/115 [00:15<01:18,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 19/115 [00:15<01:18,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 19/115 [00:15<01:18,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 19/115 [00:15<01:19,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  17%|█▋        | 19/115 [00:15<01:18,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  17%|█▋        | 19/115 [00:15<01:19,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  17%|█▋        | 19/115 [00:15<01:19,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  17%|█▋        | 19/115 [00:15<01:19,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  17%|█▋        | 19/115 [00:15<01:19,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  17%|█▋        | 19/115 [00:15<01:19,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  17%|█▋        | 19/115 [00:15<01:19,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 19/115 [00:15<01:19,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 20/115 [00:16<01:17,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 20/115 [00:16<01:17,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 20/115 [00:16<01:17,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 20/115 [00:16<01:17,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 20/115 [00:16<01:18,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 20/115 [00:16<01:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 20/115 [00:16<01:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  17%|█▋        | 20/115 [00:16<01:17,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  17%|█▋        | 20/115 [00:16<01:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  17%|█▋        | 20/115 [00:16<01:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  17%|█▋        | 20/115 [00:16<01:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  17%|█▋        | 20/115 [00:16<01:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  17%|█▋        | 20/115 [00:16<01:19,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  17%|█▋        | 20/115 [00:16<01:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  18%|█▊        | 21/115 [00:17<01:16,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  18%|█▊        | 21/115 [00:17<01:17,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  18%|█▊        | 21/115 [00:17<01:17,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  18%|█▊        | 21/115 [00:17<01:16,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  18%|█▊        | 21/115 [00:17<01:16,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  18%|█▊        | 21/115 [00:17<01:17,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  18%|█▊        | 21/115 [00:17<01:16,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  18%|█▊        | 21/115 [00:17<01:17,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  18%|█▊        | 21/115 [00:17<01:17,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  18%|█▊        | 21/115 [00:17<01:17,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  18%|█▊        | 21/115 [00:17<01:17,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  18%|█▊        | 21/115 [00:17<01:17,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  18%|█▊        | 21/115 [00:17<01:17,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  18%|█▊        | 21/115 [00:17<01:17,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  19%|█▉        | 22/115 [00:18<01:15,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  19%|█▉        | 22/115 [00:18<01:16,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  19%|█▉        | 22/115 [00:18<01:16,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  19%|█▉        | 22/115 [00:18<01:16,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  19%|█▉        | 22/115 [00:18<01:16,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  19%|█▉        | 22/115 [00:18<01:17,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  19%|█▉        | 22/115 [00:18<01:17,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  19%|█▉        | 22/115 [00:18<01:16,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  19%|█▉        | 22/115 [00:18<01:15,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  19%|█▉        | 22/115 [00:18<01:16,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  19%|█▉        | 22/115 [00:18<01:16,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  19%|█▉        | 22/115 [00:18<01:16,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  19%|█▉        | 22/115 [00:18<01:16,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  19%|█▉        | 22/115 [00:18<01:16,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  20%|██        | 23/115 [00:18<01:15,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  20%|██        | 23/115 [00:18<01:15,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  20%|██        | 23/115 [00:19<01:16,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  20%|██        | 23/115 [00:19<01:16,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  20%|██        | 23/115 [00:19<01:16,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  20%|██        | 23/115 [00:19<01:16,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  20%|██        | 23/115 [00:19<01:16,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  20%|██        | 23/115 [00:18<01:15,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  20%|██        | 23/115 [00:18<01:15,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  20%|██        | 23/115 [00:18<01:15,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  20%|██        | 23/115 [00:18<01:15,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  20%|██        | 23/115 [00:18<01:15,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  20%|██        | 23/115 [00:19<01:15,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  20%|██        | 23/115 [00:19<01:16,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  21%|██        | 24/115 [00:19<01:14,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  21%|██        | 24/115 [00:19<01:14,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  21%|██        | 24/115 [00:19<01:14,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  21%|██        | 24/115 [00:19<01:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  21%|██        | 24/115 [00:19<01:15,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  21%|██        | 24/115 [00:19<01:15,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  21%|██        | 24/115 [00:19<01:15,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  21%|██        | 24/115 [00:19<01:15,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  21%|██        | 24/115 [00:19<01:15,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  21%|██        | 24/115 [00:19<01:14,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  21%|██        | 24/115 [00:19<01:15,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  21%|██        | 24/115 [00:19<01:14,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  21%|██        | 24/115 [00:19<01:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  21%|██        | 24/115 [00:19<01:15,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  22%|██▏       | 25/115 [00:20<01:13,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  22%|██▏       | 25/115 [00:20<01:13,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  22%|██▏       | 25/115 [00:20<01:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  22%|██▏       | 25/115 [00:20<01:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  22%|██▏       | 25/115 [00:20<01:14,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  22%|██▏       | 25/115 [00:20<01:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  22%|██▏       | 25/115 [00:20<01:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  22%|██▏       | 25/115 [00:20<01:13,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  22%|██▏       | 25/115 [00:20<01:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  22%|██▏       | 25/115 [00:20<01:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  22%|██▏       | 25/115 [00:20<01:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  22%|██▏       | 25/115 [00:20<01:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  22%|██▏       | 25/115 [00:20<01:14,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  22%|██▏       | 25/115 [00:20<01:15,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  23%|██▎       | 26/115 [00:21<01:12,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 26/115 [00:21<01:12,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 26/115 [00:21<01:13,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 26/115 [00:21<01:13,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 26/115 [00:21<01:13,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 26/115 [00:21<01:13,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 26/115 [00:21<01:13,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 26/115 [00:21<01:13,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  23%|██▎       | 26/115 [00:21<01:13,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  23%|██▎       | 26/115 [00:21<01:13,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  23%|██▎       | 26/115 [00:21<01:13,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  23%|██▎       | 26/115 [00:21<01:13,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  23%|██▎       | 26/115 [00:21<01:13,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  23%|██▎       | 26/115 [00:21<01:14,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  23%|██▎       | 27/115 [00:22<01:11,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  23%|██▎       | 27/115 [00:22<01:12,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  23%|██▎       | 27/115 [00:22<01:12,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  23%|██▎       | 27/115 [00:22<01:12,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  23%|██▎       | 27/115 [00:22<01:12,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 27/115 [00:22<01:12,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 27/115 [00:22<01:12,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 27/115 [00:22<01:12,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 27/115 [00:22<01:12,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 27/115 [00:22<01:12,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 27/115 [00:22<01:12,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 27/115 [00:22<01:12,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  23%|██▎       | 27/115 [00:22<01:13,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  23%|██▎       | 27/115 [00:22<01:13,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  24%|██▍       | 28/115 [00:22<01:11,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  24%|██▍       | 28/115 [00:23<01:11,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  24%|██▍       | 28/115 [00:23<01:12,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  24%|██▍       | 28/115 [00:23<01:12,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  24%|██▍       | 28/115 [00:23<01:12,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  24%|██▍       | 28/115 [00:23<01:12,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  24%|██▍       | 28/115 [00:23<01:12,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  24%|██▍       | 28/115 [00:22<01:11,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  24%|██▍       | 28/115 [00:22<01:11,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  24%|██▍       | 28/115 [00:23<01:11,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  24%|██▍       | 28/115 [00:23<01:11,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  24%|██▍       | 28/115 [00:23<01:11,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  24%|██▍       | 28/115 [00:23<01:11,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  24%|██▍       | 28/115 [00:23<01:12,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  25%|██▌       | 29/115 [00:23<01:10,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  25%|██▌       | 29/115 [00:23<01:11,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  25%|██▌       | 29/115 [00:23<01:11,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  25%|██▌       | 29/115 [00:23<01:11,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  25%|██▌       | 29/115 [00:24<01:11,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  25%|██▌       | 29/115 [00:24<01:11,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  25%|██▌       | 29/115 [00:24<01:11,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  26%|██▌       | 30/115 [00:24<01:09,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  26%|██▌       | 30/115 [00:24<01:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  26%|██▌       | 30/115 [00:24<01:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  26%|██▌       | 30/115 [00:24<01:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  26%|██▌       | 30/115 [00:24<01:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  26%|██▌       | 30/115 [00:24<01:10,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  26%|██▌       | 30/115 [00:24<01:10,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  27%|██▋       | 31/115 [00:25<01:08,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  27%|██▋       | 31/115 [00:25<01:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  27%|██▋       | 31/115 [00:25<01:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  27%|██▋       | 31/115 [00:25<01:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  27%|██▋       | 31/115 [00:25<01:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  27%|██▋       | 31/115 [00:25<01:09,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  27%|██▋       | 31/115 [00:25<01:10,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  28%|██▊       | 32/115 [00:26<01:07,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  25%|██▌       | 29/115 [00:23<01:10,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  25%|██▌       | 29/115 [00:23<01:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  25%|██▌       | 29/115 [00:23<01:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  25%|██▌       | 29/115 [00:23<01:11,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  25%|██▌       | 29/115 [00:23<01:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  25%|██▌       | 29/115 [00:23<01:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  25%|██▌       | 29/115 [00:24<01:11,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  26%|██▌       | 30/115 [00:24<01:09,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  26%|██▌       | 30/115 [00:24<01:09,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  26%|██▌       | 30/115 [00:24<01:09,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  26%|██▌       | 30/115 [00:24<01:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  26%|██▌       | 30/115 [00:24<01:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  26%|██▌       | 30/115 [00:24<01:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  26%|██▌       | 30/115 [00:24<01:10,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  27%|██▋       | 31/115 [00:25<01:08,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  27%|██▋       | 31/115 [00:25<01:08,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  27%|██▋       | 31/115 [00:25<01:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  27%|██▋       | 31/115 [00:25<01:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  27%|██▋       | 31/115 [00:25<01:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  27%|██▋       | 31/115 [00:25<01:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  27%|██▋       | 31/115 [00:25<01:09,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  28%|██▊       | 32/115 [00:26<01:08,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  28%|██▊       | 32/115 [00:26<01:08,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  28%|██▊       | 32/115 [00:26<01:08,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  28%|██▊       | 32/115 [00:26<01:08,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  28%|██▊       | 32/115 [00:26<01:08,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  28%|██▊       | 32/115 [00:26<01:08,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  28%|██▊       | 32/115 [00:26<01:09,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  28%|██▊       | 32/115 [00:26<01:08,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  28%|██▊       | 32/115 [00:26<01:08,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  28%|██▊       | 32/115 [00:26<01:08,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  28%|██▊       | 32/115 [00:26<01:08,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  28%|██▊       | 32/115 [00:26<01:09,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  28%|██▊       | 32/115 [00:26<01:09,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  29%|██▊       | 33/115 [00:27<01:07,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  29%|██▊       | 33/115 [00:27<01:07,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  29%|██▊       | 33/115 [00:27<01:07,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  29%|██▊       | 33/115 [00:27<01:08,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  29%|██▊       | 33/115 [00:27<01:07,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  29%|██▊       | 33/115 [00:27<01:07,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  29%|██▊       | 33/115 [00:27<01:07,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  29%|██▊       | 33/115 [00:27<01:07,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  29%|██▊       | 33/115 [00:27<01:07,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  29%|██▊       | 33/115 [00:27<01:07,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  29%|██▊       | 33/115 [00:27<01:07,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  29%|██▊       | 33/115 [00:27<01:08,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  29%|██▊       | 33/115 [00:27<01:08,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  29%|██▊       | 33/115 [00:27<01:08,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  30%|██▉       | 34/115 [00:27<01:06,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  30%|██▉       | 34/115 [00:28<01:06,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  30%|██▉       | 34/115 [00:28<01:07,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  30%|██▉       | 34/115 [00:28<01:07,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  30%|██▉       | 34/115 [00:28<01:07,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  30%|██▉       | 34/115 [00:28<01:07,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  30%|██▉       | 34/115 [00:28<01:07,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|██▉       | 34/115 [00:27<01:06,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|██▉       | 34/115 [00:27<01:06,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|██▉       | 34/115 [00:27<01:06,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|██▉       | 34/115 [00:27<01:06,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|██▉       | 34/115 [00:28<01:06,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|██▉       | 34/115 [00:28<01:06,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|██▉       | 34/115 [00:28<01:07,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|███       | 35/115 [00:28<01:05,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|███       | 35/115 [00:28<01:05,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  30%|███       | 35/115 [00:28<01:05,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  30%|███       | 35/115 [00:28<01:05,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  30%|███       | 35/115 [00:28<01:06,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  30%|███       | 35/115 [00:28<01:06,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  30%|███       | 35/115 [00:28<01:06,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  30%|███       | 35/115 [00:29<01:06,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  30%|███       | 35/115 [00:29<01:06,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|███       | 35/115 [00:28<01:05,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|███       | 35/115 [00:28<01:05,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|███       | 35/115 [00:28<01:05,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|███       | 35/115 [00:28<01:05,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|███       | 35/115 [00:29<01:06,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  31%|███▏      | 36/115 [00:29<01:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  31%|███▏      | 36/115 [00:29<01:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  31%|███▏      | 36/115 [00:29<01:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  31%|███▏      | 36/115 [00:29<01:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  31%|███▏      | 36/115 [00:29<01:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  31%|███▏      | 36/115 [00:29<01:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  31%|███▏      | 36/115 [00:29<01:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  31%|███▏      | 36/115 [00:29<01:05,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  31%|███▏      | 36/115 [00:29<01:05,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  31%|███▏      | 36/115 [00:29<01:05,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  31%|███▏      | 36/115 [00:29<01:05,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  31%|███▏      | 36/115 [00:29<01:05,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  31%|███▏      | 36/115 [00:29<01:05,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  32%|███▏      | 37/115 [00:30<01:03,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  31%|███▏      | 36/115 [00:29<01:05,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  32%|███▏      | 37/115 [00:30<01:03,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  32%|███▏      | 37/115 [00:30<01:03,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  32%|███▏      | 37/115 [00:30<01:03,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  32%|███▏      | 37/115 [00:30<01:03,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  32%|███▏      | 37/115 [00:30<01:03,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  32%|███▏      | 37/115 [00:30<01:03,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  32%|███▏      | 37/115 [00:30<01:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  32%|███▏      | 37/115 [00:30<01:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  32%|███▏      | 37/115 [00:30<01:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  32%|███▏      | 37/115 [00:30<01:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  32%|███▏      | 37/115 [00:30<01:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  32%|███▏      | 37/115 [00:30<01:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  32%|███▏      | 37/115 [00:30<01:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  33%|███▎      | 38/115 [00:31<01:02,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  33%|███▎      | 38/115 [00:31<01:03,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  33%|███▎      | 38/115 [00:31<01:02,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  33%|███▎      | 38/115 [00:31<01:03,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  33%|███▎      | 38/115 [00:31<01:03,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  33%|███▎      | 38/115 [00:31<01:03,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  33%|███▎      | 38/115 [00:31<01:03,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  33%|███▎      | 38/115 [00:31<01:03,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  33%|███▎      | 38/115 [00:31<01:03,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  33%|███▎      | 38/115 [00:31<01:03,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  33%|███▎      | 38/115 [00:31<01:03,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  33%|███▎      | 38/115 [00:31<01:03,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  33%|███▎      | 38/115 [00:31<01:03,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  33%|███▎      | 38/115 [00:31<01:03,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  34%|███▍      | 39/115 [00:31<01:02,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  34%|███▍      | 39/115 [00:32<01:02,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  34%|███▍      | 39/115 [00:32<01:02,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  34%|███▍      | 39/115 [00:32<01:02,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  34%|███▍      | 39/115 [00:32<01:02,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  34%|███▍      | 39/115 [00:31<01:01,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  34%|███▍      | 39/115 [00:31<01:02,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  34%|███▍      | 39/115 [00:32<01:02,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  34%|███▍      | 39/115 [00:32<01:02,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  34%|███▍      | 39/115 [00:32<01:02,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  34%|███▍      | 39/115 [00:32<01:02,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  34%|███▍      | 39/115 [00:32<01:02,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  34%|███▍      | 39/115 [00:32<01:03,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  34%|███▍      | 39/115 [00:32<01:02,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  35%|███▍      | 40/115 [00:32<01:01,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  35%|███▍      | 40/115 [00:33<01:01,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  35%|███▍      | 40/115 [00:33<01:01,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  35%|███▍      | 40/115 [00:33<01:01,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  35%|███▍      | 40/115 [00:33<01:01,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  35%|███▍      | 40/115 [00:33<01:02,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  35%|███▍      | 40/115 [00:33<01:02,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  35%|███▍      | 40/115 [00:32<01:01,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  35%|███▍      | 40/115 [00:32<01:01,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  35%|███▍      | 40/115 [00:32<01:01,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  35%|███▍      | 40/115 [00:32<01:01,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  35%|███▍      | 40/115 [00:32<01:01,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  35%|███▍      | 40/115 [00:32<01:01,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  35%|███▍      | 40/115 [00:33<01:01,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  36%|███▌      | 41/115 [00:33<01:00,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  36%|███▌      | 41/115 [00:33<01:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  36%|███▌      | 41/115 [00:33<01:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  36%|███▌      | 41/115 [00:33<01:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  36%|███▌      | 41/115 [00:33<01:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  36%|███▌      | 41/115 [00:33<01:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  36%|███▌      | 41/115 [00:33<01:01,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  36%|███▌      | 41/115 [00:33<01:01,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  36%|███▌      | 41/115 [00:33<01:01,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  36%|███▌      | 41/115 [00:34<01:01,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  36%|███▌      | 41/115 [00:34<01:01,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  37%|███▋      | 42/115 [00:34<00:59,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  37%|███▋      | 42/115 [00:34<01:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  37%|███▋      | 42/115 [00:34<01:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  37%|███▋      | 42/115 [00:34<01:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  37%|███▋      | 42/115 [00:34<01:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  37%|███▋      | 42/115 [00:34<01:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  37%|███▋      | 42/115 [00:34<01:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  37%|███▋      | 43/115 [00:35<00:58,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  37%|███▋      | 43/115 [00:35<00:59,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  37%|███▋      | 43/115 [00:35<00:59,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  37%|███▋      | 43/115 [00:35<00:59,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  37%|███▋      | 43/115 [00:35<00:59,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  37%|███▋      | 43/115 [00:35<00:59,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  37%|███▋      | 43/115 [00:35<00:59,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  38%|███▊      | 44/115 [00:35<00:58,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  38%|███▊      | 44/115 [00:36<00:58,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  36%|███▌      | 41/115 [00:33<01:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  36%|███▌      | 41/115 [00:33<01:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  36%|███▌      | 41/115 [00:33<01:01,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 42/115 [00:34<00:59,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 42/115 [00:34<00:59,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 42/115 [00:34<00:59,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 42/115 [00:34<00:59,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 42/115 [00:34<00:59,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 42/115 [00:34<00:59,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 42/115 [00:34<01:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 43/115 [00:35<00:58,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 43/115 [00:35<00:58,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 43/115 [00:35<00:59,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 43/115 [00:35<00:59,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 43/115 [00:35<00:59,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 43/115 [00:35<00:59,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 43/115 [00:35<00:59,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  38%|███▊      | 44/115 [00:36<00:57,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  38%|███▊      | 44/115 [00:36<00:58,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  38%|███▊      | 44/115 [00:36<00:58,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  38%|███▊      | 44/115 [00:36<00:58,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  38%|███▊      | 44/115 [00:36<00:58,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  38%|███▊      | 44/115 [00:36<00:58,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  38%|███▊      | 44/115 [00:36<00:58,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  38%|███▊      | 44/115 [00:36<00:58,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  38%|███▊      | 44/115 [00:36<00:58,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  38%|███▊      | 44/115 [00:36<00:58,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  38%|███▊      | 44/115 [00:36<00:58,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  38%|███▊      | 44/115 [00:36<00:58,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  39%|███▉      | 45/115 [00:36<00:56,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  39%|███▉      | 45/115 [00:37<00:57,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  39%|███▉      | 45/115 [00:37<00:57,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  39%|███▉      | 45/115 [00:37<00:57,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  39%|███▉      | 45/115 [00:37<00:57,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  39%|███▉      | 45/115 [00:37<00:57,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  39%|███▉      | 45/115 [00:36<00:56,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  39%|███▉      | 45/115 [00:36<00:57,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  39%|███▉      | 45/115 [00:36<00:57,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  39%|███▉      | 45/115 [00:37<00:57,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  39%|███▉      | 45/115 [00:37<00:57,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  39%|███▉      | 45/115 [00:37<00:57,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  39%|███▉      | 45/115 [00:37<00:57,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  40%|████      | 46/115 [00:37<00:55,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  40%|████      | 46/115 [00:37<00:56,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  39%|███▉      | 45/115 [00:37<00:58,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  40%|████      | 46/115 [00:37<00:56,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  40%|████      | 46/115 [00:37<00:56,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  40%|████      | 46/115 [00:38<00:56,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  40%|████      | 46/115 [00:38<00:57,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  40%|████      | 46/115 [00:38<00:56,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  40%|████      | 46/115 [00:38<00:57,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  40%|████      | 46/115 [00:38<00:57,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  40%|████      | 46/115 [00:37<00:56,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  40%|████      | 46/115 [00:37<00:56,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  40%|████      | 46/115 [00:37<00:56,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  40%|████      | 46/115 [00:37<00:56,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  40%|████      | 46/115 [00:38<00:56,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  41%|████      | 47/115 [00:38<00:55,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  41%|████      | 47/115 [00:38<00:55,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  41%|████      | 47/115 [00:38<00:55,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  41%|████      | 47/115 [00:38<00:55,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  41%|████      | 47/115 [00:38<00:55,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  41%|████      | 47/115 [00:38<00:55,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  41%|████      | 47/115 [00:38<00:55,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  41%|████      | 47/115 [00:38<00:55,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  41%|████      | 47/115 [00:38<00:56,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  41%|████      | 47/115 [00:38<00:56,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  41%|████      | 47/115 [00:38<00:56,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  41%|████      | 47/115 [00:39<00:56,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  41%|████      | 47/115 [00:39<00:56,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  42%|████▏     | 48/115 [00:39<00:54,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  41%|████      | 47/115 [00:38<00:55,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  42%|████▏     | 48/115 [00:39<00:54,  1.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  42%|████▏     | 48/115 [00:39<00:54,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  42%|████▏     | 48/115 [00:39<00:54,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  42%|████▏     | 48/115 [00:39<00:54,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  42%|████▏     | 48/115 [00:39<00:54,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  42%|████▏     | 48/115 [00:39<00:54,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  42%|████▏     | 48/115 [00:39<00:55,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  42%|████▏     | 48/115 [00:39<00:55,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  42%|████▏     | 48/115 [00:39<00:55,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  42%|████▏     | 48/115 [00:39<00:55,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  42%|████▏     | 48/115 [00:39<00:55,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  42%|████▏     | 48/115 [00:39<00:55,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  42%|████▏     | 48/115 [00:39<00:55,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  43%|████▎     | 49/115 [00:40<00:53,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 49/115 [00:40<00:53,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 49/115 [00:40<00:53,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 49/115 [00:40<00:54,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 49/115 [00:40<00:54,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 49/115 [00:40<00:54,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 49/115 [00:40<00:54,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 49/115 [00:40<00:54,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  43%|████▎     | 49/115 [00:40<00:54,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  43%|████▎     | 49/115 [00:40<00:54,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  43%|████▎     | 49/115 [00:40<00:54,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  43%|████▎     | 49/115 [00:40<00:54,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  43%|████▎     | 49/115 [00:40<00:54,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  43%|████▎     | 49/115 [00:40<00:54,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  43%|████▎     | 50/115 [00:40<00:53,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  43%|████▎     | 50/115 [00:41<00:53,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  43%|████▎     | 50/115 [00:41<00:53,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  43%|████▎     | 50/115 [00:41<00:53,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  43%|████▎     | 50/115 [00:41<00:53,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 50/115 [00:40<00:52,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 50/115 [00:40<00:53,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 50/115 [00:41<00:53,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 50/115 [00:41<00:53,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 50/115 [00:41<00:53,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 50/115 [00:41<00:53,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 50/115 [00:41<00:53,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  44%|████▍     | 51/115 [00:41<00:52,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  43%|████▎     | 50/115 [00:41<00:53,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  43%|████▎     | 50/115 [00:41<00:54,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  44%|████▍     | 51/115 [00:41<00:52,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  44%|████▍     | 51/115 [00:42<00:52,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  44%|████▍     | 51/115 [00:42<00:53,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  44%|████▍     | 51/115 [00:42<00:52,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  44%|████▍     | 51/115 [00:42<00:53,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  44%|████▍     | 51/115 [00:42<00:53,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  44%|████▍     | 51/115 [00:41<00:52,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  44%|████▍     | 51/115 [00:41<00:52,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  44%|████▍     | 51/115 [00:41<00:52,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  44%|████▍     | 51/115 [00:41<00:52,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  44%|████▍     | 51/115 [00:42<00:52,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  44%|████▍     | 51/115 [00:42<00:53,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  45%|████▌     | 52/115 [00:42<00:51,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  45%|████▌     | 52/115 [00:42<00:51,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  45%|████▌     | 52/115 [00:42<00:51,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  44%|████▍     | 51/115 [00:42<00:53,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  45%|████▌     | 52/115 [00:42<00:51,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  45%|████▌     | 52/115 [00:42<00:52,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  45%|████▌     | 52/115 [00:43<00:52,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  45%|████▌     | 52/115 [00:43<00:52,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  45%|████▌     | 52/115 [00:43<00:52,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  45%|████▌     | 52/115 [00:43<00:52,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  45%|████▌     | 52/115 [00:43<00:52,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  46%|████▌     | 53/115 [00:43<00:50,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  45%|████▌     | 52/115 [00:42<00:52,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  45%|████▌     | 52/115 [00:42<00:51,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  45%|████▌     | 52/115 [00:42<00:51,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  45%|████▌     | 52/115 [00:43<00:52,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  46%|████▌     | 53/115 [00:43<00:50,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  46%|████▌     | 53/115 [00:43<00:50,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  46%|████▌     | 53/115 [00:43<00:51,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  46%|████▌     | 53/115 [00:43<00:51,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  46%|████▌     | 53/115 [00:43<00:50,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  46%|████▌     | 53/115 [00:43<00:50,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  46%|████▌     | 53/115 [00:43<00:51,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  46%|████▌     | 53/115 [00:43<00:51,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  46%|████▌     | 53/115 [00:43<00:51,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  46%|████▌     | 53/115 [00:43<00:51,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  46%|████▌     | 53/115 [00:44<00:51,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  46%|████▌     | 53/115 [00:44<00:51,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  47%|████▋     | 54/115 [00:44<00:49,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  46%|████▌     | 53/115 [00:43<00:51,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  47%|████▋     | 54/115 [00:44<00:49,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  47%|████▋     | 54/115 [00:44<00:50,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  47%|████▋     | 54/115 [00:44<00:50,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  47%|████▋     | 54/115 [00:44<00:50,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  47%|████▋     | 54/115 [00:44<00:49,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  47%|████▋     | 54/115 [00:44<00:50,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  47%|████▋     | 54/115 [00:44<00:50,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  47%|████▋     | 54/115 [00:44<00:50,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  47%|████▋     | 54/115 [00:44<00:50,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  47%|████▋     | 54/115 [00:44<00:50,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  47%|████▋     | 54/115 [00:44<00:50,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  47%|████▋     | 54/115 [00:44<00:50,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  47%|████▋     | 54/115 [00:44<00:50,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  48%|████▊     | 55/115 [00:44<00:49,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  48%|████▊     | 55/115 [00:45<00:49,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  48%|████▊     | 55/115 [00:44<00:48,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  48%|████▊     | 55/115 [00:45<00:49,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  48%|████▊     | 55/115 [00:45<00:49,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  48%|████▊     | 55/115 [00:45<00:49,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  48%|████▊     | 55/115 [00:45<00:49,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  48%|████▊     | 55/115 [00:45<00:49,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  48%|████▊     | 55/115 [00:45<00:49,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  48%|████▊     | 55/115 [00:45<00:49,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  48%|████▊     | 55/115 [00:45<00:49,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  48%|████▊     | 55/115 [00:45<00:49,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  48%|████▊     | 55/115 [00:45<00:49,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  48%|████▊     | 55/115 [00:45<00:49,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  49%|████▊     | 56/115 [00:45<00:48,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  49%|████▊     | 56/115 [00:46<00:48,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  49%|████▊     | 56/115 [00:46<00:48,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  49%|████▊     | 56/115 [00:46<00:48,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  49%|████▊     | 56/115 [00:46<00:48,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  49%|████▊     | 56/115 [00:45<00:48,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  49%|████▊     | 56/115 [00:45<00:48,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  49%|████▊     | 56/115 [00:46<00:48,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  49%|████▊     | 56/115 [00:46<00:48,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  49%|████▊     | 56/115 [00:46<00:48,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  49%|████▊     | 56/115 [00:46<00:48,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  49%|████▊     | 56/115 [00:46<00:48,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|████▉     | 57/115 [00:46<00:47,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|████▉     | 57/115 [00:46<00:47,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|████▉     | 57/115 [00:46<00:47,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|████▉     | 57/115 [00:46<00:47,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|████▉     | 57/115 [00:46<00:47,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|████▉     | 57/115 [00:46<00:47,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|████▉     | 57/115 [00:47<00:48,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|█████     | 58/115 [00:47<00:46,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|█████     | 58/115 [00:47<00:46,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|█████     | 58/115 [00:47<00:47,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|█████     | 58/115 [00:47<00:47,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|█████     | 58/115 [00:47<00:46,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|█████     | 58/115 [00:47<00:46,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|█████     | 58/115 [00:48<00:47,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  51%|█████▏    | 59/115 [00:48<00:45,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  51%|█████▏    | 59/115 [00:48<00:46,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  51%|█████▏    | 59/115 [00:48<00:46,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  51%|█████▏    | 59/115 [00:48<00:46,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  51%|█████▏    | 59/115 [00:48<00:46,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  51%|█████▏    | 59/115 [00:48<00:46,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  49%|████▊     | 56/115 [00:46<00:49,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  49%|████▊     | 56/115 [00:46<00:48,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  50%|████▉     | 57/115 [00:46<00:47,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  50%|████▉     | 57/115 [00:46<00:47,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  50%|████▉     | 57/115 [00:47<00:47,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  50%|████▉     | 57/115 [00:47<00:48,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  50%|████▉     | 57/115 [00:47<00:48,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  50%|████▉     | 57/115 [00:47<00:48,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  50%|████▉     | 57/115 [00:47<00:48,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  50%|█████     | 58/115 [00:47<00:46,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  50%|█████     | 58/115 [00:47<00:47,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  50%|█████     | 58/115 [00:47<00:47,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  50%|█████     | 58/115 [00:47<00:47,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  50%|█████     | 58/115 [00:47<00:47,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  50%|█████     | 58/115 [00:48<00:47,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  50%|█████     | 58/115 [00:48<00:47,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  51%|█████▏    | 59/115 [00:48<00:45,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  51%|█████▏    | 59/115 [00:48<00:46,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  51%|█████▏    | 59/115 [00:48<00:46,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  51%|█████▏    | 59/115 [00:48<00:46,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  51%|█████▏    | 59/115 [00:48<00:46,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  51%|█████▏    | 59/115 [00:49<00:46,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  51%|█████▏    | 59/115 [00:49<00:46,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  52%|█████▏    | 60/115 [00:49<00:45,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  51%|█████▏    | 59/115 [00:48<00:46,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  52%|█████▏    | 60/115 [00:49<00:44,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  52%|█████▏    | 60/115 [00:49<00:45,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  52%|█████▏    | 60/115 [00:49<00:45,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  52%|█████▏    | 60/115 [00:49<00:45,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  52%|█████▏    | 60/115 [00:49<00:45,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  52%|█████▏    | 60/115 [00:49<00:45,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  52%|█████▏    | 60/115 [00:49<00:45,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  52%|█████▏    | 60/115 [00:49<00:45,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  52%|█████▏    | 60/115 [00:49<00:45,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  52%|█████▏    | 60/115 [00:49<00:45,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  52%|█████▏    | 60/115 [00:49<00:45,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  52%|█████▏    | 60/115 [00:49<00:45,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  52%|█████▏    | 60/115 [00:49<00:45,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  53%|█████▎    | 61/115 [00:49<00:44,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  53%|█████▎    | 61/115 [00:50<00:44,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  53%|█████▎    | 61/115 [00:49<00:44,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  53%|█████▎    | 61/115 [00:50<00:44,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  53%|█████▎    | 61/115 [00:50<00:44,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  53%|█████▎    | 61/115 [00:50<00:44,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  53%|█████▎    | 61/115 [00:50<00:44,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  53%|█████▎    | 61/115 [00:50<00:44,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  53%|█████▎    | 61/115 [00:50<00:44,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  54%|█████▍    | 62/115 [00:50<00:43,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  53%|█████▎    | 61/115 [00:50<00:44,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  53%|█████▎    | 61/115 [00:50<00:44,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  53%|█████▎    | 61/115 [00:50<00:44,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  53%|█████▎    | 61/115 [00:50<00:45,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  53%|█████▎    | 61/115 [00:50<00:44,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  54%|█████▍    | 62/115 [00:50<00:43,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  54%|█████▍    | 62/115 [00:51<00:43,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  54%|█████▍    | 62/115 [00:51<00:43,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  54%|█████▍    | 62/115 [00:51<00:43,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  54%|█████▍    | 62/115 [00:51<00:43,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  54%|█████▍    | 62/115 [00:50<00:43,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  54%|█████▍    | 62/115 [00:50<00:43,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  54%|█████▍    | 62/115 [00:51<00:43,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  54%|█████▍    | 62/115 [00:51<00:43,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  54%|█████▍    | 62/115 [00:51<00:43,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  54%|█████▍    | 62/115 [00:51<00:43,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  55%|█████▍    | 63/115 [00:51<00:42,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  55%|█████▍    | 63/115 [00:51<00:42,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  54%|█████▍    | 62/115 [00:51<00:44,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  54%|█████▍    | 62/115 [00:51<00:44,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  55%|█████▍    | 63/115 [00:51<00:42,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  55%|█████▍    | 63/115 [00:51<00:43,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  55%|█████▍    | 63/115 [00:52<00:42,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  55%|█████▍    | 63/115 [00:52<00:42,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  55%|█████▍    | 63/115 [00:52<00:43,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  55%|█████▍    | 63/115 [00:52<00:43,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  56%|█████▌    | 64/115 [00:52<00:41,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  55%|█████▍    | 63/115 [00:52<00:43,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  55%|█████▍    | 63/115 [00:51<00:42,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  55%|█████▍    | 63/115 [00:51<00:42,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  55%|█████▍    | 63/115 [00:51<00:42,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  55%|█████▍    | 63/115 [00:51<00:42,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  55%|█████▍    | 63/115 [00:52<00:43,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  56%|█████▌    | 64/115 [00:52<00:41,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  56%|█████▌    | 64/115 [00:52<00:41,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  56%|█████▌    | 64/115 [00:52<00:41,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  56%|█████▌    | 64/115 [00:52<00:41,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  56%|█████▌    | 64/115 [00:52<00:41,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  56%|█████▌    | 64/115 [00:52<00:41,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  56%|█████▌    | 64/115 [00:52<00:42,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  56%|█████▌    | 64/115 [00:52<00:42,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  56%|█████▌    | 64/115 [00:52<00:42,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  56%|█████▌    | 64/115 [00:52<00:42,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  57%|█████▋    | 65/115 [00:53<00:40,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  56%|█████▌    | 64/115 [00:53<00:42,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  56%|█████▌    | 64/115 [00:53<00:42,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  56%|█████▌    | 64/115 [00:53<00:42,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 65/115 [00:53<00:40,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 65/115 [00:53<00:41,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 65/115 [00:53<00:41,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 65/115 [00:53<00:40,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 65/115 [00:53<00:40,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 65/115 [00:53<00:41,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  57%|█████▋    | 65/115 [00:53<00:41,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  57%|█████▋    | 65/115 [00:53<00:41,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  57%|█████▋    | 65/115 [00:53<00:41,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  57%|█████▋    | 65/115 [00:53<00:41,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  57%|█████▋    | 66/115 [00:53<00:39,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  57%|█████▋    | 65/115 [00:53<00:41,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  57%|█████▋    | 65/115 [00:53<00:41,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 65/115 [00:53<00:41,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 66/115 [00:53<00:39,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 66/115 [00:54<00:40,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 66/115 [00:54<00:40,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 66/115 [00:54<00:39,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 66/115 [00:54<00:39,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 66/115 [00:54<00:40,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 66/115 [00:54<00:40,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  57%|█████▋    | 66/115 [00:54<00:40,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  57%|█████▋    | 66/115 [00:54<00:40,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  57%|█████▋    | 66/115 [00:54<00:40,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  57%|█████▋    | 66/115 [00:54<00:40,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  58%|█████▊    | 67/115 [00:54<00:39,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  57%|█████▋    | 66/115 [00:54<00:40,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  57%|█████▋    | 66/115 [00:54<00:40,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  58%|█████▊    | 67/115 [00:55<00:39,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  58%|█████▊    | 67/115 [00:54<00:39,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  58%|█████▊    | 67/115 [00:54<00:39,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  58%|█████▊    | 67/115 [00:55<00:39,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  58%|█████▊    | 67/115 [00:55<00:39,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  58%|█████▊    | 67/115 [00:55<00:39,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  58%|█████▊    | 67/115 [00:55<00:39,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  58%|█████▊    | 67/115 [00:55<00:39,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  59%|█████▉    | 68/115 [00:55<00:38,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  58%|█████▊    | 67/115 [00:55<00:39,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  58%|█████▊    | 67/115 [00:55<00:39,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  58%|█████▊    | 67/115 [00:55<00:39,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  59%|█████▉    | 68/115 [00:55<00:38,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  58%|█████▊    | 67/115 [00:55<00:39,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  58%|█████▊    | 67/115 [00:55<00:39,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  59%|█████▉    | 68/115 [00:56<00:38,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  59%|█████▉    | 68/115 [00:56<00:38,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  59%|█████▉    | 68/115 [00:56<00:38,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  59%|█████▉    | 68/115 [00:56<00:38,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  59%|█████▉    | 68/115 [00:55<00:38,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  59%|█████▉    | 68/115 [00:55<00:38,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  59%|█████▉    | 68/115 [00:55<00:38,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  59%|█████▉    | 68/115 [00:55<00:38,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  59%|█████▉    | 68/115 [00:55<00:38,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  59%|█████▉    | 68/115 [00:56<00:38,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  60%|██████    | 69/115 [00:56<00:37,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  60%|██████    | 69/115 [00:56<00:37,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  60%|██████    | 69/115 [00:56<00:37,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  60%|██████    | 69/115 [00:56<00:37,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  60%|██████    | 69/115 [00:56<00:37,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  60%|██████    | 69/115 [00:56<00:37,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  60%|██████    | 69/115 [00:57<00:38,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  61%|██████    | 70/115 [00:57<00:36,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  61%|██████    | 70/115 [00:57<00:37,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  61%|██████    | 70/115 [00:57<00:37,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  61%|██████    | 70/115 [00:57<00:37,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  61%|██████    | 70/115 [00:57<00:37,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  61%|██████    | 70/115 [00:57<00:37,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  61%|██████    | 70/115 [00:57<00:37,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  62%|██████▏   | 71/115 [00:58<00:36,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  62%|██████▏   | 71/115 [00:58<00:36,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  62%|██████▏   | 71/115 [00:58<00:36,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  62%|██████▏   | 71/115 [00:58<00:36,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  62%|██████▏   | 71/115 [00:58<00:36,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  62%|██████▏   | 71/115 [00:58<00:36,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  60%|██████    | 69/115 [00:56<00:37,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  59%|█████▉    | 68/115 [00:56<00:39,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  59%|█████▉    | 68/115 [00:56<00:39,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  60%|██████    | 69/115 [00:56<00:38,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  60%|██████    | 69/115 [00:57<00:38,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  60%|██████    | 69/115 [00:57<00:38,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  60%|██████    | 69/115 [00:57<00:38,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  61%|██████    | 70/115 [00:57<00:36,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  60%|██████    | 69/115 [00:57<00:38,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  60%|██████    | 69/115 [00:57<00:38,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  61%|██████    | 70/115 [00:57<00:37,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  61%|██████    | 70/115 [00:57<00:37,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  61%|██████    | 70/115 [00:57<00:37,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  61%|██████    | 70/115 [00:57<00:37,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  62%|██████▏   | 71/115 [00:58<00:36,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  61%|██████    | 70/115 [00:58<00:37,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  61%|██████    | 70/115 [00:58<00:37,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  62%|██████▏   | 71/115 [00:58<00:36,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  62%|██████▏   | 71/115 [00:58<00:36,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  62%|██████▏   | 71/115 [00:58<00:36,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  62%|██████▏   | 71/115 [00:58<00:36,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  63%|██████▎   | 72/115 [00:58<00:35,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  62%|██████▏   | 71/115 [00:58<00:36,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  62%|██████▏   | 71/115 [00:58<00:36,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  62%|██████▏   | 71/115 [00:58<00:36,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 72/115 [00:58<00:35,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 72/115 [00:59<00:35,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 72/115 [00:59<00:35,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 72/115 [00:59<00:35,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 72/115 [00:59<00:35,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 72/115 [00:59<00:35,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 72/115 [00:59<00:35,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 73/115 [00:59<00:34,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  63%|██████▎   | 72/115 [00:59<00:35,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  63%|██████▎   | 72/115 [00:59<00:35,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  63%|██████▎   | 72/115 [00:59<00:35,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  63%|██████▎   | 72/115 [00:59<00:35,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  63%|██████▎   | 73/115 [00:59<00:34,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  63%|██████▎   | 72/115 [00:59<00:35,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  63%|██████▎   | 72/115 [00:59<00:35,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  63%|██████▎   | 73/115 [01:00<00:34,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  63%|██████▎   | 73/115 [01:00<00:34,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  63%|██████▎   | 73/115 [01:00<00:34,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 73/115 [00:59<00:34,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 73/115 [01:00<00:34,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 73/115 [01:00<00:34,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 73/115 [01:00<00:34,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 73/115 [01:00<00:34,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 73/115 [01:00<00:34,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  64%|██████▍   | 74/115 [01:00<00:33,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  64%|██████▍   | 74/115 [01:00<00:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  63%|██████▎   | 73/115 [01:00<00:34,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  64%|██████▍   | 74/115 [01:00<00:33,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  63%|██████▎   | 73/115 [01:00<00:34,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  63%|██████▎   | 73/115 [01:00<00:34,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  64%|██████▍   | 74/115 [01:01<00:34,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  64%|██████▍   | 74/115 [01:01<00:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  64%|██████▍   | 74/115 [01:01<00:34,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  64%|██████▍   | 74/115 [01:01<00:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  65%|██████▌   | 75/115 [01:01<00:32,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  64%|██████▍   | 74/115 [01:00<00:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  64%|██████▍   | 74/115 [01:00<00:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  64%|██████▍   | 74/115 [01:00<00:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  64%|██████▍   | 74/115 [01:00<00:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  64%|██████▍   | 74/115 [01:01<00:34,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  65%|██████▌   | 75/115 [01:01<00:32,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  65%|██████▌   | 75/115 [01:01<00:32,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  65%|██████▌   | 75/115 [01:01<00:32,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  65%|██████▌   | 75/115 [01:01<00:32,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  65%|██████▌   | 75/115 [01:01<00:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  65%|██████▌   | 75/115 [01:01<00:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  64%|██████▍   | 74/115 [01:01<00:34,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  64%|██████▍   | 74/115 [01:01<00:34,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  65%|██████▌   | 75/115 [01:01<00:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  65%|██████▌   | 75/115 [01:02<00:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  65%|██████▌   | 75/115 [01:02<00:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  65%|██████▌   | 75/115 [01:02<00:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  66%|██████▌   | 76/115 [01:02<00:31,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  65%|██████▌   | 75/115 [01:02<00:33,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  65%|██████▌   | 75/115 [01:02<00:33,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  65%|██████▌   | 75/115 [01:02<00:33,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  66%|██████▌   | 76/115 [01:02<00:31,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  66%|██████▌   | 76/115 [01:02<00:32,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  66%|██████▌   | 76/115 [01:02<00:32,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  66%|██████▌   | 76/115 [01:02<00:32,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  66%|██████▌   | 76/115 [01:02<00:32,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  66%|██████▌   | 76/115 [01:02<00:32,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  66%|██████▌   | 76/115 [01:02<00:32,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  66%|██████▌   | 76/115 [01:02<00:32,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  66%|██████▌   | 76/115 [01:02<00:32,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  66%|██████▌   | 76/115 [01:02<00:32,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  67%|██████▋   | 77/115 [01:02<00:31,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  66%|██████▌   | 76/115 [01:03<00:32,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  66%|██████▌   | 76/115 [01:03<00:32,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  66%|██████▌   | 76/115 [01:02<00:32,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  67%|██████▋   | 77/115 [01:02<00:31,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  67%|██████▋   | 77/115 [01:03<00:31,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  67%|██████▋   | 77/115 [01:03<00:31,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  67%|██████▋   | 77/115 [01:03<00:31,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  67%|██████▋   | 77/115 [01:03<00:31,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  67%|██████▋   | 77/115 [01:03<00:31,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  67%|██████▋   | 77/115 [01:03<00:31,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  67%|██████▋   | 77/115 [01:03<00:31,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  67%|██████▋   | 77/115 [01:03<00:31,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  67%|██████▋   | 77/115 [01:03<00:31,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  68%|██████▊   | 78/115 [01:03<00:30,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  67%|██████▋   | 77/115 [01:03<00:31,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  67%|██████▋   | 77/115 [01:03<00:31,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  68%|██████▊   | 78/115 [01:04<00:30,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  67%|██████▋   | 77/115 [01:03<00:31,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  68%|██████▊   | 78/115 [01:03<00:30,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  68%|██████▊   | 78/115 [01:04<00:30,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  68%|██████▊   | 78/115 [01:04<00:30,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  68%|██████▊   | 78/115 [01:04<00:30,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  68%|██████▊   | 78/115 [01:04<00:30,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  68%|██████▊   | 78/115 [01:04<00:30,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  69%|██████▊   | 79/115 [01:04<00:29,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  68%|██████▊   | 78/115 [01:04<00:30,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  68%|██████▊   | 78/115 [01:04<00:30,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  68%|██████▊   | 78/115 [01:04<00:30,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  68%|██████▊   | 78/115 [01:04<00:30,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  69%|██████▊   | 79/115 [01:04<00:29,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  68%|██████▊   | 78/115 [01:04<00:30,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  68%|██████▊   | 78/115 [01:04<00:30,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  69%|██████▊   | 79/115 [01:05<00:29,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  69%|██████▊   | 79/115 [01:05<00:29,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  69%|██████▊   | 79/115 [01:05<00:29,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  69%|██████▊   | 79/115 [01:05<00:29,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  69%|██████▊   | 79/115 [01:04<00:29,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  69%|██████▊   | 79/115 [01:04<00:29,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  69%|██████▊   | 79/115 [01:05<00:29,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  69%|██████▊   | 79/115 [01:05<00:29,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  69%|██████▊   | 79/115 [01:05<00:29,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|██████▉   | 80/115 [01:05<00:28,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  69%|██████▊   | 79/115 [01:05<00:29,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|██████▉   | 80/115 [01:05<00:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  70%|██████▉   | 80/115 [01:05<00:28,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  69%|██████▊   | 79/115 [01:05<00:29,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  69%|██████▊   | 79/115 [01:05<00:29,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  70%|██████▉   | 80/115 [01:06<00:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  70%|██████▉   | 80/115 [01:06<00:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  70%|██████▉   | 80/115 [01:06<00:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  70%|██████▉   | 80/115 [01:06<00:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  70%|███████   | 81/115 [01:06<00:27,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|██████▉   | 80/115 [01:05<00:28,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|██████▉   | 80/115 [01:05<00:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|██████▉   | 80/115 [01:05<00:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|██████▉   | 80/115 [01:05<00:28,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|███████   | 81/115 [01:06<00:27,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|██████▉   | 80/115 [01:06<00:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|███████   | 81/115 [01:06<00:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|███████   | 81/115 [01:06<00:27,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|███████   | 81/115 [01:06<00:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|███████   | 81/115 [01:06<00:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|███████   | 81/115 [01:06<00:27,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  70%|██████▉   | 80/115 [01:06<00:29,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  70%|██████▉   | 80/115 [01:06<00:29,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  70%|███████   | 81/115 [01:06<00:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  70%|███████   | 81/115 [01:06<00:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  70%|███████   | 81/115 [01:06<00:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  70%|███████   | 81/115 [01:07<00:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  71%|███████▏  | 82/115 [01:07<00:26,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  70%|███████   | 81/115 [01:07<00:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  70%|███████   | 81/115 [01:07<00:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  71%|███████▏  | 82/115 [01:07<00:26,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|███████   | 81/115 [01:07<00:28,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  71%|███████▏  | 82/115 [01:07<00:27,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  71%|███████▏  | 82/115 [01:07<00:27,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  71%|███████▏  | 82/115 [01:07<00:27,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  71%|███████▏  | 82/115 [01:07<00:27,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  71%|███████▏  | 82/115 [01:07<00:27,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  71%|███████▏  | 82/115 [01:07<00:27,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  71%|███████▏  | 82/115 [01:07<00:27,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  71%|███████▏  | 82/115 [01:07<00:27,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  71%|███████▏  | 82/115 [01:07<00:27,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  72%|███████▏  | 83/115 [01:07<00:26,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  71%|███████▏  | 82/115 [01:08<00:27,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  71%|███████▏  | 82/115 [01:08<00:27,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  72%|███████▏  | 83/115 [01:07<00:26,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  71%|███████▏  | 82/115 [01:07<00:27,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  72%|███████▏  | 83/115 [01:08<00:26,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  72%|███████▏  | 83/115 [01:08<00:26,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  72%|███████▏  | 83/115 [01:08<00:26,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  72%|███████▏  | 83/115 [01:08<00:26,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  72%|███████▏  | 83/115 [01:08<00:26,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  73%|███████▎  | 84/115 [01:08<00:25,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  72%|███████▏  | 83/115 [01:08<00:26,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  72%|███████▏  | 83/115 [01:08<00:26,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  72%|███████▏  | 83/115 [01:08<00:26,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  72%|███████▏  | 83/115 [01:08<00:26,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  72%|███████▏  | 83/115 [01:08<00:26,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  73%|███████▎  | 84/115 [01:08<00:25,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  72%|███████▏  | 83/115 [01:08<00:26,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  72%|███████▏  | 83/115 [01:08<00:26,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  73%|███████▎  | 84/115 [01:09<00:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  73%|███████▎  | 84/115 [01:09<00:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  73%|███████▎  | 84/115 [01:09<00:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  73%|███████▎  | 84/115 [01:09<00:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  74%|███████▍  | 85/115 [01:09<00:24,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  73%|███████▎  | 84/115 [01:09<00:25,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  73%|███████▎  | 84/115 [01:09<00:25,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  74%|███████▍  | 85/115 [01:10<00:24,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  74%|███████▍  | 85/115 [01:10<00:24,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  74%|███████▍  | 85/115 [01:10<00:24,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  74%|███████▍  | 85/115 [01:10<00:24,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  75%|███████▍  | 86/115 [01:10<00:23,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  74%|███████▍  | 85/115 [01:10<00:24,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  74%|███████▍  | 85/115 [01:10<00:24,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  75%|███████▍  | 86/115 [01:10<00:23,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  75%|███████▍  | 86/115 [01:11<00:23,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  75%|███████▍  | 86/115 [01:11<00:23,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  75%|███████▍  | 86/115 [01:11<00:23,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  76%|███████▌  | 87/115 [01:11<00:22,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  73%|███████▎  | 84/115 [01:08<00:25,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  73%|███████▎  | 84/115 [01:09<00:25,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  73%|███████▎  | 84/115 [01:09<00:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  73%|███████▎  | 84/115 [01:09<00:25,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  73%|███████▎  | 84/115 [01:09<00:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  74%|███████▍  | 85/115 [01:09<00:24,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  73%|███████▎  | 84/115 [01:09<00:25,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  74%|███████▍  | 85/115 [01:09<00:24,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  74%|███████▍  | 85/115 [01:09<00:24,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  74%|███████▍  | 85/115 [01:09<00:24,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  74%|███████▍  | 85/115 [01:09<00:24,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  74%|███████▍  | 85/115 [01:09<00:24,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  75%|███████▍  | 86/115 [01:10<00:23,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  74%|███████▍  | 85/115 [01:10<00:24,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  75%|███████▍  | 86/115 [01:10<00:23,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  75%|███████▍  | 86/115 [01:10<00:23,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  75%|███████▍  | 86/115 [01:10<00:23,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  75%|███████▍  | 86/115 [01:10<00:23,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  75%|███████▍  | 86/115 [01:10<00:23,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  76%|███████▌  | 87/115 [01:11<00:22,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  75%|███████▍  | 86/115 [01:11<00:23,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  76%|███████▌  | 87/115 [01:11<00:22,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  76%|███████▌  | 87/115 [01:11<00:22,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  76%|███████▌  | 87/115 [01:11<00:22,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  76%|███████▌  | 87/115 [01:11<00:23,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  76%|███████▌  | 87/115 [01:11<00:23,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  75%|███████▍  | 86/115 [01:11<00:23,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  75%|███████▍  | 86/115 [01:11<00:23,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  76%|███████▌  | 87/115 [01:11<00:23,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  76%|███████▌  | 87/115 [01:11<00:23,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  76%|███████▌  | 87/115 [01:11<00:23,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  76%|███████▌  | 87/115 [01:11<00:23,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  77%|███████▋  | 88/115 [01:11<00:22,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  76%|███████▌  | 87/115 [01:12<00:23,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  76%|███████▌  | 87/115 [01:12<00:23,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 88/115 [01:11<00:22,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  76%|███████▌  | 87/115 [01:12<00:23,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 88/115 [01:12<00:22,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 88/115 [01:12<00:22,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 88/115 [01:12<00:22,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 88/115 [01:12<00:22,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 88/115 [01:12<00:22,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  77%|███████▋  | 88/115 [01:12<00:22,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  77%|███████▋  | 88/115 [01:12<00:22,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  77%|███████▋  | 88/115 [01:12<00:22,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  77%|███████▋  | 88/115 [01:12<00:22,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  77%|███████▋  | 89/115 [01:12<00:21,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  77%|███████▋  | 88/115 [01:13<00:22,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  77%|███████▋  | 88/115 [01:13<00:22,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 89/115 [01:12<00:21,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 88/115 [01:12<00:22,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 89/115 [01:13<00:21,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 89/115 [01:13<00:21,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 89/115 [01:13<00:21,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 89/115 [01:13<00:21,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 89/115 [01:13<00:21,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  78%|███████▊  | 90/115 [01:13<00:20,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 89/115 [01:13<00:21,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  77%|███████▋  | 89/115 [01:13<00:21,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  77%|███████▋  | 89/115 [01:13<00:21,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  77%|███████▋  | 89/115 [01:13<00:21,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  77%|███████▋  | 89/115 [01:13<00:21,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  78%|███████▊  | 90/115 [01:13<00:20,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  77%|███████▋  | 89/115 [01:13<00:21,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  77%|███████▋  | 89/115 [01:13<00:21,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  78%|███████▊  | 90/115 [01:14<00:20,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  78%|███████▊  | 90/115 [01:13<00:20,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  78%|███████▊  | 90/115 [01:13<00:20,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  78%|███████▊  | 90/115 [01:14<00:20,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  78%|███████▊  | 90/115 [01:14<00:20,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  78%|███████▊  | 90/115 [01:14<00:20,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  79%|███████▉  | 91/115 [01:14<00:19,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  78%|███████▊  | 90/115 [01:14<00:20,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  79%|███████▉  | 91/115 [01:14<00:19,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  78%|███████▊  | 90/115 [01:14<00:20,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  79%|███████▉  | 91/115 [01:14<00:19,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  78%|███████▊  | 90/115 [01:14<00:20,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  78%|███████▊  | 90/115 [01:14<00:20,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  78%|███████▊  | 90/115 [01:14<00:20,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  78%|███████▊  | 90/115 [01:14<00:20,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  79%|███████▉  | 91/115 [01:15<00:19,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  80%|████████  | 92/115 [01:15<00:18,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  79%|███████▉  | 91/115 [01:15<00:19,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  79%|███████▉  | 91/115 [01:15<00:19,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  79%|███████▉  | 91/115 [01:15<00:19,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  79%|███████▉  | 91/115 [01:14<00:19,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  79%|███████▉  | 91/115 [01:14<00:19,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  79%|███████▉  | 91/115 [01:14<00:19,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  79%|███████▉  | 91/115 [01:14<00:19,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  80%|████████  | 92/115 [01:15<00:18,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  79%|███████▉  | 91/115 [01:15<00:19,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  80%|████████  | 92/115 [01:15<00:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  80%|████████  | 92/115 [01:15<00:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  80%|████████  | 92/115 [01:15<00:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  80%|████████  | 92/115 [01:15<00:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  80%|████████  | 92/115 [01:15<00:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  79%|███████▉  | 91/115 [01:15<00:19,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  79%|███████▉  | 91/115 [01:15<00:19,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  80%|████████  | 92/115 [01:15<00:19,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  81%|████████  | 93/115 [01:16<00:18,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  80%|████████  | 92/115 [01:16<00:19,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  80%|████████  | 92/115 [01:16<00:19,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  80%|████████  | 92/115 [01:16<00:19,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  81%|████████  | 93/115 [01:16<00:17,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  80%|████████  | 92/115 [01:16<00:19,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  81%|████████  | 93/115 [01:16<00:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  81%|████████  | 93/115 [01:16<00:18,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  81%|████████  | 93/115 [01:16<00:18,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  81%|████████  | 93/115 [01:16<00:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  81%|████████  | 93/115 [01:16<00:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  80%|████████  | 92/115 [01:16<00:19,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  80%|████████  | 92/115 [01:16<00:19,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  81%|████████  | 93/115 [01:16<00:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  82%|████████▏ | 94/115 [01:16<00:17,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  81%|████████  | 93/115 [01:16<00:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  81%|████████  | 93/115 [01:16<00:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  81%|████████  | 93/115 [01:16<00:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  81%|████████  | 93/115 [01:17<00:18,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  81%|████████  | 93/115 [01:17<00:18,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  82%|████████▏ | 94/115 [01:16<00:17,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  81%|████████  | 93/115 [01:16<00:18,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  82%|████████▏ | 94/115 [01:17<00:17,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  82%|████████▏ | 94/115 [01:17<00:17,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  82%|████████▏ | 94/115 [01:17<00:17,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  82%|████████▏ | 94/115 [01:17<00:17,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  82%|████████▏ | 94/115 [01:17<00:17,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 95/115 [01:17<00:16,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  82%|████████▏ | 94/115 [01:17<00:17,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  83%|████████▎ | 95/115 [01:17<00:16,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  82%|████████▏ | 94/115 [01:17<00:17,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  82%|████████▏ | 94/115 [01:17<00:17,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  82%|████████▏ | 94/115 [01:17<00:17,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  82%|████████▏ | 94/115 [01:18<00:17,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  82%|████████▏ | 94/115 [01:18<00:17,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  82%|████████▏ | 94/115 [01:17<00:17,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 95/115 [01:18<00:16,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 95/115 [01:18<00:16,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 95/115 [01:18<00:16,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 95/115 [01:18<00:16,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 95/115 [01:18<00:16,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 96/115 [01:18<00:15,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 95/115 [01:18<00:16,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  83%|████████▎ | 95/115 [01:18<00:16,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  83%|████████▎ | 96/115 [01:18<00:15,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  83%|████████▎ | 95/115 [01:18<00:16,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  83%|████████▎ | 95/115 [01:18<00:16,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  83%|████████▎ | 95/115 [01:18<00:16,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  83%|████████▎ | 95/115 [01:18<00:16,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  83%|████████▎ | 95/115 [01:18<00:16,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  83%|████████▎ | 96/115 [01:19<00:15,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  84%|████████▍ | 97/115 [01:19<00:14,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  83%|████████▎ | 96/115 [01:19<00:15,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  83%|████████▎ | 96/115 [01:19<00:15,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  83%|████████▎ | 96/115 [01:19<00:15,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  83%|████████▎ | 96/115 [01:19<00:15,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  83%|████████▎ | 96/115 [01:19<00:15,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  84%|████████▍ | 97/115 [01:20<00:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  85%|████████▌ | 98/115 [01:20<00:13,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  84%|████████▍ | 97/115 [01:20<00:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  84%|████████▍ | 97/115 [01:20<00:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  84%|████████▍ | 97/115 [01:20<00:14,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  84%|████████▍ | 97/115 [01:20<00:14,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  84%|████████▍ | 97/115 [01:20<00:14,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  85%|████████▌ | 98/115 [01:20<00:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  86%|████████▌ | 99/115 [01:20<00:13,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  85%|████████▌ | 98/115 [01:21<00:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  85%|████████▌ | 98/115 [01:21<00:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  85%|████████▌ | 98/115 [01:21<00:14,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 96/115 [01:18<00:15,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 96/115 [01:18<00:15,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 96/115 [01:19<00:15,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 96/115 [01:19<00:15,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 96/115 [01:19<00:15,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  84%|████████▍ | 97/115 [01:19<00:14,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 96/115 [01:19<00:15,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  84%|████████▍ | 97/115 [01:19<00:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  84%|████████▍ | 97/115 [01:19<00:14,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  84%|████████▍ | 97/115 [01:19<00:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  84%|████████▍ | 97/115 [01:19<00:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  84%|████████▍ | 97/115 [01:19<00:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  85%|████████▌ | 98/115 [01:20<00:13,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  84%|████████▍ | 97/115 [01:20<00:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  85%|████████▌ | 98/115 [01:20<00:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  85%|████████▌ | 98/115 [01:20<00:13,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  85%|████████▌ | 98/115 [01:20<00:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  85%|████████▌ | 98/115 [01:20<00:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  85%|████████▌ | 98/115 [01:20<00:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  86%|████████▌ | 99/115 [01:20<00:13,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  85%|████████▌ | 98/115 [01:21<00:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  86%|████████▌ | 99/115 [01:21<00:13,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  86%|████████▌ | 99/115 [01:21<00:13,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  86%|████████▌ | 99/115 [01:21<00:13,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  86%|████████▌ | 99/115 [01:21<00:13,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  86%|████████▌ | 99/115 [01:21<00:13,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  85%|████████▌ | 98/115 [01:21<00:14,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  85%|████████▌ | 98/115 [01:21<00:14,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  86%|████████▌ | 99/115 [01:21<00:13,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  87%|████████▋ | 100/115 [01:21<00:12,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  86%|████████▌ | 99/115 [01:21<00:13,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  86%|████████▌ | 99/115 [01:21<00:13,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  86%|████████▌ | 99/115 [01:21<00:13,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  86%|████████▌ | 99/115 [01:22<00:13,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  86%|████████▌ | 99/115 [01:22<00:13,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  87%|████████▋ | 100/115 [01:21<00:12,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  86%|████████▌ | 99/115 [01:21<00:13,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  87%|████████▋ | 100/115 [01:22<00:12,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  87%|████████▋ | 100/115 [01:22<00:12,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  87%|████████▋ | 100/115 [01:22<00:12,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  87%|████████▋ | 100/115 [01:22<00:12,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  87%|████████▋ | 100/115 [01:22<00:12,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  88%|████████▊ | 101/115 [01:22<00:11,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  87%|████████▋ | 100/115 [01:22<00:12,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  88%|████████▊ | 101/115 [01:22<00:11,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  87%|████████▋ | 100/115 [01:22<00:12,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  87%|████████▋ | 100/115 [01:22<00:12,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  87%|████████▋ | 100/115 [01:22<00:12,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  87%|████████▋ | 100/115 [01:23<00:12,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  87%|████████▋ | 100/115 [01:23<00:12,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  88%|████████▊ | 101/115 [01:23<00:11,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  87%|████████▋ | 100/115 [01:22<00:12,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  88%|████████▊ | 101/115 [01:23<00:11,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  88%|████████▊ | 101/115 [01:23<00:11,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  88%|████████▊ | 101/115 [01:23<00:11,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  88%|████████▊ | 101/115 [01:23<00:11,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  88%|████████▊ | 101/115 [01:23<00:11,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  89%|████████▊ | 102/115 [01:23<00:10,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  88%|████████▊ | 101/115 [01:23<00:11,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  89%|████████▊ | 102/115 [01:23<00:10,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  88%|████████▊ | 101/115 [01:23<00:11,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  88%|████████▊ | 101/115 [01:23<00:11,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  88%|████████▊ | 101/115 [01:23<00:11,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  88%|████████▊ | 101/115 [01:23<00:11,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  88%|████████▊ | 101/115 [01:23<00:11,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  89%|████████▊ | 102/115 [01:24<00:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  90%|████████▉ | 103/115 [01:24<00:09,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  89%|████████▊ | 102/115 [01:24<00:10,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  89%|████████▊ | 102/115 [01:24<00:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  89%|████████▊ | 102/115 [01:23<00:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  89%|████████▊ | 102/115 [01:23<00:10,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  89%|████████▊ | 102/115 [01:23<00:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  89%|████████▊ | 102/115 [01:23<00:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  89%|████████▊ | 102/115 [01:24<00:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|████████▉ | 103/115 [01:24<00:09,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  89%|████████▊ | 102/115 [01:24<00:10,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|████████▉ | 103/115 [01:24<00:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|████████▉ | 103/115 [01:24<00:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  89%|████████▊ | 102/115 [01:24<00:10,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  89%|████████▊ | 102/115 [01:24<00:10,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  89%|████████▊ | 102/115 [01:24<00:10,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  90%|████████▉ | 103/115 [01:25<00:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  90%|█████████ | 104/115 [01:25<00:09,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  90%|████████▉ | 103/115 [01:25<00:09,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  90%|████████▉ | 103/115 [01:25<00:09,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  90%|████████▉ | 103/115 [01:25<00:09,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|████████▉ | 103/115 [01:24<00:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|████████▉ | 103/115 [01:24<00:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|████████▉ | 103/115 [01:24<00:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|█████████ | 104/115 [01:25<00:08,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|████████▉ | 103/115 [01:25<00:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|█████████ | 104/115 [01:25<00:09,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|█████████ | 104/115 [01:25<00:09,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|█████████ | 104/115 [01:25<00:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|█████████ | 104/115 [01:25<00:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|█████████ | 104/115 [01:25<00:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  90%|████████▉ | 103/115 [01:25<00:10,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  90%|████████▉ | 103/115 [01:25<00:10,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  90%|█████████ | 104/115 [01:25<00:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  91%|█████████▏| 105/115 [01:25<00:08,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  90%|█████████ | 104/115 [01:26<00:09,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  90%|█████████ | 104/115 [01:26<00:09,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  90%|█████████ | 104/115 [01:26<00:09,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  91%|█████████▏| 105/115 [01:25<00:08,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|█████████ | 104/115 [01:26<00:09,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  91%|█████████▏| 105/115 [01:26<00:08,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  91%|█████████▏| 105/115 [01:26<00:08,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  91%|█████████▏| 105/115 [01:26<00:08,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  91%|█████████▏| 105/115 [01:26<00:08,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  91%|█████████▏| 105/115 [01:26<00:08,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  92%|█████████▏| 106/115 [01:26<00:07,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  90%|█████████ | 104/115 [01:26<00:09,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  90%|█████████ | 104/115 [01:26<00:09,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  91%|█████████▏| 105/115 [01:26<00:08,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  92%|█████████▏| 106/115 [01:26<00:07,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  91%|█████████▏| 105/115 [01:26<00:08,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  91%|█████████▏| 105/115 [01:26<00:08,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  91%|█████████▏| 105/115 [01:26<00:08,  1.19ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  91%|█████████▏| 105/115 [01:27<00:08,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  91%|█████████▏| 105/115 [01:27<00:08,  1.20ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  91%|█████████▏| 105/115 [01:26<00:08,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  92%|█████████▏| 106/115 [01:27<00:07,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  92%|█████████▏| 106/115 [01:27<00:07,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  92%|█████████▏| 106/115 [01:27<00:07,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  92%|█████████▏| 106/115 [01:27<00:07,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  92%|█████████▏| 106/115 [01:27<00:07,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  93%|█████████▎| 107/115 [01:27<00:06,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  92%|█████████▏| 106/115 [01:27<00:07,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  92%|█████████▏| 106/115 [01:27<00:07,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  93%|█████████▎| 107/115 [01:27<00:06,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  92%|█████████▏| 106/115 [01:27<00:07,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  92%|█████████▏| 106/115 [01:27<00:07,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  92%|█████████▏| 106/115 [01:27<00:07,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  92%|█████████▏| 106/115 [01:28<00:07,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  92%|█████████▏| 106/115 [01:28<00:07,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  93%|█████████▎| 107/115 [01:28<00:06,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  94%|█████████▍| 108/115 [01:28<00:05,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  93%|█████████▎| 107/115 [01:27<00:06,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  93%|█████████▎| 107/115 [01:27<00:06,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  93%|█████████▎| 107/115 [01:28<00:06,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  93%|█████████▎| 107/115 [01:28<00:06,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  93%|█████████▎| 107/115 [01:28<00:06,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  94%|█████████▍| 108/115 [01:28<00:05,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  93%|█████████▎| 107/115 [01:28<00:06,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  94%|█████████▍| 108/115 [01:28<00:05,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  93%|█████████▎| 107/115 [01:28<00:06,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  93%|█████████▎| 107/115 [01:28<00:06,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  93%|█████████▎| 107/115 [01:28<00:06,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  93%|█████████▎| 107/115 [01:28<00:06,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  93%|█████████▎| 107/115 [01:28<00:06,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  94%|█████████▍| 108/115 [01:29<00:05,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  95%|█████████▍| 109/115 [01:29<00:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  94%|█████████▍| 108/115 [01:29<00:05,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  94%|█████████▍| 108/115 [01:29<00:05,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  94%|█████████▍| 108/115 [01:28<00:05,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  94%|█████████▍| 108/115 [01:28<00:05,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  94%|█████████▍| 108/115 [01:28<00:05,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  94%|█████████▍| 108/115 [01:28<00:05,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  95%|█████████▍| 109/115 [01:29<00:04,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  94%|█████████▍| 108/115 [01:29<00:05,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  95%|█████████▍| 109/115 [01:29<00:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  95%|█████████▍| 109/115 [01:29<00:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  95%|█████████▍| 109/115 [01:29<00:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  95%|█████████▍| 109/115 [01:29<00:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  95%|█████████▍| 109/115 [01:29<00:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  94%|█████████▍| 108/115 [01:29<00:05,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  94%|█████████▍| 108/115 [01:29<00:05,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  94%|█████████▍| 108/115 [01:29<00:05,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  95%|█████████▍| 109/115 [01:29<00:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  96%|█████████▌| 110/115 [01:30<00:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  95%|█████████▍| 109/115 [01:30<00:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  95%|█████████▍| 109/115 [01:30<00:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  95%|█████████▍| 109/115 [01:30<00:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  96%|█████████▌| 110/115 [01:29<00:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  95%|█████████▍| 109/115 [01:30<00:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  96%|█████████▌| 110/115 [01:30<00:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  96%|█████████▌| 110/115 [01:30<00:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  96%|█████████▌| 110/115 [01:30<00:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  96%|█████████▌| 110/115 [01:30<00:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  96%|█████████▌| 110/115 [01:30<00:04,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 111/115 [01:30<00:03,  1.23ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  95%|█████████▍| 109/115 [01:30<00:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  95%|█████████▍| 109/115 [01:30<00:04,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  97%|█████████▋| 111/115 [01:30<00:03,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  96%|█████████▌| 110/115 [01:30<00:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  96%|█████████▌| 110/115 [01:31<00:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  96%|█████████▌| 110/115 [01:31<00:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  96%|█████████▌| 110/115 [01:31<00:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  96%|█████████▌| 110/115 [01:31<00:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  96%|█████████▌| 110/115 [01:31<00:04,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 111/115 [01:31<00:03,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 111/115 [01:31<00:03,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 111/115 [01:31<00:03,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 111/115 [01:31<00:03,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 111/115 [01:31<00:03,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 112/115 [01:31<00:02,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 111/115 [01:31<00:03,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 112/115 [01:32<00:02,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 112/115 [01:32<00:02,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 112/115 [01:32<00:02,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 112/115 [01:32<00:02,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 112/115 [01:32<00:02,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  98%|█████████▊| 113/115 [01:32<00:01,  1.23ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 112/115 [01:32<00:02,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  98%|█████████▊| 113/115 [01:32<00:01,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  98%|█████████▊| 113/115 [01:32<00:01,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  98%|█████████▊| 113/115 [01:33<00:01,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  98%|█████████▊| 113/115 [01:33<00:01,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  98%|█████████▊| 113/115 [01:33<00:01,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  99%|█████████▉| 114/115 [01:33<00:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  98%|█████████▊| 113/115 [01:33<00:01,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:33<00:00,  1.39ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:33<00:00,  1.23ba/s]\u001b[0m\n",
      "\u001b[34minput_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\u001b[0m\n",
      "\u001b[34minputs 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[34mlabel_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[34mlabels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  99%|█████████▉| 114/115 [01:33<00:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  99%|█████████▉| 114/115 [01:33<00:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  96%|█████████▌| 110/115 [01:31<00:04,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  97%|█████████▋| 112/115 [01:31<00:02,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  97%|█████████▋| 111/115 [01:31<00:03,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  97%|█████████▋| 111/115 [01:31<00:03,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  97%|█████████▋| 111/115 [01:31<00:03,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  97%|█████████▋| 111/115 [01:31<00:03,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  97%|█████████▋| 111/115 [01:32<00:03,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  97%|█████████▋| 111/115 [01:32<00:03,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  98%|█████████▊| 113/115 [01:32<00:01,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  97%|█████████▋| 112/115 [01:32<00:02,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  97%|█████████▋| 112/115 [01:32<00:02,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  97%|█████████▋| 112/115 [01:32<00:02,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  97%|█████████▋| 112/115 [01:32<00:02,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  97%|█████████▋| 112/115 [01:33<00:02,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  97%|█████████▋| 112/115 [01:33<00:02,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  99%|█████████▉| 114/115 [01:33<00:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  98%|█████████▊| 113/115 [01:33<00:01,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  98%|█████████▊| 113/115 [01:33<00:01,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  98%|█████████▊| 113/115 [01:33<00:01,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  98%|█████████▊| 113/115 [01:33<00:01,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:33<00:00,  1.39ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:33<00:00,  1.23ba/s]\u001b[0m\n",
      "\u001b[35minput_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\u001b[0m\n",
      "\u001b[35minputs 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[35mlabel_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[35mlabels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  98%|█████████▊| 113/115 [01:33<00:01,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  98%|█████████▊| 113/115 [01:33<00:01,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  99%|█████████▉| 114/115 [01:34<00:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  99%|█████████▉| 114/115 [01:34<00:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  99%|█████████▉| 114/115 [01:34<00:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  99%|█████████▉| 114/115 [01:34<00:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  99%|█████████▉| 114/115 [01:33<00:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  99%|█████████▉| 114/115 [01:33<00:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  99%|█████████▉| 114/115 [01:33<00:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.38ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.38ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[34minput_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\u001b[0m\n",
      "\u001b[34minputs 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[34mlabel_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[34minput_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\u001b[0m\n",
      "\u001b[34mlabels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\u001b[0m\n",
      "\u001b[34minputs 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[34mlabel_ids\u001b[0m\n",
      "\u001b[34m[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[34mlabels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.38ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[34minput_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\u001b[0m\n",
      "\u001b[34minputs 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[34mlabel_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[34mlabels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.38ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[34minput_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\u001b[0m\n",
      "\u001b[34minputs 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[34mlabel_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[34mlabels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  99%|█████████▉| 114/115 [01:34<00:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.38ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[34minput_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\u001b[0m\n",
      "\u001b[34minputs 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[34mlabel_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[34mlabels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.37ba/s]#015Running tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.22ba/s]\u001b[0m\n",
      "\u001b[35minput_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\u001b[0m\n",
      "\u001b[35minputs 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[35mlabel_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[35mlabels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  99%|█████████▉| 114/115 [01:34<00:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset:  99%|█████████▉| 114/115 [01:34<00:00,  1.20ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.37ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[35minput_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\u001b[0m\n",
      "\u001b[35minputs 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[35mlabel_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[35mlabels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.37ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[35minput_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\u001b[0m\n",
      "\u001b[35minputs 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[35mlabel_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[35mlabels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.37ba/s]#015Running tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[35minput_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\u001b[0m\n",
      "\u001b[35minputs 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[35mlabel_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[35mlabels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:35<00:00,  1.37ba/s]#015Running tokenizer on train dataset: 100%|██████████| 115/115 [01:35<00:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:35<00:00,  1.37ba/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:35<00:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[35minput_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\u001b[0m\n",
      "\u001b[35minputs 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[35mlabel_ids\u001b[0m\n",
      "\u001b[35m[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[35minput_idslabels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\n",
      " [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\u001b[0m\n",
      "\u001b[35minputs 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[35mlabel_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[35mlabels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.38ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 115/115 [01:34<00:00,  1.21ba/s]\u001b[0m\n",
      "\u001b[34minput_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\u001b[0m\n",
      "\u001b[34minputs 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\u001b[0m\n",
      "\u001b[34mlabel_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[34mlabels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:33:56,594] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:33:57,804] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:33:57,805] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:33:57,827] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:33:57,829] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:33:57,829] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:33:57,833] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:33:57,834] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:33:57,836] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:33:57,827] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:33:57,829] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:33:57,832] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:33:57,845] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:33:57,848] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:33:57,848] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:33:57,851] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:33:57,851] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py39_cu117/cpu_adam...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mCreating extension directory /root/.cache/torch_extensions/py39_cu117/cpu_adam...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[35mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...\u001b[0m\n",
      "\u001b[35mBuilding extension module cpu_adam...\u001b[0m\n",
      "\u001b[35mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34m[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o\u001b[0m\n",
      "\u001b[35m[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o\u001b[0m\n",
      "\u001b[34m[2/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o\u001b[0m\n",
      "\u001b[34m[3/3] c++ cpu_adam.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 28.594136238098145 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 28.569611310958862 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 28.603925466537476 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 28.555718183517456 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 28.55447220802307 seconds\u001b[0m\n",
      "\u001b[35m[2/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o\u001b[0m\n",
      "\u001b[35m[3/3] c++ cpu_adam.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so\u001b[0m\n",
      "\u001b[35mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[35mTime to load cpu_adam op: 28.834490299224854 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[35mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[35mTime to load cpu_adam op: 28.7480411529541 seconds\u001b[0m\n",
      "\u001b[35mTime to load cpu_adam op: 28.758978128433228 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[35mTime to load cpu_adam op: 28.858872652053833 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[35mTime to load cpu_adam op: 28.88273310661316 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[35mTime to load cpu_adam op: 28.85590171813965 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[35mTime to load cpu_adam op: 28.849780559539795 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[35mTime to load cpu_adam op: 28.84841012954712 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 28.473832845687866 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 28.6611545085907 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 28.65548014640808 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py39_cu117/utils...\u001b[0m\n",
      "\u001b[34mAdam Optimizer #0 is created with AVX2 arithmetic capability.\u001b[0m\n",
      "\u001b[34mConfig: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:34:28,503] [INFO] [logging.py:93:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:34:28,524] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:34:28,524] [INFO] [utils.py:55:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:34:28,524] [INFO] [logging.py:93:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:34:28,524] [INFO] [stage_1_and_2.py:144:__init__] Reduce bucket size 500000000\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:34:28,524] [INFO] [stage_1_and_2.py:145:__init__] Allgather bucket size 500000000\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:34:28,524] [INFO] [stage_1_and_2.py:146:__init__] CPU Offload: True\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:34:28,524] [INFO] [stage_1_and_2.py:147:__init__] Round robin gradient partitioning: False\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module utils...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mCreating extension directory /root/.cache/torch_extensions/py39_cu117/utils...\u001b[0m\n",
      "\u001b[35mAdam Optimizer #0 is created with AVX2 arithmetic capability.\u001b[0m\n",
      "\u001b[35mConfig: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...\u001b[0m\n",
      "\u001b[35mBuilding extension module utils...\u001b[0m\n",
      "\u001b[35mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[34m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.353912830352783 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.422343969345093 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.422369480133057 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.421467781066895 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.421525239944458 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.421649932861328 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.422573566436768 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.42250394821167 seconds\u001b[0m\n",
      "\u001b[35m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[35m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 15.392097473144531 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 15.32282018661499 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 15.422184467315674 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 15.42227292060852 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 15.422245740890503 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 15.421727657318115 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 15.422040939331055 seconds\u001b[0m\n",
      "\u001b[35mTime to load utils op: 15.422648906707764 seconds\u001b[0m\n",
      "\u001b[35mRank: 11 partition count [16] and sizes[(385830400, False)]\u001b[0m\n",
      "\u001b[35mRank: 14 partition count [16] and sizes[(385830400, False)]\u001b[0m\n",
      "\u001b[35mRank: 8 partition count [16] and sizes[(385830400, False)]\u001b[0m\n",
      "\u001b[35mRank: 12 partition count [16] and sizes[(385830400, False)]\u001b[0m\n",
      "\u001b[35mRank: 10 partition count [16] and sizes[(385830400, False)]\u001b[0m\n",
      "\u001b[35mRank: 9 partition count [16] and sizes[(385830400, False)]\u001b[0m\n",
      "\u001b[35mRank: 15 partition count [16] and sizes[(385830400, False)]\u001b[0m\n",
      "\u001b[35mRank: 13 partition count [16] and sizes[(385830400, False)]\u001b[0m\n",
      "\u001b[34mRank: 6 partition count [16] and sizes[(385830400, False)]\u001b[0m\n",
      "\u001b[34mRank: 2 partition count [16] and sizes[(385830400, False)]\u001b[0m\n",
      "\u001b[34mRank: 1 partition count [16] and sizes[(385830400, False)]\u001b[0m\n",
      "\u001b[34mRank: 0 partition count [16] and sizes[(385830400, False)]\u001b[0m\n",
      "\u001b[34mRank: 5 partition count [16] and sizes[(385830400, False)]\u001b[0m\n",
      "\u001b[34mRank: 7 partition count [16] and sizes[(385830400, False)]\u001b[0m\n",
      "\u001b[34mRank: 4 partition count [16] and sizes[(385830400, False)]\u001b[0m\n",
      "\u001b[34mRank: 3 partition count [16] and sizes[(385830400, False)]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:34:59,749] [INFO] [utils.py:829:see_memory_usage] Before initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:34:59,750] [INFO] [utils.py:830:see_memory_usage] MA 12.5 GB         Max_MA 12.5 GB         CA 12.5 GB         Max_CA 12 GB\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:34:59,750] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 101.12 GB, percent = 13.5%\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.00046515464782714844 seconds\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.00039386749267578125 seconds\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.130: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.151: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.157 algo-2:455 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.179 algo-2:452 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.184 algo-2:455 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.185 algo-2:455 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.185 algo-2:455 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.186 algo-2:455 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.186 algo-2:455 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.205 algo-2:452 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.206 algo-2:452 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.206 algo-2:452 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.207 algo-2:452 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.207 algo-2:452 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m05/11/2023 10:35:02 - WARNING - transformers_modules.THUDM.chatglm-6b.4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[35m05/11/2023 10:35:02 - WARNING - transformers_modules.THUDM.chatglm-6b.4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.0006144046783447266 seconds\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.0005960464477539062 seconds\u001b[0m\n",
      "\u001b[35m0%|          | 0/50 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.743: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0006849765777587891 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00057220458984375 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0005605220794677734 seconds\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.505: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.528: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.537 algo-1:451 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.543: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.557 algo-1:446 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00038123130798339844 seconds\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.566 algo-1:451 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.567 algo-1:451 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.567 algo-1:451 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.568 algo-1:451 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.568 algo-1:451 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.572 algo-1:447 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0003898143768310547 seconds\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.585 algo-1:446 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.586 algo-1:446 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.586 algo-1:446 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.587 algo-1:446 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.587 algo-1:446 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0003829002380371094 seconds\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.599 algo-1:447 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.600 algo-1:447 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.600 algo-1:447 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.601 algo-1:447 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.601 algo-1:447 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.668: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.688: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.696: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.696 algo-1:448 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.716 algo-1:450 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.722 algo-1:448 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.723 algo-1:448 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.723 algo-1:448 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.724 algo-1:448 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.724 algo-1:449 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.724 algo-1:448 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.742 algo-1:450 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.743 algo-1:450 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.743 algo-1:450 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.744 algo-1:450 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.744 algo-1:450 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.750 algo-1:449 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.751 algo-1:449 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.751 algo-1:449 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.752 algo-1:449 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.752 algo-1:449 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00038051605224609375 seconds\u001b[0m\n",
      "\u001b[34m05/11/2023 10:35:02 - WARNING - transformers_modules.THUDM.chatglm-6b.4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m05/11/2023 10:35:02 - WARNING - transformers_modules.THUDM.chatglm-6b.4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m05/11/2023 10:35:02 - WARNING - transformers_modules.THUDM.chatglm-6b.4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,851] [INFO] [utils.py:829:see_memory_usage] After initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,852] [INFO] [utils.py:830:see_memory_usage] MA 12.5 GB         Max_MA 12.5 GB         CA 12.5 GB         Max_CA 12 GB\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,852] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 138.52 GB, percent = 18.5%\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,852] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.864: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.892 algo-1:452 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.917 algo-1:452 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.918 algo-1:452 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.918 algo-1:452 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.919 algo-1:452 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02.919 algo-1:452 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,950] [INFO] [utils.py:829:see_memory_usage] After initializing ZeRO optimizer\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,950] [INFO] [utils.py:830:see_memory_usage] MA 12.5 GB         Max_MA 12.5 GB         CA 12.5 GB         Max_CA 12 GB\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,951] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 138.76 GB, percent = 18.6%\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,951] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,952] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,952] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fd2714ffa30>\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,952] [INFO] [logging.py:93:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,952] [INFO] [config.py:1018:print] DeepSpeedEngine configuration:\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,953] [INFO] [config.py:1022:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,953] [INFO] [config.py:1022:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,953] [INFO] [config.py:1022:print]   amp_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,953] [INFO] [config.py:1022:print]   amp_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,953] [INFO] [config.py:1022:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,953] [INFO] [config.py:1022:print]   bfloat16_enabled ............. False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,953] [INFO] [config.py:1022:print]   checkpoint_parallel_write_pipeline  False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,953] [INFO] [config.py:1022:print]   checkpoint_tag_validation_enabled  True\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,953] [INFO] [config.py:1022:print]   checkpoint_tag_validation_fail  False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,953] [INFO] [config.py:1022:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd27160e670>\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,953] [INFO] [config.py:1022:print]   communication_data_type ...... None\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,953] [INFO] [config.py:1022:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,953] [INFO] [config.py:1022:print]   curriculum_enabled_legacy .... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,953] [INFO] [config.py:1022:print]   curriculum_params_legacy ..... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,953] [INFO] [config.py:1022:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,953] [INFO] [config.py:1022:print]   data_efficiency_enabled ...... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,953] [INFO] [config.py:1022:print]   dataloader_drop_last ......... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,953] [INFO] [config.py:1022:print]   disable_allgather ............ False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,953] [INFO] [config.py:1022:print]   dump_state ................... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   eigenvalue_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   eigenvalue_gas_boundary_resolution  1\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   eigenvalue_layer_name ........ bert.encoder.layer\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   eigenvalue_layer_num ......... 0\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   eigenvalue_max_iter .......... 100\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   eigenvalue_stability ......... 1e-06\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   eigenvalue_tol ............... 0.01\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   eigenvalue_verbose ........... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   elasticity_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   fp16_auto_cast ............... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   fp16_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   fp16_master_weights_and_gradients  False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   global_rank .................. 0\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   grad_accum_dtype ............. None\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   gradient_accumulation_steps .. 1\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   gradient_clipping ............ 0.0\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   gradient_predivide_factor .... 1.0\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   initial_dynamic_scale ........ 65536\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   load_universal_checkpoint .... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   loss_scale ................... 0\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   memory_breakdown ............. False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   optimizer_legacy_fusion ...... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   optimizer_name ............... adamw\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   optimizer_params ............. {'lr': 0.0001, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   pld_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   pld_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   prescale_gradients ........... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,954] [INFO] [config.py:1022:print]   scheduler_name ............... WarmupLR\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,955] [INFO] [config.py:1022:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 0}\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,955] [INFO] [config.py:1022:print]   sparse_attention ............. None\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,955] [INFO] [config.py:1022:print]   sparse_gradients_enabled ..... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,955] [INFO] [config.py:1022:print]   steps_per_print .............. 10\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,955] [INFO] [config.py:1022:print]   train_batch_size ............. 16\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,955] [INFO] [config.py:1022:print]   train_micro_batch_size_per_gpu  1\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,955] [INFO] [config.py:1022:print]   use_node_local_storage ....... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,955] [INFO] [config.py:1022:print]   wall_clock_breakdown ......... False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,955] [INFO] [config.py:1022:print]   world_size ................... 16\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,955] [INFO] [config.py:1022:print]   zero_allow_untested_optimizer  True\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,955] [INFO] [config.py:1022:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,955] [INFO] [config.py:1022:print]   zero_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,955] [INFO] [config.py:1022:print]   zero_force_ds_cpu_optimizer .. True\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,955] [INFO] [config.py:1022:print]   zero_optimization_stage ...... 2\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:02,955] [INFO] [config.py:1007:print_user_config]   json = {\n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"zero_allow_untested_optimizer\": true, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"loss_scale\": 0, \n",
      "        \"initial_scale_power\": 16, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 0.0001, \n",
      "            \"betas\": [0.9, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 0.0\n",
      "        }\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupLR\", \n",
      "        \"params\": {\n",
      "            \"warmup_min_lr\": 0, \n",
      "            \"warmup_max_lr\": 0.0001, \n",
      "            \"warmup_num_steps\": 0\n",
      "        }\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 5.000000e+08, \n",
      "        \"overlap_comm\": false, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 5.000000e+08, \n",
      "        \"contiguous_gradients\": true\n",
      "    }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0003192424774169922 seconds\u001b[0m\n",
      "\u001b[34m0%|          | 0/50 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m05/11/2023 10:35:02 - WARNING - transformers_modules.THUDM.chatglm-6b.4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m05/11/2023 10:35:02 - WARNING - transformers_modules.THUDM.chatglm-6b.4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m05/11/2023 10:35:02 - WARNING - transformers_modules.THUDM.chatglm-6b.4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:03.058: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:03.084 algo-1:445 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:03.109 algo-1:445 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:03.110 algo-1:445 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:03.110 algo-1:445 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:03.111 algo-1:445 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:03.111 algo-1:445 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.753: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.772 algo-2:451 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.782 algo-2:449 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.800 algo-2:451 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.801 algo-2:451 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.801 algo-2:451 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.802 algo-2:451 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.802 algo-2:451 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.810 algo-2:449 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.811 algo-2:449 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.811 algo-2:449 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.812 algo-2:449 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.812 algo-2:449 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.0004932880401611328 seconds\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.0004673004150390625 seconds\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.0004706382751464844 seconds\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.0004642009735107422 seconds\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:02.997: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:03.007: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:03.008: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:03.009: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:03.026 algo-2:456 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:03.036 algo-2:450 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:03.037 algo-2:454 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:03.037 algo-2:453 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:03.053 algo-2:456 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:03.053 algo-2:456 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:03.054 algo-2:456 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:03.054 algo-2:456 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:03.054 algo-2:456 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:03.063 algo-2:450 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:03.063 algo-2:450 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:03.063 algo-2:450 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:03.064 algo-2:454 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:03.064 algo-2:450 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:03.064 algo-2:450 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:03.064 algo-2:453 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:03.065 algo-2:454 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:03.065 algo-2:454 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:03.065 algo-2:453 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:03.065 algo-2:453 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:03.066 algo-2:454 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:03.066 algo-2:454 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:03.066 algo-2:453 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:35:03.066 algo-2:453 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m05/11/2023 10:35:03 - WARNING - transformers_modules.THUDM.chatglm-6b.4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[35m05/11/2023 10:35:03 - WARNING - transformers_modules.THUDM.chatglm-6b.4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[35m05/11/2023 10:35:03 - WARNING - transformers_modules.THUDM.chatglm-6b.4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[35m05/11/2023 10:35:03 - WARNING - transformers_modules.THUDM.chatglm-6b.4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[35m05/11/2023 10:35:03 - WARNING - transformers_modules.THUDM.chatglm-6b.4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[35m05/11/2023 10:35:03 - WARNING - transformers_modules.THUDM.chatglm-6b.4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m05/11/2023 10:35:03 - WARNING - transformers_modules.THUDM.chatglm-6b.4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m05/11/2023 10:35:03 - WARNING - transformers_modules.THUDM.chatglm-6b.4d0fc39a58dcb747ab74ab2c4587bd66dcfa7e74.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:12,916] [INFO] [loss_scaler.py:180:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\u001b[0m\n",
      "\u001b[34m2%|▏         | 1/50 [00:09<08:08,  9.96s/it]\u001b[0m\n",
      "\u001b[35m2%|▏         | 1/50 [00:10<08:23, 10.27s/it]\u001b[0m\n",
      "\u001b[35m4%|▍         | 2/50 [00:18<07:26,  9.30s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:21,536] [INFO] [loss_scaler.py:173:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\u001b[0m\n",
      "\u001b[34m4%|▍         | 2/50 [00:18<07:20,  9.17s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:30,104] [INFO] [loss_scaler.py:173:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\u001b[0m\n",
      "\u001b[34m6%|▌         | 3/50 [00:27<06:58,  8.90s/it]\u001b[0m\n",
      "\u001b[35m6%|▌         | 3/50 [00:27<07:01,  8.97s/it]\u001b[0m\n",
      "\u001b[35m8%|▊         | 4/50 [00:36<06:45,  8.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:38,705] [INFO] [loss_scaler.py:173:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\u001b[0m\n",
      "\u001b[34m8%|▊         | 4/50 [00:35<06:43,  8.78s/it]\u001b[0m\n",
      "\u001b[35m10%|█         | 5/50 [00:44<06:33,  8.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:47,289] [INFO] [loss_scaler.py:173:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\u001b[0m\n",
      "\u001b[34m10%|█         | 5/50 [00:44<06:31,  8.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:35:55,884] [INFO] [loss_scaler.py:173:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\u001b[0m\n",
      "\u001b[34m12%|█▏        | 6/50 [00:52<06:21,  8.67s/it]\u001b[0m\n",
      "\u001b[35m12%|█▏        | 6/50 [00:53<06:22,  8.69s/it]\u001b[0m\n",
      "\u001b[35m14%|█▍        | 7/50 [01:20<10:37, 14.83s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 7/50 [01:20<10:37, 14.82s/it]\u001b[0m\n",
      "\u001b[35m16%|█▌        | 8/50 [01:29<09:06, 13.00s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:36:32,450] [INFO] [loss_scaler.py:173:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024\u001b[0m\n",
      "\u001b[34m16%|█▌        | 8/50 [01:29<09:05, 12.99s/it]\u001b[0m\n",
      "\u001b[35m18%|█▊        | 9/50 [01:57<11:55, 17.45s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 9/50 [01:56<11:55, 17.44s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:37:20,206] [INFO] [logging.py:93:log_dist] [Rank 0] step=10, skipped=7, lr=[0.0001], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[35m20%|██        | 10/50 [02:24<13:45, 20.64s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 6.4698, 'learning_rate': 0.0001, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[35m20%|██        | 10/50 [02:24<13:45, 20.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:37:27,463] [INFO] [timer.py:198:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=1.016690312199126, CurrSamplesPerSec=0.5757637436728834, MemAllocated=12.5GB, MaxMemAllocated=17.3GB\u001b[0m\n",
      "\u001b[34m20%|██        | 10/50 [02:24<13:45, 20.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 6.4698, 'learning_rate': 0.0001, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m20%|██        | 10/50 [02:24<13:45, 20.64s/it]\u001b[0m\n",
      "\u001b[35m22%|██▏       | 11/50 [02:33<11:07, 17.11s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:37:36,570] [INFO] [loss_scaler.py:173:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024, reducing to 512\u001b[0m\n",
      "\u001b[34m22%|██▏       | 11/50 [02:33<11:07, 17.11s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:37:45,138] [INFO] [loss_scaler.py:173:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 512, reducing to 256\u001b[0m\n",
      "\u001b[34m24%|██▍       | 12/50 [02:42<09:11, 14.51s/it]\u001b[0m\n",
      "\u001b[35m24%|██▍       | 12/50 [02:42<09:11, 14.51s/it]\u001b[0m\n",
      "\u001b[35m26%|██▌       | 13/50 [02:51<07:50, 12.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:37:53,761] [INFO] [loss_scaler.py:173:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 256, reducing to 128\u001b[0m\n",
      "\u001b[34m26%|██▌       | 13/50 [02:50<07:50, 12.73s/it]\u001b[0m\n",
      "\u001b[35m28%|██▊       | 14/50 [03:18<10:18, 17.18s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 14/50 [03:18<10:18, 17.18s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 15/50 [03:46<11:54, 20.41s/it]\u001b[0m\n",
      "\u001b[35m30%|███       | 15/50 [03:46<11:54, 20.41s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 16/50 [04:14<12:51, 22.68s/it]\u001b[0m\n",
      "\u001b[35m32%|███▏      | 16/50 [04:14<12:51, 22.68s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 17/50 [04:41<13:19, 24.22s/it]\u001b[0m\n",
      "\u001b[35m34%|███▍      | 17/50 [04:42<13:19, 24.22s/it]\u001b[0m\n",
      "\u001b[35m36%|███▌      | 18/50 [05:10<13:30, 25.32s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 18/50 [05:09<13:30, 25.32s/it]\u001b[0m\n",
      "\u001b[35m38%|███▊      | 19/50 [05:37<13:27, 26.05s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 19/50 [05:37<13:27, 26.05s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:41:00,400] [INFO] [logging.py:93:log_dist] [Rank 0] step=20, skipped=10, lr=[0.0001], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:41:09,067] [INFO] [timer.py:198:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=0.8288625920285457, CurrSamplesPerSec=0.5602221312090491, MemAllocated=12.5GB, MaxMemAllocated=17.3GB\u001b[0m\n",
      "\u001b[34m40%|████      | 20/50 [06:06<13:24, 26.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 16.3398, 'learning_rate': 0.0001, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m40%|████      | 20/50 [06:06<13:24, 26.80s/it]\u001b[0m\n",
      "\u001b[35m40%|████      | 20/50 [06:06<13:24, 26.80s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 16.3398, 'learning_rate': 0.0001, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[35m40%|████      | 20/50 [06:06<13:24, 26.80s/it]\u001b[0m\n",
      "\u001b[35m42%|████▏     | 21/50 [06:35<13:13, 27.35s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 21/50 [06:34<13:13, 27.35s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 22/50 [07:03<12:54, 27.66s/it]\u001b[0m\n",
      "\u001b[35m44%|████▍     | 22/50 [07:03<12:54, 27.66s/it]\u001b[0m\n",
      "\u001b[35m46%|████▌     | 23/50 [07:31<12:33, 27.90s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 23/50 [07:31<12:33, 27.90s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 24/50 [07:59<12:09, 28.05s/it]\u001b[0m\n",
      "\u001b[35m48%|████▊     | 24/50 [08:00<12:09, 28.05s/it]\u001b[0m\n",
      "\u001b[35m50%|█████     | 25/50 [08:28<11:44, 28.20s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 25/50 [08:28<11:44, 28.20s/it]\u001b[0m\n",
      "\u001b[35m52%|█████▏    | 26/50 [08:56<11:15, 28.17s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 26/50 [08:56<11:15, 28.17s/it]\u001b[0m\n",
      "\u001b[35m54%|█████▍    | 27/50 [09:24<10:46, 28.11s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 27/50 [09:24<10:46, 28.11s/it]\u001b[0m\n",
      "\u001b[35m56%|█████▌    | 28/50 [09:52<10:18, 28.10s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 28/50 [09:52<10:18, 28.10s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 29/50 [10:21<09:52, 28.20s/it]\u001b[0m\n",
      "\u001b[35m58%|█████▊    | 29/50 [10:21<09:52, 28.20s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:45:43,803] [INFO] [logging.py:93:log_dist] [Rank 0] step=30, skipped=10, lr=[0.0001], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[35m60%|██████    | 30/50 [10:49<09:25, 28.27s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 9.7627, 'learning_rate': 0.0001, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[35m60%|██████    | 30/50 [10:49<09:25, 28.27s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:45:52,502] [INFO] [timer.py:198:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=0.7101442666996118, CurrSamplesPerSec=0.5627898755515587, MemAllocated=12.5GB, MaxMemAllocated=17.3GB\u001b[0m\n",
      "\u001b[34m60%|██████    | 30/50 [10:49<09:25, 28.27s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 9.7627, 'learning_rate': 0.0001, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m60%|██████    | 30/50 [10:49<09:25, 28.27s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 31/50 [11:17<08:58, 28.32s/it]\u001b[0m\n",
      "\u001b[35m62%|██████▏   | 31/50 [11:18<08:58, 28.32s/it]\u001b[0m\n",
      "\u001b[35m64%|██████▍   | 32/50 [11:46<08:31, 28.41s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 32/50 [11:46<08:31, 28.41s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 33/50 [12:15<08:04, 28.48s/it]\u001b[0m\n",
      "\u001b[35m66%|██████▌   | 33/50 [12:15<08:04, 28.48s/it]\u001b[0m\n",
      "\u001b[35m68%|██████▊   | 34/50 [12:44<07:36, 28.51s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 34/50 [12:43<07:36, 28.51s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 35/50 [13:12<07:07, 28.47s/it]\u001b[0m\n",
      "\u001b[35m70%|███████   | 35/50 [13:12<07:07, 28.47s/it]\u001b[0m\n",
      "\u001b[35m72%|███████▏  | 36/50 [13:40<06:38, 28.45s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 36/50 [13:40<06:38, 28.45s/it]\u001b[0m\n",
      "\u001b[35m74%|███████▍  | 37/50 [14:08<06:07, 28.27s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 37/50 [14:08<06:07, 28.27s/it]\u001b[0m\n",
      "\u001b[35m76%|███████▌  | 38/50 [14:36<05:38, 28.18s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 38/50 [14:36<05:38, 28.18s/it]\u001b[0m\n",
      "\u001b[35m78%|███████▊  | 39/50 [15:04<05:09, 28.13s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 39/50 [15:04<05:09, 28.13s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:50:26,790] [INFO] [logging.py:93:log_dist] [Rank 0] step=40, skipped=10, lr=[0.0001], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[35m80%|████████  | 40/50 [15:32<04:40, 28.08s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 8.0115, 'learning_rate': 0.0001, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[35m80%|████████  | 40/50 [15:32<04:40, 28.08s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:50:35,362] [INFO] [timer.py:198:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=0.6654432554777656, CurrSamplesPerSec=0.5717315820040626, MemAllocated=12.5GB, MaxMemAllocated=17.3GB\u001b[0m\n",
      "\u001b[34m80%|████████  | 40/50 [15:32<04:40, 28.09s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 8.0115, 'learning_rate': 0.0001, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m80%|████████  | 40/50 [15:32<04:40, 28.09s/it]\u001b[0m\n",
      "\u001b[35m82%|████████▏ | 41/50 [16:01<04:13, 28.15s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 41/50 [16:00<04:13, 28.15s/it]\u001b[0m\n",
      "\u001b[35m84%|████████▍ | 42/50 [16:29<03:45, 28.19s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 42/50 [16:28<03:45, 28.19s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 43/50 [16:57<03:17, 28.22s/it]\u001b[0m\n",
      "\u001b[35m86%|████████▌ | 43/50 [16:57<03:17, 28.22s/it]\u001b[0m\n",
      "\u001b[35m88%|████████▊ | 44/50 [17:26<02:49, 28.28s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 44/50 [17:25<02:49, 28.28s/it]\u001b[0m\n",
      "\u001b[35m90%|█████████ | 45/50 [17:53<02:20, 28.18s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 45/50 [17:53<02:20, 28.18s/it]\u001b[0m\n",
      "\u001b[35m92%|█████████▏| 46/50 [18:22<01:52, 28.15s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 46/50 [18:21<01:52, 28.15s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 47/50 [18:50<01:24, 28.30s/it]\u001b[0m\n",
      "\u001b[35m94%|█████████▍| 47/50 [18:50<01:24, 28.30s/it]\u001b[0m\n",
      "\u001b[35m96%|█████████▌| 48/50 [19:18<00:56, 28.28s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 48/50 [19:18<00:56, 28.28s/it]\u001b[0m\n",
      "\u001b[35m98%|█████████▊| 49/50 [19:47<00:28, 28.24s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 49/50 [19:46<00:28, 28.24s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:55:09,424] [INFO] [logging.py:93:log_dist] [Rank 0] step=50, skipped=10, lr=[0.0001], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:55:18,046] [INFO] [timer.py:198:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=0.6419698284008188, CurrSamplesPerSec=0.5649196898056054, MemAllocated=12.5GB, MaxMemAllocated=17.3GB\u001b[0m\n",
      "\u001b[34m100%|██████████| 50/50 [20:15<00:00, 28.27s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 7.3646, 'learning_rate': 0.0001, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m100%|██████████| 50/50 [20:15<00:00, 28.27s/it]\u001b[0m\n",
      "\u001b[34mSaving the whole model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-05-11 10:55:18,053 >> Configuration saved in /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-05-11 10:55:18,053 >> Configuration saved in /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-05-11 10:55:18,054 >> Configuration saved in /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-05-11 10:55:18,054 >> Configuration saved in /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/generation_config.json\u001b[0m\n",
      "\u001b[35m100%|██████████| 50/50 [20:15<00:00, 28.27s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 7.3646, 'learning_rate': 0.0001, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[35m100%|██████████| 50/50 [20:15<00:00, 28.27s/it]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1855] 2023-05-11 10:55:47,352 >> The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/pytorch_model.bin.index.json.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1855] 2023-05-11 10:55:47,352 >> The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/pytorch_model.bin.index.json.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-05-11 10:55:47,353 >> tokenizer config file saved in /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-05-11 10:55:47,353 >> tokenizer config file saved in /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-05-11 10:55:47,354 >> Special tokens file saved in /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-05-11 10:55:47,354 >> Special tokens file saved in /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:55:47,356] [INFO] [logging.py:93:log_dist] [Rank 0] [Torch] Checkpoint global_step50 is about to be saved!\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:55:47,361] [INFO] [logging.py:93:log_dist] [Rank 0] Saving model checkpoint: /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/global_step50/mp_rank_00_model_states.pt\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:55:47,361] [INFO] [torch_checkpoint_engine.py:17:save] [Torch] Saving /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/global_step50/mp_rank_00_model_states.pt...\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:56:01,997] [INFO] [torch_checkpoint_engine.py:17:save] [Torch] Saving /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/global_step50/zero_pp_rank_8_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:56:01,992] [INFO] [torch_checkpoint_engine.py:19:save] [Torch] Saved /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/global_step50/mp_rank_00_model_states.pt.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:56:01,997] [INFO] [torch_checkpoint_engine.py:17:save] [Torch] Saving /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/global_step50/zero_pp_rank_0_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:56:09,986] [INFO] [torch_checkpoint_engine.py:19:save] [Torch] Saved /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/global_step50/zero_pp_rank_8_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:56:09,987] [INFO] [engine.py:3430:_save_zero_checkpoint] zero checkpoint saved /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/global_step50/zero_pp_rank_8_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[35m[2023-05-11 10:56:09,987] [INFO] [torch_checkpoint_engine.py:29:commit] [Torch] Checkpoint global_step50 is ready now!\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:56:10,136] [INFO] [torch_checkpoint_engine.py:19:save] [Torch] Saved /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/global_step50/zero_pp_rank_0_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:56:10,136] [INFO] [engine.py:3430:_save_zero_checkpoint] zero checkpoint saved /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/global_step50/zero_pp_rank_0_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34m[2023-05-11 10:56:10,136] [INFO] [torch_checkpoint_engine.py:29:commit] [Torch] Checkpoint global_step50 is ready now!\u001b[0m\n",
      "\u001b[34m{'train_runtime': 1267.1839, 'train_samples_per_second': 0.631, 'train_steps_per_second': 0.039, 'train_loss': 9.58969482421875, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m100%|██████████| 50/50 [21:07<00:00, 28.27s/it]\u001b[0m\n",
      "\u001b[34m------saving model!-----\u001b[0m\n",
      "\u001b[34m------saving model!-----\u001b[0m\n",
      "\u001b[34m------saving model!-----\u001b[0m\n",
      "\u001b[34m------saving model!-----\u001b[0m\n",
      "\u001b[34m------saving model!-----\u001b[0m\n",
      "\u001b[34m------saving model!-----------saving model!-----\u001b[0m\n",
      "\u001b[34m100%|██████████| 50/50 [21:07<00:00, 25.34s/it]\u001b[0m\n",
      "\u001b[34m***** train metrics *****\u001b[0m\n",
      "\u001b[34mepoch                    =       0.01\n",
      "  train_loss               =     9.5897\n",
      "  train_runtime            = 0:21:07.18\n",
      "  train_samples            =     114599\n",
      "  train_samples_per_second =      0.631\n",
      "  train_steps_per_second   =      0.039\u001b[0m\n",
      "\u001b[34m------saving model!-----\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-05-11 10:56:10,144 >> tokenizer config file saved in /tmp/model/adgen-chatglm-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-05-11 10:56:10,144 >> tokenizer config file saved in /tmp/model/adgen-chatglm-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-05-11 10:56:10,144 >> Special tokens file saved in /tmp/model/adgen-chatglm-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-05-11 10:56:10,144 >> Special tokens file saved in /tmp/model/adgen-chatglm-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m------model is saved!-----\u001b[0m\n",
      "\u001b[34mworld_rank===7\u001b[0m\n",
      "\u001b[34m------model is saved!-----\u001b[0m\n",
      "\u001b[34mworld_rank===3\u001b[0m\n",
      "\u001b[34m------model is saved!-----\u001b[0m\n",
      "\u001b[34mworld_rank===1\u001b[0m\n",
      "\u001b[34m------model is saved!-----\u001b[0m\n",
      "\u001b[34mworld_rank===6\u001b[0m\n",
      "\u001b[34m------model is saved!-----\u001b[0m\n",
      "\u001b[34mworld_rank===5\u001b[0m\n",
      "\u001b[34m------model is saved!-----\u001b[0m\n",
      "\u001b[34mworld_rank===2\u001b[0m\n",
      "\u001b[34m------model is saved!-----\u001b[0m\n",
      "\u001b[34mworld_rank===4\u001b[0m\n",
      "\u001b[34mSaving the whole model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-05-11 10:56:10,156 >> Configuration saved in /tmp/model/adgen-chatglm-6b-ft/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-05-11 10:56:10,156 >> Configuration saved in /tmp/model/adgen-chatglm-6b-ft/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-05-11 10:56:10,156 >> Configuration saved in /tmp/model/adgen-chatglm-6b-ft/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-05-11 10:56:10,156 >> Configuration saved in /tmp/model/adgen-chatglm-6b-ft/generation_config.json\u001b[0m\n",
      "\u001b[35m{'train_runtime': 1267.4991, 'train_samples_per_second': 0.631, 'train_steps_per_second': 0.039, 'train_loss': 9.58969482421875, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[35m100%|██████████| 50/50 [21:07<00:00, 28.27s/it]\u001b[0m\n",
      "\u001b[35m------saving model!-----\u001b[0m\n",
      "\u001b[35m------saving model!-----\u001b[0m\n",
      "\u001b[35m------saving model!-----\u001b[0m\n",
      "\u001b[35m------saving model!-----\u001b[0m\n",
      "\u001b[35m------saving model!-----\u001b[0m\n",
      "\u001b[35m------saving model!-----\u001b[0m\n",
      "\u001b[35m------saving model!-----\u001b[0m\n",
      "\u001b[35m100%|██████████| 50/50 [21:07<00:00, 25.35s/it]\u001b[0m\n",
      "\u001b[35m------saving model!-----\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2171] 2023-05-11 10:56:10,143 >> tokenizer config file saved in /tmp/model/adgen-chatglm-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2171] 2023-05-11 10:56:10,143 >> tokenizer config file saved in /tmp/model/adgen-chatglm-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2178] 2023-05-11 10:56:10,144 >> Special tokens file saved in /tmp/model/adgen-chatglm-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2178] 2023-05-11 10:56:10,144 >> Special tokens file saved in /tmp/model/adgen-chatglm-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[35m------model is saved!-----\u001b[0m\n",
      "\u001b[35mworld_rank===14\u001b[0m\n",
      "\u001b[35m------model is saved!-----\u001b[0m\n",
      "\u001b[35mworld_rank===11\u001b[0m\n",
      "\u001b[35m------model is saved!-----\u001b[0m\n",
      "\u001b[35mworld_rank===15\u001b[0m\n",
      "\u001b[35m------model is saved!-----\u001b[0m\n",
      "\u001b[35mworld_rank===10\u001b[0m\n",
      "\u001b[35m------model is saved!-----\u001b[0m\n",
      "\u001b[35mworld_rank===13\u001b[0m\n",
      "\u001b[35m------model is saved!-----\u001b[0m\n",
      "\u001b[35mworld_rank===9\u001b[0m\n",
      "\u001b[35m------model is saved!-----\u001b[0m\n",
      "\u001b[35mworld_rank===8\u001b[0m\n",
      "\u001b[35m------model is saved!-----\u001b[0m\n",
      "\u001b[35mworld_rank===12\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1855] 2023-05-11 10:56:39,295 >> The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/model/adgen-chatglm-6b-ft/pytorch_model.bin.index.json.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1855] 2023-05-11 10:56:39,295 >> The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/model/adgen-chatglm-6b-ft/pytorch_model.bin.index.json.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-05-11 10:56:39,297 >> tokenizer config file saved in /tmp/model/adgen-chatglm-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-05-11 10:56:39,297 >> tokenizer config file saved in /tmp/model/adgen-chatglm-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-05-11 10:56:39,297 >> Special tokens file saved in /tmp/model/adgen-chatglm-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-05-11 10:56:39,297 >> Special tokens file saved in /tmp/model/adgen-chatglm-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m------model is saved!-----\u001b[0m\n",
      "\u001b[34mworld_rank===0\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/config.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/latest s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/latest\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/all_results.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/all_results.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/tokenizer_config.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/tokenization_chatglm.py s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/tokenization_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/rng_state_6.pth s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/rng_state_6.pth\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/special_tokens_map.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/configuration_chatglm.py s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/configuration_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/generation_config.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/generation_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/rng_state_2.pth s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/rng_state_2.pth\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/special_tokens_map.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/generation_config.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/generation_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/rng_state_7.pth s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/rng_state_7.pth\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/training_args.bin s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/training_args.bin\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/trainer_state.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/trainer_state.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/configuration_chatglm.py s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/configuration_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/quantization.py s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/quantization.py\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/training_args.bin s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/training_args.bin\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/config.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/rng_state_5.pth s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/rng_state_5.pth\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/rng_state_4.pth s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/rng_state_4.pth\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/zero_to_fp32.py s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/zero_to_fp32.py\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/rng_state_1.pth s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/rng_state_1.pth\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/train_results.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/train_results.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/modeling_chatglm.py s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/modeling_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/pytorch_model.bin.index.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/rng_state_0.pth s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/rng_state_0.pth\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/tokenization_chatglm.py s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/tokenization_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/trainer_state.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/trainer_state.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/rng_state_3.pth s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/rng_state_3.pth\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/pytorch_model.bin.index.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/quantization.py s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/quantization.py\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/ice_text.model s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/ice_text.model\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/tokenizer_config.json s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/modeling_chatglm.py s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/modeling_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/ice_text.model s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/ice_text.model\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/global_step50/zero_pp_rank_6_mp_rank_00_optim_states.pt s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/global_step50/zero_pp_rank_6_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/global_step50/zero_pp_rank_7_mp_rank_00_optim_states.pt s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/global_step50/zero_pp_rank_7_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/global_step50/zero_pp_rank_1_mp_rank_00_optim_states.pt s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/global_step50/zero_pp_rank_1_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/global_step50/zero_pp_rank_3_mp_rank_00_optim_states.pt s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/global_step50/zero_pp_rank_3_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/global_step50/zero_pp_rank_2_mp_rank_00_optim_states.pt s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/global_step50/zero_pp_rank_2_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/global_step50/zero_pp_rank_4_mp_rank_00_optim_states.pt s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/global_step50/zero_pp_rank_4_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/global_step50/zero_pp_rank_0_mp_rank_00_optim_states.pt s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/global_step50/zero_pp_rank_0_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/global_step50/zero_pp_rank_5_mp_rank_00_optim_states.pt s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/global_step50/zero_pp_rank_5_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/pytorch_model-00002-of-00002.bin s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/pytorch_model-00002-of-00002.bin\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/pytorch_model-00001-of-00002.bin s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/pytorch_model-00001-of-00002.bin\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/pytorch_model-00001-of-00002.bin s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/pytorch_model-00001-of-00002.bin\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/checkpoint-50/global_step50/mp_rank_00_model_states.pt s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/checkpoint-50/global_step50/mp_rank_00_model_states.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/model/adgen-chatglm-6b-ft/pytorch_model-00002-of-00002.bin s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/pytorch_model-00002-of-00002.bin\u001b[0m\n",
      "\u001b[35m2023-05-11 10:57:32,752 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2023-05-11 10:57:32,752 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2023-05-11 10:57:32,752 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m2023-05-11 10:57:32,757 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-05-11 10:57:32,757 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-05-11 10:57:32,757 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-05-11 10:58:09 Uploading - Uploading generated training model\n",
      "2023-05-11 10:58:09 Completed - Training job completed\n",
      "Training seconds: 4062\n",
      "Billable seconds: 4062\n"
     ]
    }
   ],
   "source": [
    "# define Training Job Name \n",
    "job_name = f'huggingface-chatglm-deepspeed-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "#define the model s3 path which will store your trained model asset\n",
    "#Note: you should use your real s3 path to configure model_s3_path\n",
    "model_s3_path='s3://{}/llm/models/chatglm/deepspeed-mutip-nodes/'.format(sagemaker_session.default_bucket())\n",
    "output_dir = '/tmp/model/adgen-chatglm-6b-ft'\n",
    "model_name_or_path = 'THUDM/chatglm-6b'\n",
    "\n",
    "\n",
    "instance_count = 2\n",
    "#define the enviroment variables for your scripts.\n",
    "environment = {\n",
    "              'NODE_NUMBER'            : str(instance_count),\n",
    "              'MODEL_S3_PATH'          : model_uri,\n",
    "              'PYTORCH_CUDA_ALLOC_CONF': 'max_split_size_mb:32',\n",
    "              #'LD_LIBRARY_PATH'        : '${LD_LIBRARY_PATH}:/opt/conda/lib/',\n",
    "              'NUM_GPUS'               : str(processes_per_host),\n",
    "              'TRAIN_DATASET'          : '/opt/ml/input/data/AdvertiseGen/train.json',\n",
    "              'TEST_DATASET'           : '/opt/ml/input/data/AdvertiseGen/dev.json',\n",
    "              'PROMPT_COLUMN'          : 'content',\n",
    "              'RESPONSE_COLUMN'        : 'summary',\n",
    "              'MODEL_NAME_OR_PATH'     : model_name_or_path,\n",
    "              'OUTPUT_DIR'             : output_dir,\n",
    "              'MODEL_OUTPUT_S3_PATH'   : model_s3_path,\n",
    "              'TRAIN_STEPS'            :'50'\n",
    "}\n",
    "\n",
    "inputs={\n",
    "   'AdvertiseGen': f\"s3://{bucket}/llm/chatglm/datasets/\"\n",
    "}\n",
    "\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'start.py',          # user endpoint script\n",
    "    source_dir           = './ptuning',               # directory which includes all the files needed for training\n",
    "    instance_type        = instance_type, # instances type used for the training job\n",
    "    instance_count       = instance_count,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    script_mode          = True,\n",
    "    transformers_version = '4.26',            # the transformers version used in the training job\n",
    "    pytorch_version      = '1.13',            # the pytorch_version version used in the training job\n",
    "    py_version           = 'py39',            # the python version used in the training job\n",
    "    environment = environment,\n",
    ")\n",
    "huggingface_estimator.fit(inputs=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec49d8f8-0c83-4c1d-af1b-42a2df6c2f14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE checkpoint-50/\n",
      "2023-05-11 10:56:40        194 all_results.json\n",
      "2023-05-11 10:56:40        870 config.json\n",
      "2023-05-11 10:56:40       4276 configuration_chatglm.py\n",
      "2023-05-11 10:56:40        142 generation_config.json\n",
      "2023-05-11 10:56:40    2706249 ice_text.model\n",
      "2023-05-11 10:56:40      57620 modeling_chatglm.py\n",
      "2023-05-11 10:56:40 12346621179 pytorch_model-00001-of-00002.bin\n",
      "2023-05-11 10:56:40 12346585635 pytorch_model-00002-of-00002.bin\n",
      "2023-05-11 10:56:40      33416 pytorch_model.bin.index.json\n",
      "2023-05-11 10:56:40      15054 quantization.py\n",
      "2023-05-11 10:56:40        125 special_tokens_map.json\n",
      "2023-05-11 10:56:40      17047 tokenization_chatglm.py\n",
      "2023-05-11 10:56:40        495 tokenizer_config.json\n",
      "2023-05-11 10:56:40        194 train_results.json\n",
      "2023-05-11 10:56:40       1123 trainer_state.json\n",
      "2023-05-11 10:56:40       4795 training_args.bin\n"
     ]
    }
   ],
   "source": [
    "#!aws s3 rm s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed/adgen-chatglm-6b-ft/checkpoint-50 --recursive\n",
    "!aws s3 ls s3://sagemaker-us-west-2-687912291502/llm/models/chatglm/deepspeed-mutip-nodes/adgen-chatglm-6b-ft/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37933c32-ad7c-4564-a8d1-dcfd79531338",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##for local test only\n",
    "!cd ./ChatGLM-6B/ptuning/ && bash ./ds_train_finetune.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "008b7a15-0eb5-4a60-803e-132687cb5156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE chatglm/\n",
      "                           PRE model/\n",
      "                           PRE models/\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://sagemaker-us-west-2-687912291502/llm/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
